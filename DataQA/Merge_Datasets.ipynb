{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9ebd96",
   "metadata": {},
   "source": [
    "## Notebook Merge_Datasets\n",
    "This notebook takes the data from pi0, eta and bck previously separated into individual train/test-sets and creates a train-dataset and a test-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52c3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2518d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe fix following issue in the future:\n",
    "#due to the split in previous notebook the size and NCell is missmatched\n",
    "# this leads to more memory being used than necessary\n",
    "# if memory becomes a problem one could split the merging of test and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebd990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to files\n",
    "pi0_train = 'Data-Split/pi0_train.npz'\n",
    "eta_train = 'Data-Split/eta_train.npz'\n",
    "bck_train = 'Data-Split/bck_train.npz'\n",
    "\n",
    "pi0_test = 'Data-Split/pi0_test.npz'\n",
    "eta_test = 'Data-Split/eta_test.npz'\n",
    "bck_test = 'Data-Split/bck_test.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574a5a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Size', 'ClusterN', 'Cluster', 'ClusterTiming', 'ClusterE', 'ClusterPt', 'ClusterModuleNumber', 'ClusterType', 'ClusterRow', 'ClusterCol', 'ClusterM02', 'ClusterM20', 'ClusterDistFromVert', 'PartE', 'PartPt', 'PartEta', 'PartPhi', 'PartIsPrimary', 'PartPID']\n"
     ]
    }
   ],
   "source": [
    "#Create dataloader\n",
    "data_pi0_train = np.load(pi0_train, allow_pickle=True)\n",
    "data_eta_train = np.load(eta_train, allow_pickle=True)\n",
    "data_bck_train = np.load(bck_train, allow_pickle=True)\n",
    "\n",
    "data_pi0_test = np.load(pi0_test, allow_pickle=True)\n",
    "data_eta_test = np.load(eta_test, allow_pickle=True)\n",
    "data_bck_test = np.load(bck_test, allow_pickle=True)\n",
    "\n",
    "print(list(data_bck_train.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac3588",
   "metadata": {},
   "source": [
    "## Calculate size of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7943a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0_size_train = data_pi0_train['Size']\n",
    "eta_size_train = data_eta_train['Size']\n",
    "bck_size_train = data_bck_train['Size']\n",
    "\n",
    "size_train = pi0_size_train.item() + eta_size_train.item() + bck_size_train.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ccdb6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0_size_test = data_pi0_test['Size']\n",
    "eta_size_test = data_eta_test['Size']\n",
    "bck_size_test = data_bck_test['Size']\n",
    "\n",
    "size_test = pi0_size_test.item() + eta_size_test.item() + bck_size_test.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea534e1",
   "metadata": {},
   "source": [
    "## Initialise arrays \n",
    "These arrays are initialised to fit the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bed2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_train = np.zeros((size_train), dtype=np.ubyte)\n",
    "ClusterN_test = np.zeros((size_test), dtype=np.ubyte)\n",
    "\n",
    "ClusterN_train[0:pi0_size_train] = data_pi0_train[\"ClusterN\"]\n",
    "ClusterN_train[pi0_size_train:pi0_size_train+eta_size_train] = data_eta_train[\"ClusterN\"]\n",
    "ClusterN_train[-bck_size_train:] = data_bck_train[\"ClusterN\"]\n",
    "\n",
    "ClusterN_test[0:pi0_size_test] = data_pi0_test[\"ClusterN\"]\n",
    "ClusterN_test[pi0_size_test:pi0_size_test+eta_size_test] = data_eta_test[\"ClusterN\"]\n",
    "ClusterN_test[-bck_size_test:] = data_bck_test[\"ClusterN\"]\n",
    "\n",
    "ClusN_train_max = np.max(ClusterN_train)\n",
    "ClusN_test_max = np.max(ClusterN_test)\n",
    "ClusN_max = np.max([ClusN_train_max, ClusN_test_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5298045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClusN_test_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65426263",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_train = np.zeros((size_train, ClusN_max), dtype=np.float32)\n",
    "ClusterTiming_train = np.zeros((size_train, ClusN_max), dtype=np.float32)\n",
    "ClusterType_train = np.zeros((size_train), dtype=np.ubyte)\n",
    "ClusterE_train = np.zeros((size_train), dtype=np.float32)\n",
    "ClusterPt_train = np.zeros((size_train), dtype=np.float32)\n",
    "ClusterModuleNumber_train = np.zeros((size_train, ClusN_max), dtype=np.ubyte)\n",
    "ClusterCol_train = np.zeros((size_train, ClusN_max), dtype=np.ubyte)\n",
    "ClusterRow_train = np.zeros((size_train, ClusN_max), dtype=np.ubyte)\n",
    "ClusterM20_train = np.zeros((size_train), dtype=np.float32)\n",
    "ClusterM02_train = np.zeros((size_train), dtype= np.float32)\n",
    "ClusterDistFromVert_train = np.zeros((size_train), dtype=np.float32)\n",
    "PartE_train = np.zeros((size_train), dtype=np.float32)\n",
    "PartPt_train = np.zeros((size_train), dtype=np.float32)\n",
    "PartEta_train = np.zeros((size_train), dtype=np.float32)\n",
    "PartPhi_train = np.zeros((size_train), dtype=np.float32)\n",
    "PartIsPrimary_train = np.zeros((size_train), dtype=bool)\n",
    "PartPID_train = np.zeros((size_train), dtype=np.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163b3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_test = np.zeros((size_test, ClusN_max), dtype=np.float32)\n",
    "ClusterTiming_test = np.zeros((size_test, ClusN_max), dtype=np.float32)\n",
    "ClusterType_test = np.zeros((size_test), dtype=np.ubyte)\n",
    "ClusterE_test = np.zeros((size_test), dtype=np.float32)\n",
    "ClusterPt_test = np.zeros((size_test), dtype=np.float32)\n",
    "ClusterModuleNumber_test = np.zeros((size_test, ClusN_max), dtype=np.ubyte)\n",
    "ClusterCol_test = np.zeros((size_test, ClusN_max), dtype=np.ubyte)\n",
    "ClusterRow_test = np.zeros((size_test, ClusN_max), dtype=np.ubyte)\n",
    "ClusterM20_test = np.zeros((size_test), dtype=np.float32)\n",
    "ClusterM02_test = np.zeros((size_test), dtype= np.float32)\n",
    "ClusterDistFromVert_test = np.zeros((size_test), np.float32)\n",
    "PartE_test = np.zeros((size_test), dtype=np.float32)\n",
    "PartPt_test = np.zeros((size_test), dtype=np.float32)\n",
    "PartEta_test = np.zeros((size_test), dtype=np.float32)\n",
    "PartPhi_test = np.zeros((size_test), dtype=np.float32)\n",
    "PartIsPrimary_test = np.zeros((size_test), dtype=bool)\n",
    "PartPID_test = np.zeros((size_test), dtype=np.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed462afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cluster_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a17275",
   "metadata": {},
   "source": [
    "# Load data into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86063c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_into(arr, keyword, dataset):\n",
    "    if dataset == 'train':\n",
    "        if arr.ndim == 2:\n",
    "            if arr.dtype == np.float32:\n",
    "                arr[0:pi0_size_train] = np.lib.pad( data_pi0_train[keyword], ((0,0),(0, np.abs(data_pi0_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "                arr[pi0_size_train:pi0_size_train+eta_size_train] = np.lib.pad( data_eta_train[keyword], ((0,0),(0, np.abs(data_eta_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "                arr[-bck_size_train:] = np.lib.pad( data_bck_train[keyword], ((0,0),(0, np.abs(data_bck_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "            else:\n",
    "                arr[0:pi0_size_train] = np.lib.pad( data_pi0_train[keyword], ((0,0),(0, np.abs(data_pi0_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "                arr[pi0_size_train:pi0_size_train+eta_size_train] = np.lib.pad( data_eta_train[keyword], ((0,0),(0, np.abs(data_eta_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "                arr[-bck_size_train:] = np.lib.pad( data_bck_train[keyword], ((0,0),(0, np.abs(data_bck_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "\n",
    "        else:        \n",
    "            arr[0:pi0_size_train] = data_pi0_train[keyword]\n",
    "            arr[pi0_size_train:pi0_size_train+eta_size_train] = data_eta_train[keyword]\n",
    "            arr[-bck_size_train:] = data_bck_train[keyword]\n",
    "    \n",
    "    if dataset == 'test':\n",
    "        if arr.ndim == 2:\n",
    "            if arr.dtype == np.float32:\n",
    "                arr[0:pi0_size_test] = np.lib.pad( data_pi0_test[keyword], ((0,0),(0, np.abs(data_pi0_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "                arr[pi0_size_test:pi0_size_test+eta_size_test] = np.lib.pad( data_eta_test[keyword], ((0,0),(0, np.abs(data_eta_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "                arr[-bck_size_test:] = np.lib.pad( data_bck_test[keyword], ((0,0),(0, np.abs(data_bck_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "            else:\n",
    "                arr[0:pi0_size_test] = np.lib.pad( data_pi0_test[keyword], ((0,0),(0, np.abs(data_pi0_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "                arr[pi0_size_test:pi0_size_test+eta_size_test] = np.lib.pad( data_eta_test[keyword], ((0,0),(0, np.abs(data_eta_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "                arr[-bck_size_test:] = np.lib.pad( data_bck_test[keyword], ((0,0),(0, np.abs(data_bck_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))    \n",
    "        else:        \n",
    "            arr[0:pi0_size_test] = data_pi0_test[keyword]\n",
    "            arr[pi0_size_test:pi0_size_test+eta_size_test] = data_eta_test[keyword]\n",
    "            arr[-bck_size_test:] = data_bck_test[keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ba96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_into(Cluster_train, 'Cluster', 'train')\n",
    "load_into(ClusterTiming_train, 'ClusterTiming', 'train')\n",
    "load_into(ClusterType_train, 'ClusterType', 'train')\n",
    "load_into(ClusterE_train, 'ClusterE', 'train')\n",
    "load_into(ClusterPt_train, 'ClusterPt', 'train')\n",
    "load_into(ClusterModuleNumber_train, 'ClusterModuleNumber', 'train')\n",
    "load_into(ClusterRow_train, 'ClusterRow', 'train')\n",
    "load_into(ClusterCol_train, 'ClusterCol', 'train')\n",
    "load_into(ClusterM20_train, 'ClusterM20','train')\n",
    "load_into(ClusterM02_train, 'ClusterM02','train')\n",
    "load_into(ClusterDistFromVert_train, 'ClusterDistFromVert', 'train')\n",
    "load_into(PartE_train, 'PartE','train')\n",
    "load_into(PartPt_train, 'PartPt','train')\n",
    "load_into(PartEta_train, 'PartEta', 'train')\n",
    "load_into(PartPhi_train, 'PartPhi', 'train')\n",
    "load_into(PartIsPrimary_train, 'PartIsPrimary', 'train')\n",
    "load_into(PartPID_train, 'PartPID', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa8361e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_into(Cluster_test, 'Cluster', 'test')\n",
    "load_into(ClusterTiming_test, 'ClusterTiming', 'test')\n",
    "load_into(ClusterType_test, 'ClusterType', 'test')\n",
    "load_into(ClusterE_test, 'ClusterE', 'test')\n",
    "load_into(ClusterPt_test, 'ClusterPt', 'test')\n",
    "load_into(ClusterModuleNumber_test, 'ClusterModuleNumber', 'test')\n",
    "load_into(ClusterCol_test, 'ClusterCol', 'test')\n",
    "load_into(ClusterRow_test, 'ClusterRow', 'test')\n",
    "load_into(ClusterM20_test, 'ClusterM20','test')\n",
    "load_into(ClusterM02_test, 'ClusterM02','test')\n",
    "load_into(ClusterDistFromVert_test, 'ClusterDistFromVert', 'test')\n",
    "load_into(PartE_test, 'PartE','test')\n",
    "load_into(PartPt_test, 'PartPt','test')\n",
    "load_into(PartEta_test, 'PartEta', 'test')\n",
    "load_into(PartPhi_test, 'PartPhi', 'test')\n",
    "load_into(PartIsPrimary_test, 'PartIsPrimary', 'test')\n",
    "load_into(PartPID_test, 'PartPID', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0ca8920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cluster_test.dtype == np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d5cf0",
   "metadata": {},
   "source": [
    "## Save the train- and test-datasets for further usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6807ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../CNN/Data/data_train', ClusterN=ClusterN_train, Cluster=Cluster_train\n",
    "                    , ClusterE=ClusterE_train, ClusterPt=ClusterPt_train\n",
    "                    , ClusterModuleNumber=ClusterModuleNumber_train, ClusterType=ClusterType_train\n",
    "                    , ClusterCol=ClusterCol_train, ClusterRow=ClusterRow_train, ClusterM02=ClusterM02_train\n",
    "                    , ClusterM20=ClusterM20_train, PartE=PartE_train, PartPt=PartPt_train\n",
    "                    , PartEta=PartEta_train, PartPhi=PartPhi_train, PartIsPrimary=PartIsPrimary_train\n",
    "                    , PartPID=PartPID_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35f32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../CNN/Data/data_test', ClusterN=ClusterN_test, Cluster=Cluster_test\n",
    "                    , ClusterE=ClusterE_test, ClusterPt=ClusterPt_test\n",
    "                    , ClusterModuleNumber=ClusterModuleNumber_test, ClusterType=ClusterType_test\n",
    "                    , ClusterCol=ClusterCol_test, ClusterRow=ClusterRow_test, ClusterM02=ClusterM02_test\n",
    "                    , ClusterM20=ClusterM20_test, PartE=PartE_test, PartPt=PartPt_test\n",
    "                    , PartEta=PartEta_test, PartPhi=PartPhi_test, PartIsPrimary=PartIsPrimary_test\n",
    "                    , PartPID=PartPID_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7141738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root-env]",
   "language": "python",
   "name": "conda-env-root-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
