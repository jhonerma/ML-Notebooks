{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9ebd96",
   "metadata": {},
   "source": [
    "## Notebook Merge_Datasets\n",
    "This notebook takes the data from pi0, eta and bck previously separated into individual train/test-sets and creates a train-dataset and a test-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52c3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb5e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_into(arr, keyword, dataset, ClusN_max):\n",
    "    #function used for loading data from file into array \n",
    "    print(\"Currently merging {}\".format(keyword))\n",
    "    \n",
    "    #Create dataloader and load size of datasets\n",
    "    if dataset == 'train':\n",
    "        data_pi0 = np.load('Data-Split/pi0_train.npz', allow_pickle=True)\n",
    "        data_eta = np.load('Data-Split/eta_train.npz', allow_pickle=True)\n",
    "        data_bck = np.load('Data-Split/bck_train.npz', allow_pickle=True)\n",
    "        \n",
    "        pi0_size = data_pi0['Size']\n",
    "        eta_size = data_eta['Size']\n",
    "        bck_size = data_bck['Size']\n",
    "        \n",
    "    elif dataset == 'test':       \n",
    "        data_pi0 = np.load('Data-Split/pi0_test.npz', allow_pickle=True)\n",
    "        data_eta = np.load('Data-Split/eta_test.npz', allow_pickle=True)\n",
    "        data_bck = np.load('Data-Split/bck_test.npz', allow_pickle=True)\n",
    "        \n",
    "        pi0_size = data_pi0['Size']\n",
    "        eta_size = data_eta['Size']\n",
    "        bck_size = data_bck['Size']\n",
    "        \n",
    "    else:\n",
    "        print(\"Give valid dataset type!\")\n",
    "        \n",
    "    #Fill given array with data from files, merging the seperate datasets into a single train-/testset    \n",
    "    if arr.ndim == 2:\n",
    "        if arr.dtype == np.float32:\n",
    "            arr[0:pi0_size] = np.lib.pad( data_pi0[keyword], ((0,0)\n",
    "                                         ,(0, np.abs(data_pi0[keyword].shape[1] - ClusN_max)))\n",
    "                                         ,'constant', constant_values=(np.NaN))            \n",
    "            arr[pi0_size:pi0_size+eta_size] = np.lib.pad( data_eta[keyword], ((0,0)\n",
    "                                                ,(0, np.abs(data_eta[keyword].shape[1] - ClusN_max)))\n",
    "                                                ,'constant', constant_values=(np.NaN))            \n",
    "            arr[-bck_size:] = np.lib.pad( data_bck[keyword], ((0,0)\n",
    "                                         ,(0, np.abs(data_bck[keyword].shape[1] - ClusN_max)))\n",
    "                                         ,'constant', constant_values=(np.NaN))\n",
    "        else:\n",
    "            arr[0:pi0_size] = np.lib.pad (data_pi0[keyword], ((0,0)\n",
    "                                         ,(0, np.abs(data_pi0[keyword].shape[1] - ClusN_max)))\n",
    "                                         ,'constant', constant_values=(100))           \n",
    "            arr[pi0_size:pi0_size+eta_size] = np.lib.pad( data_eta[keyword], ((0,0)\n",
    "                                                         ,(0, np.abs(data_eta[keyword].shape[1] - ClusN_max)))\n",
    "                                                         ,'constant', constant_values=(100))            \n",
    "            arr[-bck_size:] = np.lib.pad( data_bck[keyword], ((0,0)\n",
    "                                         ,(0, np.abs(data_bck[keyword].shape[1] - ClusN_max)))\n",
    "                                         ,'constant', constant_values=(100))\n",
    "\n",
    "    else:        \n",
    "        arr[0:pi0_size] = data_pi0[keyword]\n",
    "        arr[pi0_size:pi0_size+eta_size] = data_eta[keyword]\n",
    "        arr[-bck_size:] = data_bck[keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189737a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_train(size_train, ClusN_max):\n",
    "    #setup arrays\n",
    "    ClusterN_train = np.zeros((size_train), dtype=np.ubyte)\n",
    "    Cluster_train = np.zeros((size_train, ClusN_max), dtype=np.float32)\n",
    "    ClusterTiming_train = np.zeros((size_train, ClusN_max), dtype=np.float32)\n",
    "    ClusterType_train = np.zeros((size_train), dtype=np.ubyte)\n",
    "    ClusterE_train = np.zeros((size_train), dtype=np.float32)\n",
    "    ClusterPt_train = np.zeros((size_train), dtype=np.float32)\n",
    "    ClusterModuleNumber_train = np.zeros((size_train, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterCol_train = np.zeros((size_train, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterRow_train = np.zeros((size_train, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterM20_train = np.zeros((size_train), dtype=np.float32)\n",
    "    ClusterM02_train = np.zeros((size_train), dtype= np.float32)\n",
    "    ClusterDistFromVert_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartE_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartPt_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartEta_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartPhi_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartIsPrimary_train = np.zeros((size_train), dtype=bool)\n",
    "    PartPID_train = np.zeros((size_train), dtype=np.short)\n",
    "    \n",
    "    #load data into arrays\n",
    "    load_into(ClusterN_train, 'ClusterN', 'train', ClusN_max)\n",
    "    load_into(Cluster_train, 'Cluster', 'train', ClusN_max)\n",
    "    load_into(ClusterTiming_train, 'ClusterTiming', 'train', ClusN_max)\n",
    "    load_into(ClusterType_train, 'ClusterType', 'train', ClusN_max)\n",
    "    load_into(ClusterE_train, 'ClusterE', 'train', ClusN_max)\n",
    "    load_into(ClusterPt_train, 'ClusterPt', 'train', ClusN_max)\n",
    "    load_into(ClusterModuleNumber_train, 'ClusterModuleNumber', 'train', ClusN_max)\n",
    "    load_into(ClusterRow_train, 'ClusterRow', 'train', ClusN_max)\n",
    "    load_into(ClusterCol_train, 'ClusterCol', 'train', ClusN_max)\n",
    "    load_into(ClusterM20_train, 'ClusterM20','train', ClusN_max)\n",
    "    load_into(ClusterM02_train, 'ClusterM02','train', ClusN_max)\n",
    "    load_into(ClusterDistFromVert_train, 'ClusterDistFromVert', 'train', ClusN_max)\n",
    "    load_into(PartE_train, 'PartE','train', ClusN_max)\n",
    "    load_into(PartPt_train, 'PartPt','train', ClusN_max)\n",
    "    load_into(PartEta_train, 'PartEta', 'train', ClusN_max)\n",
    "    load_into(PartPhi_train, 'PartPhi', 'train', ClusN_max)\n",
    "    load_into(PartIsPrimary_train, 'PartIsPrimary', 'train', ClusN_max)\n",
    "    load_into(PartPID_train, 'PartPID', 'train', ClusN_max)\n",
    "    \n",
    "    #Get minimum and maximum values for normalization in later stage\n",
    "    maxCellEnergy = np.nanmax(Cluster_train)\n",
    "    maxCellTiming = np.nanmax(ClusterTiming_train)\n",
    "    maxClusterEnergy = np.nanmax(ClusterE_train)\n",
    "    maxClusterPt = np.nanmax(ClusterPt_train)\n",
    "    maxClusterM20 = np.nanmax(ClusterM20_train)\n",
    "    maxClusterM02 = np.nanmax(ClusterM02_train)\n",
    "    maxClusterDistFromVert = np.nanmax(ClusterDistFromVert_train)\n",
    "    maxPartE = np.nanmax(PartE_train)\n",
    "    maxPartPt = np.nanmax(PartPt_train)\n",
    "    maxPartEta = np.nanmax(PartEta_train)\n",
    "    maxPartPhi = np.nanmax(PartPhi_train)\n",
    "    \n",
    "    minCellEnergy = np.nanmin(Cluster_train)\n",
    "    minCellTiming = np.nanmin(ClusterTiming_train)\n",
    "    minClusterEnergy = np.nanmin(ClusterE_train)\n",
    "    minClusterPt = np.nanmin(ClusterPt_train)\n",
    "    minClusterM20 = np.nanmin(ClusterM20_train)\n",
    "    minClusterM02 = np.nanmin(ClusterM02_train)\n",
    "    minClusterDistFromVert = np.nanmin(ClusterDistFromVert_train)\n",
    "    minPartE = np.nanmin(PartE_train)\n",
    "    minPartPt = np.nanmin(PartPt_train)\n",
    "    minPartEta = np.nanmin(PartEta_train)\n",
    "    minPartPhi = np.nanmin(PartPhi_train)\n",
    "    \n",
    "    #save merged dataset and normalization data\n",
    "    print(\"Saving train-dataset\")\n",
    "    np.savez_compressed('../CNN/Data/data_train', Size = ClusterN_train.size, ClusterN=ClusterN_train\n",
    "                        , Cluster=Cluster_train, ClusterTiming=ClusterTiming_train, ClusterE=ClusterE_train\n",
    "                        , ClusterPt=ClusterPt_train, ClusterModuleNumber=ClusterModuleNumber_train\n",
    "                        , ClusterType=ClusterType_train, ClusterCol=ClusterCol_train\n",
    "                        , ClusterRow=ClusterRow_train, ClusterM02=ClusterM02_train, ClusterM20=ClusterM20_train\n",
    "                        , ClusterDistFromVert=ClusterDistFromVert_train\n",
    "                        , PartE=PartE_train, PartPt=PartPt_train, PartEta=PartEta_train, PartPhi=PartPhi_train\n",
    "                        , PartIsPrimary=PartIsPrimary_train, PartPID=PartPID_train)\n",
    "    \n",
    "    print(\"Saving normalization data\")    \n",
    "    np.savez_compressed('../CNN/Data/normalization', maxCellEnergy = maxCellEnergy\n",
    "                        , maxCellTiming = maxCellTiming, maxClusterE = maxClusterEnergy\n",
    "                        , maxClusterPt = maxClusterPt, maxClusterM20 = maxClusterM20\n",
    "                        , maxClusterM02 = maxClusterM02, maxClusterDistFromVert = maxClusterDistFromVert\n",
    "                        , maxPartE = maxPartE, maxPartPt = maxPartPt, maxPartEta = maxPartEta\n",
    "                        , maxPartPhi= maxPartPhi, minCellEnergy = minCellEnergy, minCellTiming = minCellTiming\n",
    "                        , minClusterE = minClusterEnergy, minClusterPt = minClusterPt\n",
    "                        , minClusterM20 = minClusterM20, minClusterM02 = minClusterM02\n",
    "                        , minClusterDistFromVert = minClusterDistFromVert, minPartE = minPartE\n",
    "                        , minPartPt = minPartPt, minPartEta = minPartEta, minPartPhi = minPartPhi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d9fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test(size_test, ClusN_max):\n",
    "    #setup arrays\n",
    "    ClusterN_test = np.zeros((size_test), dtype=np.ubyte)\n",
    "    Cluster_test = np.zeros((size_test, ClusN_max), dtype=np.float32)\n",
    "    ClusterTiming_test = np.zeros((size_test, ClusN_max), dtype=np.float32)\n",
    "    ClusterType_test = np.zeros((size_test), dtype=np.ubyte)\n",
    "    ClusterE_test = np.zeros((size_test), dtype=np.float32)\n",
    "    ClusterPt_test = np.zeros((size_test), dtype=np.float32)\n",
    "    ClusterModuleNumber_test = np.zeros((size_test, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterCol_test = np.zeros((size_test, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterRow_test = np.zeros((size_test, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterM20_test = np.zeros((size_test), dtype=np.float32)\n",
    "    ClusterM02_test = np.zeros((size_test), dtype= np.float32)\n",
    "    ClusterDistFromVert_test = np.zeros((size_test), np.float32)\n",
    "    PartE_test = np.zeros((size_test), dtype=np.float32)\n",
    "    PartPt_test = np.zeros((size_test), dtype=np.float32)\n",
    "    PartEta_test = np.zeros((size_test), dtype=np.float32)\n",
    "    PartPhi_test = np.zeros((size_test), dtype=np.float32)\n",
    "    PartIsPrimary_test = np.zeros((size_test), dtype=bool)\n",
    "    PartPID_test = np.zeros((size_test), dtype=np.short)\n",
    "    \n",
    "    #load data into arrays\n",
    "    load_into(ClusterN_test, 'ClusterN', 'test', ClusN_max)\n",
    "    load_into(Cluster_test, 'Cluster', 'test', ClusN_max)\n",
    "    load_into(ClusterTiming_test, 'ClusterTiming', 'test', ClusN_max)\n",
    "    load_into(ClusterType_test, 'ClusterType', 'test', ClusN_max)\n",
    "    load_into(ClusterE_test, 'ClusterE', 'test', ClusN_max)\n",
    "    load_into(ClusterPt_test, 'ClusterPt', 'test', ClusN_max)\n",
    "    load_into(ClusterModuleNumber_test, 'ClusterModuleNumber', 'test', ClusN_max)\n",
    "    load_into(ClusterCol_test, 'ClusterCol', 'test', ClusN_max)\n",
    "    load_into(ClusterRow_test, 'ClusterRow', 'test', ClusN_max)\n",
    "    load_into(ClusterM20_test, 'ClusterM20','test', ClusN_max)\n",
    "    load_into(ClusterM02_test, 'ClusterM02','test', ClusN_max)\n",
    "    load_into(ClusterDistFromVert_test, 'ClusterDistFromVert', 'test', ClusN_max)\n",
    "    load_into(PartE_test, 'PartE','test', ClusN_max)\n",
    "    load_into(PartPt_test, 'PartPt','test', ClusN_max)\n",
    "    load_into(PartEta_test, 'PartEta', 'test', ClusN_max)\n",
    "    load_into(PartPhi_test, 'PartPhi', 'test', ClusN_max)\n",
    "    load_into(PartIsPrimary_test, 'PartIsPrimary', 'test', ClusN_max)\n",
    "    load_into(PartPID_test, 'PartPID', 'test', ClusN_max)\n",
    "    \n",
    "    #save merged datasets\n",
    "    print(\"Saving test-dataset\")\n",
    "    np.savez_compressed('../CNN/Data/data_test', Size = ClusterN_test.size, ClusterN=ClusterN_test\n",
    "                        , Cluster=Cluster_test, ClusterTiming=ClusterTiming_test\n",
    "                        , ClusterE=ClusterE_test, ClusterPt=ClusterPt_test\n",
    "                        , ClusterModuleNumber=ClusterModuleNumber_test, ClusterType=ClusterType_test\n",
    "                        , ClusterCol=ClusterCol_test, ClusterRow=ClusterRow_test, ClusterM02=ClusterM02_test\n",
    "                        , ClusterM20=ClusterM20_test, ClusterDistFromVert=ClusterDistFromVert_test\n",
    "                        , PartE=PartE_test, PartPt=PartPt_test, PartEta=PartEta_test, PartPhi=PartPhi_test\n",
    "                        , PartIsPrimary=PartIsPrimary_test, PartPID=PartPID_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f172915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SizeAndCellN():\n",
    "    #The maximum size of clusters and the individual size are needed for further steps\n",
    "    #first is needed to set up array sizes\n",
    "    #second is needed later for reconstruction of the clusters\n",
    "    \n",
    "    #path to files\n",
    "    pi0_train = 'Data-Split/pi0_train.npz'\n",
    "    eta_train = 'Data-Split/eta_train.npz'\n",
    "    bck_train = 'Data-Split/bck_train.npz'\n",
    "\n",
    "    pi0_test = 'Data-Split/pi0_test.npz'\n",
    "    eta_test = 'Data-Split/eta_test.npz'\n",
    "    bck_test = 'Data-Split/bck_test.npz'\n",
    "    \n",
    "    #Create dataloader\n",
    "    data_pi0_train = np.load(pi0_train, allow_pickle=True)\n",
    "    data_eta_train = np.load(eta_train, allow_pickle=True)\n",
    "    data_bck_train = np.load(bck_train, allow_pickle=True)\n",
    "    \n",
    "    data_pi0_test = np.load(pi0_test, allow_pickle=True)\n",
    "    data_eta_test = np.load(eta_test, allow_pickle=True)\n",
    "    data_bck_test = np.load(bck_test, allow_pickle=True)\n",
    "\n",
    "    \n",
    "    #Calculate size of dataset\n",
    "    pi0_size_train = data_pi0_train['Size']\n",
    "    eta_size_train = data_eta_train['Size']\n",
    "    bck_size_train = data_bck_train['Size']\n",
    "    \n",
    "    train = [pi0_size_train, eta_size_train, eta_size_train]\n",
    "\n",
    "    Size_train = pi0_size_train.item() + eta_size_train.item() + bck_size_train.item()\n",
    "    print(\"Size of train dataset: {}\".format(Size_train))\n",
    "    \n",
    "    pi0_size_test = data_pi0_test['Size']\n",
    "    eta_size_test = data_eta_test['Size']\n",
    "    bck_size_test = data_bck_test['Size']\n",
    "\n",
    "    Size_test = pi0_size_test.item() + eta_size_test.item() + bck_size_test.item()\n",
    "    print(\"Size of test dataset: {}\\n\".format(Size_test))\n",
    "    \n",
    "    #Readout the clustersize from file\n",
    "    ClusterN_train = np.zeros((Size_train), dtype=np.ubyte)\n",
    "    ClusterN_test = np.zeros((Size_test), dtype=np.ubyte)\n",
    "\n",
    "    ClusterN_train[0:pi0_size_train] = data_pi0_train[\"ClusterN\"]\n",
    "    ClusterN_train[pi0_size_train:pi0_size_train+eta_size_train] = data_eta_train[\"ClusterN\"]\n",
    "    ClusterN_train[-bck_size_train:] = data_bck_train[\"ClusterN\"]\n",
    "\n",
    "    ClusterN_test[0:pi0_size_test] = data_pi0_test[\"ClusterN\"]\n",
    "    ClusterN_test[pi0_size_test:pi0_size_test+eta_size_test] = data_eta_test[\"ClusterN\"]\n",
    "    ClusterN_test[-bck_size_test:] = data_bck_test[\"ClusterN\"]\n",
    "\n",
    "    #Get maximum value\n",
    "    ClusN_train_max = np.max(ClusterN_train)\n",
    "    ClusN_test_max = np.max(ClusterN_test)\n",
    "    ClusN_max = np.max([ClusN_train_max, ClusN_test_max])\n",
    "    \n",
    "    return Size_train, Size_test, ClusN_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75e1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all():\n",
    "    \n",
    "    size_train, size_test, ClusN_max = get_SizeAndCellN()\n",
    "    \n",
    "    print(\"-------- Start Merging Trainset -------\")\n",
    "    merge_train(size_train, ClusN_max)\n",
    "    print(\"------ Finished Merging Trainset ------\\n\")\n",
    "    \n",
    "    print(\"-------- Start Merging Testset --------\")\n",
    "    merge_test(size_test, ClusN_max)\n",
    "    print(\"------ Finished Merging Testset -------\\n\")\n",
    "    print(\"Merging finished!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a43c539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 5508407\n",
      "Size of test dataset: 1377103\n",
      "\n",
      "-------- Start Merging Trainset -------\n",
      "Currently merging ClusterN\n",
      "Currently merging Cluster\n",
      "Currently merging ClusterTiming\n",
      "Currently merging ClusterType\n",
      "Currently merging ClusterE\n",
      "Currently merging ClusterPt\n",
      "Currently merging ClusterModuleNumber\n",
      "Currently merging ClusterRow\n",
      "Currently merging ClusterCol\n",
      "Currently merging ClusterM20\n",
      "Currently merging ClusterM02\n",
      "Currently merging ClusterDistFromVert\n",
      "Currently merging PartE\n",
      "Currently merging PartPt\n",
      "Currently merging PartEta\n",
      "Currently merging PartPhi\n",
      "Currently merging PartIsPrimary\n",
      "Currently merging PartPID\n",
      "Saving train-dataset\n",
      "Saving normalization data\n",
      "------ Finished Merging Trainset ------\n",
      "\n",
      "-------- Start Merging Testset --------\n",
      "Currently merging ClusterN\n",
      "Currently merging Cluster\n",
      "Currently merging ClusterTiming\n",
      "Currently merging ClusterType\n",
      "Currently merging ClusterE\n",
      "Currently merging ClusterPt\n",
      "Currently merging ClusterModuleNumber\n",
      "Currently merging ClusterCol\n",
      "Currently merging ClusterRow\n",
      "Currently merging ClusterM20\n",
      "Currently merging ClusterM02\n",
      "Currently merging ClusterDistFromVert\n",
      "Currently merging PartE\n",
      "Currently merging PartPt\n",
      "Currently merging PartEta\n",
      "Currently merging PartPhi\n",
      "Currently merging PartIsPrimary\n",
      "Currently merging PartPID\n",
      "Saving test-dataset\n",
      "------ Finished Merging Testset -------\n",
      "\n",
      "Merging finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f793d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root-env]",
   "language": "python",
   "name": "conda-env-root-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
