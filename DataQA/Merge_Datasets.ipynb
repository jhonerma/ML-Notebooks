{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9ebd96",
   "metadata": {},
   "source": [
    "## Notebook Merge_Datasets\n",
    "This notebook takes the data from pi0, eta and bck previously separated into individual train/test-sets and creates a train-dataset and a test-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52c3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86063c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_into(arr, keyword, dataset, ClusN_max):\n",
    "    print(\"Currently loading {} from {}\".format(keyword, dataset))\n",
    "    \n",
    "    if dataset == 'train':\n",
    "        #Create dataloader\n",
    "        data_pi0_train = np.load('Data-Split/pi0_train.npz', allow_pickle=True)\n",
    "        data_eta_train = np.load('Data-Split/eta_train.npz', allow_pickle=True)\n",
    "        data_bck_train = np.load('Data-Split/bck_train.npz', allow_pickle=True)\n",
    "        \n",
    "        pi0_size_train = data_pi0_train['Size']\n",
    "        eta_size_train = data_eta_train['Size']\n",
    "        bck_size_train = data_bck_train['Size']\n",
    "        \n",
    "        if arr.ndim == 2:\n",
    "            if arr.dtype == np.float32:\n",
    "                arr[0:pi0_size_train] = np.lib.pad( data_pi0_train[keyword], ((0,0),(0, np.abs(data_pi0_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "                arr[pi0_size_train:pi0_size_train+eta_size_train] = np.lib.pad( data_eta_train[keyword], ((0,0),(0, np.abs(data_eta_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "                arr[-bck_size_train:] = np.lib.pad( data_bck_train[keyword], ((0,0),(0, np.abs(data_bck_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "            else:\n",
    "                arr[0:pi0_size_train] = np.lib.pad( data_pi0_train[keyword], ((0,0),(0, np.abs(data_pi0_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "                arr[pi0_size_train:pi0_size_train+eta_size_train] = np.lib.pad( data_eta_train[keyword], ((0,0),(0, np.abs(data_eta_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "                arr[-bck_size_train:] = np.lib.pad( data_bck_train[keyword], ((0,0),(0, np.abs(data_bck_train[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "\n",
    "        else:        \n",
    "            arr[0:pi0_size_train] = data_pi0_train[keyword]\n",
    "            arr[pi0_size_train:pi0_size_train+eta_size_train] = data_eta_train[keyword]\n",
    "            arr[-bck_size_train:] = data_bck_train[keyword]\n",
    "    \n",
    "    if dataset == 'test':\n",
    "        \n",
    "        data_pi0_test = np.load('Data-Split/pi0_test.npz', allow_pickle=True)\n",
    "        data_eta_test = np.load('Data-Split/eta_test.npz', allow_pickle=True)\n",
    "        data_bck_test = np.load('Data-Split/bck_test.npz', allow_pickle=True)\n",
    "        \n",
    "        pi0_size_test = data_pi0_test['Size']\n",
    "        eta_size_test = data_eta_test['Size']\n",
    "        bck_size_test = data_bck_test['Size']\n",
    "    \n",
    "        if arr.ndim == 2:\n",
    "            if arr.dtype == np.float32:\n",
    "                arr[0:pi0_size_test] = np.lib.pad( data_pi0_test[keyword], ((0,0),(0, np.abs(data_pi0_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "                arr[pi0_size_test:pi0_size_test+eta_size_test] = np.lib.pad( data_eta_test[keyword], ((0,0),(0, np.abs(data_eta_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "                arr[-bck_size_test:] = np.lib.pad( data_bck_test[keyword], ((0,0),(0, np.abs(data_bck_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(np.NaN))\n",
    "            else:\n",
    "                arr[0:pi0_size_test] = np.lib.pad( data_pi0_test[keyword], ((0,0),(0, np.abs(data_pi0_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "                arr[pi0_size_test:pi0_size_test+eta_size_test] = np.lib.pad( data_eta_test[keyword], ((0,0),(0, np.abs(data_eta_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))\n",
    "                arr[-bck_size_test:] = np.lib.pad( data_bck_test[keyword], ((0,0),(0, np.abs(data_bck_test[keyword].shape[1] - ClusN_max))),'constant', constant_values=(100))    \n",
    "        else:        \n",
    "            arr[0:pi0_size_test] = data_pi0_test[keyword]\n",
    "            arr[pi0_size_test:pi0_size_test+eta_size_test] = data_eta_test[keyword]\n",
    "            arr[-bck_size_test:] = data_bck_test[keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189737a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_train(size_train, ClusN_max):\n",
    "\n",
    "    ClusterN_train = np.zeros((size_train), dtype=np.ubyte)\n",
    "    Cluster_train = np.zeros((size_train, ClusN_max), dtype=np.float32)\n",
    "    ClusterTiming_train = np.zeros((size_train, ClusN_max), dtype=np.float32)\n",
    "    ClusterType_train = np.zeros((size_train), dtype=np.ubyte)\n",
    "    ClusterE_train = np.zeros((size_train), dtype=np.float32)\n",
    "    ClusterPt_train = np.zeros((size_train), dtype=np.float32)\n",
    "    ClusterModuleNumber_train = np.zeros((size_train, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterCol_train = np.zeros((size_train, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterRow_train = np.zeros((size_train, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterM20_train = np.zeros((size_train), dtype=np.float32)\n",
    "    ClusterM02_train = np.zeros((size_train), dtype= np.float32)\n",
    "    ClusterDistFromVert_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartE_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartPt_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartEta_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartPhi_train = np.zeros((size_train), dtype=np.float32)\n",
    "    PartIsPrimary_train = np.zeros((size_train), dtype=bool)\n",
    "    PartPID_train = np.zeros((size_train), dtype=np.short)\n",
    "    \n",
    "    load_into(ClusterN_train, 'ClusterN', 'train', ClusN_max)\n",
    "    load_into(Cluster_train, 'Cluster', 'train', ClusN_max)\n",
    "    load_into(ClusterTiming_train, 'ClusterTiming', 'train', ClusN_max)\n",
    "    load_into(ClusterType_train, 'ClusterType', 'train', ClusN_max)\n",
    "    load_into(ClusterE_train, 'ClusterE', 'train', ClusN_max)\n",
    "    load_into(ClusterPt_train, 'ClusterPt', 'train', ClusN_max)\n",
    "    load_into(ClusterModuleNumber_train, 'ClusterModuleNumber', 'train', ClusN_max)\n",
    "    load_into(ClusterRow_train, 'ClusterRow', 'train', ClusN_max)\n",
    "    load_into(ClusterCol_train, 'ClusterCol', 'train', ClusN_max)\n",
    "    load_into(ClusterM20_train, 'ClusterM20','train', ClusN_max)\n",
    "    load_into(ClusterM02_train, 'ClusterM02','train', ClusN_max)\n",
    "    load_into(ClusterDistFromVert_train, 'ClusterDistFromVert', 'train', ClusN_max)\n",
    "    load_into(PartE_train, 'PartE','train', ClusN_max)\n",
    "    load_into(PartPt_train, 'PartPt','train', ClusN_max)\n",
    "    load_into(PartEta_train, 'PartEta', 'train', ClusN_max)\n",
    "    load_into(PartPhi_train, 'PartPhi', 'train', ClusN_max)\n",
    "    load_into(PartIsPrimary_train, 'PartIsPrimary', 'train', ClusN_max)\n",
    "    load_into(PartPID_train, 'PartPID', 'train', ClusN_max)\n",
    "    \n",
    "    maxCellEnergy = np.nanmax(Cluster_train)\n",
    "    maxCellTiming = np.nanmax(ClusterTiming_train)\n",
    "    maxClusterEnergy = np.nanmax(ClusterE_train)\n",
    "    maxClusterPt = np.nanmax(ClusterPt_train)\n",
    "    maxClusterM20 = np.nanmax(ClusterM20_train)\n",
    "    maxClusterM02 = np.nanmax(ClusterM02_train)\n",
    "    maxClusterDistFromVert = np.nanmax(ClusterDistFromVert_train)\n",
    "    maxPartE = np.nanmax(PartE_train)\n",
    "    maxPartPt = np.nanmax(PartPt_train)\n",
    "    maxPartEta = np.nanmax(PartEta_train)\n",
    "    maxPartPhi = np.nanmax(PartPhi_train)\n",
    "    \n",
    "    minCellEnergy = np.nanmin(Cluster_train)\n",
    "    minCellTiming = np.nanmin(ClusterTiming_train)\n",
    "    minClusterEnergy = np.nanmin(ClusterE_train)\n",
    "    minClusterPt = np.nanmin(ClusterPt_train)\n",
    "    minClusterM20 = np.nanmin(ClusterM20_train)\n",
    "    minClusterM02 = np.nanmin(ClusterM02_train)\n",
    "    minClusterDistFromVert = np.nanmin(ClusterDistFromVert_train)\n",
    "    minPartE = np.nanmin(PartE_train)\n",
    "    minPartPt = np.nanmin(PartPt_train)\n",
    "    minPartEta = np.nanmin(PartEta_train)\n",
    "    minPartPhi = np.nanmin(PartPhi_train)\n",
    "    \n",
    "    print(\"Saving train-dataset\")\n",
    "    \n",
    "    np.savez_compressed('../CNN/Data/data_train', Size = ClusterN_train.size, ClusterN=ClusterN_train\n",
    "                    , Cluster=Cluster_train, ClusterTiming=ClusterTiming_train, ClusterE=ClusterE_train\n",
    "                    , ClusterPt=ClusterPt_train, ClusterModuleNumber=ClusterModuleNumber_train\n",
    "                    , ClusterType=ClusterType_train, ClusterCol=ClusterCol_train, ClusterRow=ClusterRow_train\n",
    "                    , ClusterM02=ClusterM02_train, ClusterM20=ClusterM20_train\n",
    "                    , ClusterDistFromVert=ClusterDistFromVert_train\n",
    "                    , PartE=PartE_train, PartPt=PartPt_train, PartEta=PartEta_train, PartPhi=PartPhi_train\n",
    "                    , PartIsPrimary=PartIsPrimary_train, PartPID=PartPID_train)\n",
    "    \n",
    "    print(\"Saving normalization data\")\n",
    "    \n",
    "    np.savez_compressed('../CNN/Data/normalization', maxCellEnergy = maxCellEnergy, maxCellTiming = maxCellTiming\n",
    "                    , maxClusterE = maxClusterEnergy, maxClusterPt = maxClusterPt, maxClusterM20 = maxClusterM20\n",
    "                    , maxClusterM02 = maxClusterM02, maxClusterDistFromVert = maxClusterDistFromVert\n",
    "                    , maxPartE = maxPartE, maxPartPt = maxPartPt, maxPartEta = maxPartEta, maxPartPhi= maxPartPhi\n",
    "                    , minCellEnergy = minCellEnergy, minCellTiming = minCellTiming\n",
    "                    , minClusterE = minClusterEnergy, minClusterPt = minClusterPt\n",
    "                    , minClusterM20 = minClusterM20, minClusterM02 = minClusterM02\n",
    "                    , minClusterDistFromVert = minClusterDistFromVert, minPartE = minPartE, minPartPt = minPartPt\n",
    "                    , minPartEta = minPartEta, minPartPhi = minPartPhi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d9fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test(size_test, ClusN_max):\n",
    "    \n",
    "    ClusterN_test = np.zeros((size_test), dtype=np.ubyte)\n",
    "    Cluster_test = np.zeros((size_test, ClusN_max), dtype=np.float32)\n",
    "    ClusterTiming_test = np.zeros((size_test, ClusN_max), dtype=np.float32)\n",
    "    ClusterType_test = np.zeros((size_test), dtype=np.ubyte)\n",
    "    ClusterE_test = np.zeros((size_test), dtype=np.float32)\n",
    "    ClusterPt_test = np.zeros((size_test), dtype=np.float32)\n",
    "    ClusterModuleNumber_test = np.zeros((size_test, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterCol_test = np.zeros((size_test, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterRow_test = np.zeros((size_test, ClusN_max), dtype=np.ubyte)\n",
    "    ClusterM20_test = np.zeros((size_test), dtype=np.float32)\n",
    "    ClusterM02_test = np.zeros((size_test), dtype= np.float32)\n",
    "    ClusterDistFromVert_test = np.zeros((size_test), np.float32)\n",
    "    PartE_test = np.zeros((size_test), dtype=np.float32)\n",
    "    PartPt_test = np.zeros((size_test), dtype=np.float32)\n",
    "    PartEta_test = np.zeros((size_test), dtype=np.float32)\n",
    "    PartPhi_test = np.zeros((size_test), dtype=np.float32)\n",
    "    PartIsPrimary_test = np.zeros((size_test), dtype=bool)\n",
    "    PartPID_test = np.zeros((size_test), dtype=np.short)\n",
    "    \n",
    "    load_into(ClusterN_test, 'ClusterN', 'test', ClusN_max)\n",
    "    load_into(Cluster_test, 'Cluster', 'test', ClusN_max)\n",
    "    load_into(ClusterTiming_test, 'ClusterTiming', 'test', ClusN_max)\n",
    "    load_into(ClusterType_test, 'ClusterType', 'test', ClusN_max)\n",
    "    load_into(ClusterE_test, 'ClusterE', 'test', ClusN_max)\n",
    "    load_into(ClusterPt_test, 'ClusterPt', 'test', ClusN_max)\n",
    "    load_into(ClusterModuleNumber_test, 'ClusterModuleNumber', 'test', ClusN_max)\n",
    "    load_into(ClusterCol_test, 'ClusterCol', 'test', ClusN_max)\n",
    "    load_into(ClusterRow_test, 'ClusterRow', 'test', ClusN_max)\n",
    "    load_into(ClusterM20_test, 'ClusterM20','test', ClusN_max)\n",
    "    load_into(ClusterM02_test, 'ClusterM02','test', ClusN_max)\n",
    "    load_into(ClusterDistFromVert_test, 'ClusterDistFromVert', 'test', ClusN_max)\n",
    "    load_into(PartE_test, 'PartE','test', ClusN_max)\n",
    "    load_into(PartPt_test, 'PartPt','test', ClusN_max)\n",
    "    load_into(PartEta_test, 'PartEta', 'test', ClusN_max)\n",
    "    load_into(PartPhi_test, 'PartPhi', 'test', ClusN_max)\n",
    "    load_into(PartIsPrimary_test, 'PartIsPrimary', 'test', ClusN_max)\n",
    "    load_into(PartPID_test, 'PartPID', 'test', ClusN_max)\n",
    "    \n",
    "    print(\"Saving test-dataset\")\n",
    "    np.savez_compressed('../CNN/Data/data_test', Size = ClusterN_test.size, ClusterN=ClusterN_test, Cluster=Cluster_test\n",
    "                    , ClusterTiming=ClusterTiming_test, ClusterE=ClusterE_test, ClusterPt=ClusterPt_test\n",
    "                    , ClusterModuleNumber=ClusterModuleNumber_test, ClusterType=ClusterType_test\n",
    "                    , ClusterCol=ClusterCol_test, ClusterRow=ClusterRow_test, ClusterM02=ClusterM02_test\n",
    "                    , ClusterM20=ClusterM20_test, ClusterDistFromVert=ClusterDistFromVert_test\n",
    "                    , PartE=PartE_test, PartPt=PartPt_test, PartEta=PartEta_test, PartPhi=PartPhi_test\n",
    "                    , PartIsPrimary=PartIsPrimary_test, PartPID=PartPID_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f172915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SizeAndCellN():\n",
    "    #path to files\n",
    "    pi0_train = 'Data-Split/pi0_train.npz'\n",
    "    eta_train = 'Data-Split/eta_train.npz'\n",
    "    bck_train = 'Data-Split/bck_train.npz'\n",
    "\n",
    "    pi0_test = 'Data-Split/pi0_test.npz'\n",
    "    eta_test = 'Data-Split/eta_test.npz'\n",
    "    bck_test = 'Data-Split/bck_test.npz'\n",
    "    \n",
    "    #Create dataloader\n",
    "    data_pi0_train = np.load(pi0_train, allow_pickle=True)\n",
    "    data_eta_train = np.load(eta_train, allow_pickle=True)\n",
    "    data_bck_train = np.load(bck_train, allow_pickle=True)\n",
    "    \n",
    "    data_pi0_test = np.load(pi0_test, allow_pickle=True)\n",
    "    data_eta_test = np.load(eta_test, allow_pickle=True)\n",
    "    data_bck_test = np.load(bck_test, allow_pickle=True)\n",
    "    \n",
    "    print(\"List of keys:\")\n",
    "    print(list(data_bck_train.keys()))\n",
    "    print()\n",
    "    \n",
    "    #Calculate size of dataset\n",
    "    pi0_size_train = data_pi0_train['Size']\n",
    "    eta_size_train = data_eta_train['Size']\n",
    "    bck_size_train = data_bck_train['Size']\n",
    "\n",
    "    size_train = pi0_size_train.item() + eta_size_train.item() + bck_size_train.item()\n",
    "    print(\"Size of train dataset: {}\".format(size_train))\n",
    "    \n",
    "    pi0_size_test = data_pi0_test['Size']\n",
    "    eta_size_test = data_eta_test['Size']\n",
    "    bck_size_test = data_bck_test['Size']\n",
    "\n",
    "    size_test = pi0_size_test.item() + eta_size_test.item() + bck_size_test.item()\n",
    "    print(\"Size of test dataset: {}\\n\".format(size_test))\n",
    "    \n",
    "    ClusterN_train = np.zeros((size_train), dtype=np.ubyte)\n",
    "    ClusterN_test = np.zeros((size_test), dtype=np.ubyte)\n",
    "\n",
    "    ClusterN_train[0:pi0_size_train] = data_pi0_train[\"ClusterN\"]\n",
    "    ClusterN_train[pi0_size_train:pi0_size_train+eta_size_train] = data_eta_train[\"ClusterN\"]\n",
    "    ClusterN_train[-bck_size_train:] = data_bck_train[\"ClusterN\"]\n",
    "\n",
    "    ClusterN_test[0:pi0_size_test] = data_pi0_test[\"ClusterN\"]\n",
    "    ClusterN_test[pi0_size_test:pi0_size_test+eta_size_test] = data_eta_test[\"ClusterN\"]\n",
    "    ClusterN_test[-bck_size_test:] = data_bck_test[\"ClusterN\"]\n",
    "\n",
    "    ClusN_train_max = np.max(ClusterN_train)\n",
    "    ClusN_test_max = np.max(ClusterN_test)\n",
    "    ClusN_max = np.max([ClusN_train_max, ClusN_test_max])\n",
    "    \n",
    "    return size_train, size_test, ClusN_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75e1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all():\n",
    "    \n",
    "    size_train, size_test, ClusN_max = get_SizeAndCellN()\n",
    "    \n",
    "    print(\"-------- Start Merging Trainset -------\")\n",
    "    merge_train(size_train, ClusN_max)\n",
    "    print(\"------ Finished Merging Trainset ------\\n\")\n",
    "    \n",
    "    print(\"-------- Start Merging Testset --------\")\n",
    "    merge_test(size_test, ClusN_max)\n",
    "    print(\"------ Finished Merging Testset -------\\n\")\n",
    "    print(\"Merging finished!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a43c539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Size', 'ClusterN', 'Cluster', 'ClusterTiming', 'ClusterE', 'ClusterPt', 'ClusterModuleNumber', 'ClusterType', 'ClusterRow', 'ClusterCol', 'ClusterM02', 'ClusterM20', 'ClusterDistFromVert', 'PartE', 'PartPt', 'PartEta', 'PartPhi', 'PartIsPrimary', 'PartPID']\n",
      "Size of train dataset: 23640464\n",
      "Size of test dataset: 5910117\n",
      "Currently loading ClusterN from train\n",
      "Currently loading Cluster from train\n",
      "Currently loading ClusterTiming from train\n",
      "Currently loading ClusterType from train\n",
      "Currently loading ClusterE from train\n",
      "Currently loading ClusterPt from train\n",
      "Currently loading ClusterModuleNumber from train\n",
      "Currently loading ClusterRow from train\n",
      "Currently loading ClusterCol from train\n",
      "Currently loading ClusterM20 from train\n",
      "Currently loading ClusterM02 from train\n",
      "Currently loading ClusterDistFromVert from train\n",
      "Currently loading PartE from train\n",
      "Currently loading PartPt from train\n",
      "Currently loading PartEta from train\n",
      "Currently loading PartPhi from train\n",
      "Currently loading PartIsPrimary from train\n",
      "Currently loading PartPID from train\n",
      "Currently loading ClusterN from test\n",
      "Currently loading Cluster from test\n",
      "Currently loading ClusterTiming from test\n",
      "Currently loading ClusterType from test\n",
      "Currently loading ClusterE from test\n",
      "Currently loading ClusterPt from test\n",
      "Currently loading ClusterModuleNumber from test\n",
      "Currently loading ClusterCol from test\n",
      "Currently loading ClusterRow from test\n",
      "Currently loading ClusterM20 from test\n",
      "Currently loading ClusterM02 from test\n",
      "Currently loading ClusterDistFromVert from test\n",
      "Currently loading PartE from test\n",
      "Currently loading PartPt from test\n",
      "Currently loading PartEta from test\n",
      "Currently loading PartPhi from test\n",
      "Currently loading PartIsPrimary from test\n",
      "Currently loading PartPID from test\n"
     ]
    }
   ],
   "source": [
    "merge_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root-env]",
   "language": "python",
   "name": "conda-env-root-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
