{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#from tqdm import tqdm,tnrange,tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to data\n",
    "pi0_data_path = 'Data/pi0.npz'\n",
    "eta_data_path = 'Data/eta.npz'\n",
    "bck_data_path = 'Data/bck.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cluster', 'ClusterType', 'ClusterE', 'ClusterPt', 'ClusterModuleNumber', 'ClusterX', 'ClusterY', 'ClusterM20', 'ClusterM02', 'PartE', 'PartPt', 'PartEta', 'PartPhi', 'PartIsPrimary', 'PartPID']\n"
     ]
    }
   ],
   "source": [
    "#Load data from files\n",
    "\n",
    "######--------------------- pi0 -------------------------######\n",
    "data_pi0 = np.load(pi0_data_path, allow_pickle=True)\n",
    "print(list(data_pi0.keys()))\n",
    "\n",
    "pi0_Cluster = data_pi0[\"Cluster\"]\n",
    "pi0_Cluster.reshape(-1,1,50,50)\n",
    "\n",
    "pi0_ClusterType = data_pi0[\"ClusterType\"]\n",
    "pi0_ClusterE = data_pi0[\"ClusterE\"]\n",
    "pi0_ClusterPt = data_pi0[\"ClusterPt\"]\n",
    "pi0_ClusterModuleNumber = data_pi0[\"ClusterModuleNumber\"]\n",
    "pi0_ClusterX = data_pi0[\"ClusterX\"]\n",
    "pi0_ClusterY = data_pi0[\"ClusterY\"]\n",
    "pi0_ClusterM02 = data_pi0[\"ClusterM02\"]\n",
    "pi0_ClusterM20 = data_pi0[\"ClusterM20\"]\n",
    "pi0_PartE = data_pi0[\"PartE\"]\n",
    "pi0_PartPt = data_pi0[\"PartPt\"]\n",
    "pi0_PartEta = data_pi0[\"PartEta\"]\n",
    "pi0_PartPhi = data_pi0[\"PartPhi\"]\n",
    "pi0_PartIsPrimary = data_pi0[\"PartIsPrimary\"]\n",
    "pi0_PartPID = data_pi0[\"PartPID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cluster', 'ClusterType', 'ClusterE', 'ClusterPt', 'ClusterModuleNumber', 'ClusterX', 'ClusterY', 'ClusterM20', 'ClusterM02', 'PartE', 'PartPt', 'PartEta', 'PartPhi', 'PartIsPrimary', 'PartPID']\n"
     ]
    }
   ],
   "source": [
    "######--------------------- eta -------------------------######\n",
    "data_eta = np.load(eta_data_path, allow_pickle=True)\n",
    "print(list(data_eta.keys()))\n",
    "\n",
    "eta_Cluster = data_eta[\"Cluster\"]\n",
    "eta_Cluster.reshape(-1,1,50,50)\n",
    "\n",
    "eta_ClusterType = data_eta[\"ClusterType\"]\n",
    "eta_ClusterE = data_eta[\"ClusterE\"]\n",
    "eta_ClusterPt = data_eta[\"ClusterPt\"]\n",
    "eta_ClusterModuleNumber = data_eta[\"ClusterModuleNumber\"]\n",
    "eta_ClusterX = data_eta[\"ClusterX\"]\n",
    "eta_ClusterY = data_eta[\"ClusterY\"]\n",
    "eta_ClusterM02 = data_eta[\"ClusterM02\"]\n",
    "eta_ClusterM20 = data_eta[\"ClusterM20\"]\n",
    "eta_PartE = data_eta[\"PartE\"]\n",
    "eta_PartPt = data_eta[\"PartPt\"]\n",
    "eta_PartEta = data_eta[\"PartEta\"]\n",
    "eta_PartPhi = data_eta[\"PartPhi\"]\n",
    "eta_PartIsPrimary = data_eta[\"PartIsPrimary\"]\n",
    "eta_PartPID = data_eta[\"PartPID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cluster', 'ClusterType', 'ClusterE', 'ClusterPt', 'ClusterModuleNumber', 'ClusterX', 'ClusterY', 'ClusterM20', 'ClusterM02', 'PartE', 'PartPt', 'PartEta', 'PartPhi', 'PartPID']\n"
     ]
    }
   ],
   "source": [
    "######--------------------- bck -------------------------######\n",
    "data_bck = np.load(bck_data_path, allow_pickle=True)\n",
    "print(list(data_bck.keys()))\n",
    "\n",
    "bck_Cluster = data_bck[\"Cluster\"]\n",
    "bck_Cluster.reshape(-1,1,50,50)\n",
    "\n",
    "bck_ClusterType = data_bck[\"ClusterType\"]\n",
    "bck_ClusterE = data_bck[\"ClusterE\"]\n",
    "bck_ClusterPt = data_bck[\"ClusterPt\"]\n",
    "bck_ClusterModuleNumber = data_bck[\"ClusterModuleNumber\"]\n",
    "bck_ClusterX = data_bck[\"ClusterX\"]\n",
    "bck_ClusterY = data_bck[\"ClusterY\"]\n",
    "bck_ClusterM02 = data_bck[\"ClusterM02\"]\n",
    "bck_ClusterM20 = data_bck[\"ClusterM20\"]\n",
    "bck_PartE = data_bck[\"PartE\"]\n",
    "bck_PartPt = data_bck[\"PartPt\"]\n",
    "bck_PartEta = data_bck[\"PartEta\"]\n",
    "bck_PartPhi = data_bck[\"PartPhi\"]\n",
    "bck_PartIsPrimary = np.zeros_like(bck_PartE, dtype=bool)\n",
    "bck_PartPID = data_bck[\"PartPID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary change for PID into three categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck_PartPID[:] = 0 \n",
    "pi0_PartPID[:] = 1\n",
    "eta_PartPID[:] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test_Train_split\n",
    "Create a training set and a test set from the initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0_Cluster_train, pi0_Cluster_test, pi0_ClusterE_train, pi0_ClusterE_test, \\\n",
    "pi0_ClusterType_train, pi0_ClusterType_test,\\\n",
    "pi0_ClusterPt_train, pi0_ClusterPt_test, pi0_ClusterModuleNumber_train, pi0_ClusterModuleNumber_test, \\\n",
    "pi0_ClusterX_train, pi0_ClusterX_test, pi0_ClusterY_train, pi0_ClusterY_test, \\\n",
    "pi0_ClusterM02_train, pi0_ClusterM02_test, pi0_ClusterM20_train, pi0_ClusterM20_test, \\\n",
    "pi0_PartE_train, pi0_PartE_test, pi0_PartPt_train, pi0_PartPt_test, pi0_PartEta_train, \\\n",
    "pi0_PartEta_test, pi0_PartPhi_train, pi0_PartPhi_test, pi0_PartIsPrimary_train, pi0_PartIsPrimary_test, \\\n",
    "pi0_PartPID_train, pi0_PartPID_test = train_test_split(\n",
    "    pi0_Cluster, pi0_ClusterE, pi0_ClusterType, pi0_ClusterPt, pi0_ClusterModuleNumber, pi0_ClusterX, pi0_ClusterY,\n",
    "    pi0_ClusterM02, pi0_ClusterM20, pi0_PartE, pi0_PartPt, pi0_PartEta, pi0_PartPhi, pi0_PartIsPrimary, \n",
    "    pi0_PartPID, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_Cluster_train, eta_Cluster_test, eta_ClusterE_train, eta_ClusterE_test, \\\n",
    "eta_ClusterType_train, eta_ClusterType_test,\\\n",
    "eta_ClusterPt_train, eta_ClusterPt_test, eta_ClusterModuleNumber_train, eta_ClusterModuleNumber_test, \\\n",
    "eta_ClusterX_train, eta_ClusterX_test, eta_ClusterY_train, eta_ClusterY_test, \\\n",
    "eta_ClusterM02_train, eta_ClusterM02_test, eta_ClusterM20_train, eta_ClusterM20_test, \\\n",
    "eta_PartE_train, eta_PartE_test, eta_PartPt_train, eta_PartPt_test, eta_PartEta_train, \\\n",
    "eta_PartEta_test, eta_PartPhi_train, eta_PartPhi_test, eta_PartIsPrimary_train, eta_PartIsPrimary_test, \\\n",
    "eta_PartPID_train, eta_PartPID_test = train_test_split(\n",
    "    eta_Cluster, eta_ClusterE, eta_ClusterType, eta_ClusterPt, eta_ClusterModuleNumber, eta_ClusterX, eta_ClusterY,\n",
    "    eta_ClusterM02, eta_ClusterM20, eta_PartE, eta_PartPt, eta_PartEta, eta_PartPhi, eta_PartIsPrimary, \n",
    "    eta_PartPID, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck_Cluster_train, bck_Cluster_test, bck_ClusterE_train, bck_ClusterE_test, \\\n",
    "bck_ClusterType_train, bck_ClusterType_test,\\\n",
    "bck_ClusterPt_train, bck_ClusterPt_test, bck_ClusterModuleNumber_train, bck_ClusterModuleNumber_test, \\\n",
    "bck_ClusterX_train, bck_ClusterX_test, bck_ClusterY_train, bck_ClusterY_test, \\\n",
    "bck_ClusterM02_train, bck_ClusterM02_test, bck_ClusterM20_train, bck_ClusterM20_test, \\\n",
    "bck_PartE_train, bck_PartE_test, bck_PartPt_train, bck_PartPt_test, bck_PartEta_train, \\\n",
    "bck_PartEta_test, bck_PartPhi_train, bck_PartPhi_test, bck_PartIsPrimary_train, bck_PartIsPrimary_test, \\\n",
    "bck_PartPID_train, bck_PartPID_test = train_test_split(\n",
    "    bck_Cluster, bck_ClusterE, bck_ClusterType, bck_ClusterPt, bck_ClusterModuleNumber, bck_ClusterX, bck_ClusterY,\n",
    "    bck_ClusterM02, bck_ClusterM20, bck_PartE, bck_PartPt, bck_PartEta, bck_PartPhi, bck_PartIsPrimary, \n",
    "    bck_PartPID, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_train = np.concatenate((pi0_Cluster_train, eta_Cluster_train, bck_Cluster_train))\n",
    "ClusterType_train = np.concatenate((pi0_ClusterType_train, eta_ClusterType_train, bck_ClusterType_train))\n",
    "ClusterE_train = np.concatenate((pi0_ClusterE_train, eta_ClusterE_train, bck_ClusterE_train))\n",
    "ClusterPt_train = np.concatenate((pi0_ClusterPt_train, eta_ClusterPt_train, bck_ClusterPt_train))\n",
    "ClusterModuleNumber_train = np.concatenate((pi0_ClusterModuleNumber_train\n",
    "                                            , eta_ClusterModuleNumber_train, bck_ClusterModuleNumber_train))\n",
    "ClusterX_train = np.concatenate((pi0_ClusterX_train, eta_ClusterX_train, bck_ClusterX_train))\n",
    "ClusterY_train = np.concatenate((pi0_ClusterY_train, eta_ClusterY_train, bck_ClusterY_train))\n",
    "ClusterM02_train = np.concatenate((pi0_ClusterM02_train, eta_ClusterM02_train, bck_ClusterM02_train))\n",
    "ClusterM20_train = np.concatenate((pi0_ClusterM20_train ,eta_ClusterM20_train, bck_ClusterM20_train))\n",
    "PartE_train = np.concatenate((pi0_PartE_train, eta_PartE_train, bck_PartE_train))\n",
    "PartPt_train = np.concatenate((pi0_PartPt_train, eta_PartPt_train, bck_PartPt_train))\n",
    "PartEta_train = np.concatenate((pi0_PartEta_train, eta_PartEta_train, bck_PartEta_train))\n",
    "PartPhi_train = np.concatenate((pi0_PartPhi_train, eta_PartPhi_train, bck_PartPhi_train))\n",
    "PartIsPrimary_train = np.concatenate((pi0_PartIsPrimary_train, eta_PartIsPrimary_train, bck_PartIsPrimary_train))\n",
    "PartPID_train = np.concatenate((pi0_PartPID_train, eta_PartPID_train, bck_PartPID_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_test = np.concatenate((pi0_Cluster_test, eta_Cluster_test, bck_Cluster_test))\n",
    "ClusterType_test = np.concatenate((pi0_ClusterType_test, eta_ClusterType_test, bck_ClusterType_test))\n",
    "ClusterE_test = np.concatenate((pi0_ClusterE_test, eta_ClusterE_test, bck_ClusterE_test))\n",
    "ClusterPt_test = np.concatenate((pi0_ClusterPt_test, eta_ClusterPt_test, bck_ClusterPt_test))\n",
    "ClusterModuleNumber_test = np.concatenate((pi0_ClusterModuleNumber_test\n",
    "                                            , eta_ClusterModuleNumber_test, bck_ClusterModuleNumber_test))\n",
    "ClusterX_test = np.concatenate((pi0_ClusterX_test, eta_ClusterX_test, bck_ClusterX_test))\n",
    "ClusterY_test = np.concatenate((pi0_ClusterY_test, eta_ClusterY_test, bck_ClusterY_test))\n",
    "ClusterM02_test = np.concatenate((pi0_ClusterM02_test, eta_ClusterM02_test, bck_ClusterM02_test))\n",
    "ClusterM20_test = np.concatenate((pi0_ClusterM20_test ,eta_ClusterM20_test, bck_ClusterM20_test))\n",
    "PartE_test = np.concatenate((pi0_PartE_test, eta_PartE_test, bck_PartE_test))\n",
    "PartPt_test = np.concatenate((pi0_PartPt_test, eta_PartPt_test, bck_PartPt_test))\n",
    "PartEta_test = np.concatenate((pi0_PartEta_test, eta_PartEta_test, bck_PartEta_test))\n",
    "PartPhi_test = np.concatenate((pi0_PartPhi_test, eta_PartPhi_test, bck_PartPhi_test))\n",
    "PartIsPrimary_test = np.concatenate((pi0_PartIsPrimary_test, eta_PartIsPrimary_test, bck_PartIsPrimary_test))\n",
    "PartPID_test = np.concatenate((pi0_PartPID_test, eta_PartPID_test, bck_PartPID_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the arrays into [size, 1] for usage with ptorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_train = Cluster_train.reshape((ClusterE_train.size,1,50,50))\n",
    "ClusterType_train = ClusterType_train.reshape((ClusterType_train.size, 1))\n",
    "ClusterE_train = ClusterE_train.reshape((ClusterE_train.size, 1))\n",
    "ClusterPt_train = ClusterPt_train.reshape((ClusterPt_train.size, 1))\n",
    "ClusterModuleNumber_train = ClusterModuleNumber_train.reshape((ClusterModuleNumber_train.size, 1))\n",
    "ClusterX_train = ClusterX_train.reshape((ClusterX_train.size, 1))\n",
    "ClusterY_train = ClusterY_train.reshape((ClusterY_train.size, 1))\n",
    "ClusterM02_train = ClusterM02_train.reshape((ClusterM02_train.size, 1))\n",
    "ClusterM20_train = ClusterM20_train.reshape((ClusterM20_train.size, 1))\n",
    "PartE_train = PartE_train.reshape((PartE_train.size, 1))\n",
    "PartPt_train = PartPt_train.reshape((PartPt_train.size, 1))\n",
    "PartEta_train = PartEta_train.reshape((PartEta_train.size, 1))\n",
    "PartPhi_train = PartPhi_train.reshape((PartPhi_train.size, 1))\n",
    "PartIsPrimary_train = PartIsPrimary_train.reshape((PartIsPrimary_train.size, 1))\n",
    "PartPID_train = PartPID_train.reshape((PartPID_train.size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_test = Cluster_test.reshape((ClusterE_test.size,1,50,50))\n",
    "ClusterType_test = ClusterType_test.reshape((ClusterType_test.size, 1))\n",
    "ClusterE_test = ClusterE_test.reshape((ClusterE_test.size, 1))\n",
    "ClusterPt_test = ClusterPt_test.reshape((ClusterPt_test.size, 1))\n",
    "ClusterModuleNumber_test = ClusterModuleNumber_test.reshape((ClusterModuleNumber_test.size, 1))\n",
    "ClusterX_test = ClusterX_test.reshape((ClusterX_test.size, 1))\n",
    "ClusterY_test = ClusterY_test.reshape((ClusterY_test.size, 1))\n",
    "ClusterM02_test = ClusterM02_test.reshape((ClusterM02_test.size, 1))\n",
    "ClusterM20_test = ClusterM20_test.reshape((ClusterM20_test.size, 1))\n",
    "PartE_test = PartE_test.reshape((PartE_test.size, 1))\n",
    "PartPt_test = PartPt_test.reshape((PartPt_test.size, 1))\n",
    "PartEta_test = PartEta_test.reshape((PartEta_test.size, 1))\n",
    "PartPhi_test = PartPhi_test.reshape((PartPhi_test.size, 1))\n",
    "PartIsPrimary_test = PartIsPrimary_test.reshape((PartIsPrimary_test.size, 1))\n",
    "PartPID_test = PartPID_test.reshape((PartPID_test.size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load it to pytorch `tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_train = torch.tensor(Cluster_train, dtype=torch.float32)\n",
    "ClusterType_train = torch.tensor(ClusterType_train, dtype=torch.uint8)\n",
    "ClusterE_train = torch.tensor(ClusterE_train, dtype=torch.float32)\n",
    "ClusterPt_train = torch.tensor(ClusterPt_train, dtype=torch.float32)\n",
    "ClusterModuleNumber_train = torch.tensor(ClusterModuleNumber_train, dtype=torch.uint8)\n",
    "ClusterX_train = torch.tensor(ClusterX_train, dtype=torch.uint8)\n",
    "ClusterY_train = torch.tensor(ClusterY_train, dtype=torch.uint8)\n",
    "ClusterM02_train = torch.tensor(ClusterM02_train, dtype=torch.float32)\n",
    "ClusterM20_train = torch.tensor(ClusterM20_train, dtype=torch.float32)\n",
    "PartE_train = torch.tensor(PartE_train, dtype=torch.float32)\n",
    "PartPt_train = torch.tensor(PartPt_train, dtype=torch.float32)\n",
    "PartEta_train = torch.tensor(PartEta_train, dtype=torch.float32)\n",
    "PartPhi_train = torch.tensor(PartPhi_train, dtype=torch.float32)\n",
    "PartIsPrimary_train = torch.tensor(PartIsPrimary_train, dtype=torch.bool)\n",
    "PartPID_train = torch.tensor(PartPID_train, dtype=torch.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_test = torch.tensor(Cluster_test, dtype=torch.float32)\n",
    "ClusterType_test = torch.tensor(ClusterType_test, dtype=torch.uint8)\n",
    "ClusterE_test = torch.tensor(ClusterE_test, dtype=torch.float32)\n",
    "ClusterPt_test = torch.tensor(ClusterPt_test, dtype=torch.float32)\n",
    "ClusterModuleNumber_test = torch.tensor(ClusterModuleNumber_test, dtype=torch.uint8)\n",
    "ClusterX_test = torch.tensor(ClusterX_test, dtype=torch.uint8)\n",
    "ClusterY_test = torch.tensor(ClusterY_test, dtype=torch.uint8)\n",
    "ClusterM02_test = torch.tensor(ClusterM02_test, dtype=torch.float32)\n",
    "ClusterM20_test = torch.tensor(ClusterM20_test, dtype=torch.float32)\n",
    "PartE_test = torch.tensor(PartE_test, dtype=torch.float32)\n",
    "PartPt_test = torch.tensor(PartPt_test, dtype=torch.float32)\n",
    "PartEta_test = torch.tensor(PartEta_test, dtype=torch.float32)\n",
    "PartPhi_test = torch.tensor(PartPhi_test, dtype=torch.float32)\n",
    "PartIsPrimary_test = torch.tensor(PartIsPrimary_test, dtype=torch.bool)\n",
    "PartPID_test = torch.tensor(PartPID_test, dtype=torch.short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load it to pytorch `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = utils.TensorDataset( Cluster_train, ClusterType_train, ClusterE_train, ClusterPt_train\n",
    "                                    , ClusterModuleNumber_train, ClusterX_train, ClusterY_train\n",
    "                                    , ClusterM02_train, ClusterM20_train, PartE_train, PartPt_train\n",
    "                                    , PartEta_train, PartPhi_train, PartIsPrimary_train, PartPID_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = utils.TensorDataset( Cluster_test, ClusterType_test, ClusterE_test, ClusterPt_test\n",
    "                                    , ClusterModuleNumber_test, ClusterX_test, ClusterY_test\n",
    "                                    , ClusterM02_test, ClusterM20_test, PartE_test, PartPt_test\n",
    "                                    , PartEta_test, PartPhi_test, PartIsPrimary_test, PartPID_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1610.04490\n",
    "INSTANCE_NOISE = True\n",
    "\n",
    "def add_instance_noise(data, std=0.01):\n",
    "    return data + torch.distributions.Normal(0, std).sample(data.shape).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10,10, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(10,5, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_nn = nn.Sequential(\n",
    "            nn.Linear(3130, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, cluster, clusNumXYEPt):\n",
    "        cluster = F.relu(self.conv1(cluster))\n",
    "        cluster = F.relu(self.conv2(cluster))\n",
    "        cluster = F.relu(self.conv3(cluster))\n",
    "        x = self.flatten(cluster)\n",
    "        x = torch.cat([x, clusNumXYEPt], dim=1)\n",
    "        logits = self.dense_nn(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set loss function and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement train and test loop\n",
    "[0:'Cluster', 1:'ClusterType', 2:'ClusterE', 3:'ClusterPt', 4:'ClusterModuleNumber', 5:'ClusterX', 6:'ClusterY', 7:'ClusterM02', 8:'ClusterM20', 9:'PartE', 10:'PartPt', 11:'PartEta', 12:'PartPhi', 13:'PartIsPrimary', 14:'PartPID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        Cl, ClE, ClPt, ClModNum, ClX, ClY, PartPID = data[0].float().to(device), data[2].float().to(device), \\\n",
    "        data[3].float().to(device), data[4].float().to(device), data[5].float().to(device), \\\n",
    "        data[6].float().to(device), data[14].float().to(device)\n",
    "        \n",
    "        ClusterProperties = torch.cat([ClE.to(device), ClPt.to(device), ClModNum.to(device), ClX.to(device), ClY.to(device)], dim=1)\n",
    "        \n",
    "        #prediction and loss\n",
    "        pred = model(Cl, ClusterProperties)\n",
    "        loss = loss_fn(pred, PartPID[:,0].long())\n",
    "        \n",
    "        #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 10000 == 0:\n",
    "            loss, current = loss.item(), batch * len(ClE)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            Cl, ClE, ClPt, ClModNum, ClX, ClY, PartPID = data[0].float().to(device), data[2].float().to(device), \\\n",
    "            data[3].float().to(device), data[4].float().to(device), data[5].float().to(device), \\\n",
    "            data[6].float().to(device), data[14].float().to(device)\n",
    "            \n",
    "            ClusterProperties = torch.cat([ClE.to(device), ClPt.to(device), ClModNum.to(device), ClX.to(device), ClY.to(device)], dim=1)\n",
    "        \n",
    "            pred = model(Cl, ClusterProperties)\n",
    "            test_loss += loss_fn(pred, PartPID[:,0].long()).item()\n",
    "            correct += (pred.argmax(1) == PartPID[:,0]).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(epochs, start=0):\n",
    "    dat=[0]\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    line1, = ax.plot(dat, label='epoch_loss')\n",
    "    plt.legend()\n",
    "    ax.set_xlim([0,epochs])\n",
    "    fig.canvas.draw()\n",
    "    for epoch in tqdm(range(start, epochs)):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        \n",
    "        train_loop(dataloader_train, model, loss_fn, optimizer)\n",
    "        test_loss, _ = test_loop(dataloader_test, model, loss_fn)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            dat = [test_loss]\n",
    "        else:    \n",
    "            dat.append(test_loss)\n",
    "        line1.set_ydata(dat)\n",
    "        line1.set_xdata(range(len(dat)))\n",
    "        ax.set_ylim(0, np.max(dat)+1)\n",
    "        fig.canvas.draw()\n",
    "        #time.sleep(0.1)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "run_training(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn-env]",
   "language": "python",
   "name": "conda-env-cnn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
