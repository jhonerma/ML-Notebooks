{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU's: 12\n"
     ]
    }
   ],
   "source": [
    "# Show number of avlaible CPU threads\n",
    "# With mulithreading this number is twice the number of physical cores\n",
    "cpu_av = os.cpu_count()\n",
    "print(\"Number of available CPU's: {}\".format(cpu_av))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number CPUS that should be used per trial and dataloader\n",
    "# If set to 1 number of cucurrent training networking is equal to this number\n",
    "# In case of training with GPU this will be limited to number of models training simultaneously on GPU\n",
    "# So number of CPU threads for each trial can be increased \n",
    "cpus_per_trial = 1\n",
    "gpus_per_trial = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to data\n",
    "train_path = 'Data/data_train.npz'\n",
    "test_path = 'Data/data_test.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(train_path, allow_pickle=True)\n",
    "test_data = np.load(test_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_train = train_data['ClusterN']\n",
    "Cluster_train = train_data['Cluster']\n",
    "ClusterTiming_train = train_data['ClusterTiming']\n",
    "ClusterType_train = train_data['ClusterType']\n",
    "ClusterE_train = train_data['ClusterE']\n",
    "ClusterPt_train = train_data['ClusterPt']\n",
    "ClusterModuleNumber_train = train_data['ClusterModuleNumber']\n",
    "ClusterCol_train = train_data['ClusterCol']\n",
    "ClusterRow_train = train_data['ClusterRow']\n",
    "ClusterM02_train = train_data['ClusterM02']\n",
    "ClusterM20_train = train_data['ClusterM20']\n",
    "ClusterDistFromVert_train = train_data['ClusterDistFromVert']\n",
    "PartE_train = train_data['PartE']\n",
    "PartPt_train = train_data['PartPt']\n",
    "PartEta_train = train_data['PartEta']\n",
    "PartPhi_train = train_data['PartPhi']\n",
    "PartIsPrimary_train = train_data['PartIsPrimary']\n",
    "PartPID_train = train_data['PartPID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_test = test_data['ClusterN']\n",
    "Cluster_test = test_data['Cluster']\n",
    "ClusterTiming_test = test_data['ClusterTiming']\n",
    "ClusterType_test = test_data['ClusterType']\n",
    "ClusterE_test = test_data['ClusterE']\n",
    "ClusterPt_test = test_data['ClusterPt']\n",
    "ClusterModuleNumber_test = test_data['ClusterModuleNumber']\n",
    "ClusterCol_test = test_data['ClusterCol']\n",
    "ClusterRow_test = test_data['ClusterRow']\n",
    "ClusterM02_test = test_data['ClusterM02']\n",
    "ClusterM20_test = test_data['ClusterM20']\n",
    "ClusterDistFromVert_test = test_data['ClusterDistFromVert']\n",
    "PartE_test = test_data['PartE']\n",
    "PartPt_test = test_data['PartPt']\n",
    "PartEta_test = test_data['PartEta']\n",
    "PartPhi_test = test_data['PartPhi']\n",
    "PartIsPrimary_test = test_data['PartIsPrimary']\n",
    "PartPID_test = test_data['PartPID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary change for PID into three categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_pid(arr):\n",
    "    arr[np.nonzero((arr != 111) & (arr != 221))] = 0\n",
    "    arr[arr == 111] = 1\n",
    "    arr[arr == 221] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_pid(PartPID_test)\n",
    "change_pid(PartPID_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the arrays into [size, 1] for usage with ptorch\n",
    "\n",
    "For linear layers input is expected as [batch_size, num_features] so no need to reshape the existing arrays like Cluster\n",
    "\n",
    "reconstrcuted clusters later will have to have dim [batch_size, channel, height, width] as input for conv2d-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxClusN_train = np.max(ClusterN_train)\n",
    "maxClusN_test = np.max(ClusterN_test)\n",
    "maxClusN = np.max([maxClusN_test, maxClusN_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_train = ClusterN_train.reshape((ClusterN_train.size, 1))\n",
    "#Cluster_train = Cluster_train.reshape((ClusterE_train.size, maxClusN))\n",
    "ClusterType_train = ClusterType_train.reshape((ClusterType_train.size, 1))\n",
    "ClusterE_train = ClusterE_train.reshape((ClusterE_train.size, 1))\n",
    "ClusterPt_train = ClusterPt_train.reshape((ClusterPt_train.size, 1))\n",
    "#ClusterModuleNumber_train = ClusterModuleNumber_train.reshape((ClusterModuleNumber_train.size, maxClusN))\n",
    "#ClusterRow_train = ClusterRow_train.reshape((ClusterRow_train.size, maxClusN))\n",
    "#ClusterCol_train = ClusterCol_train.reshape((ClusterCol_train.size, maxClusN))\n",
    "ClusterM02_train = ClusterM02_train.reshape((ClusterM02_train.size, 1))\n",
    "ClusterM20_train = ClusterM20_train.reshape((ClusterM20_train.size, 1))\n",
    "ClusterDistFromVert_train = ClusterDistFromVert_train.reshape((ClusterDistFromVert_train.size, 1))\n",
    "PartE_train = PartE_train.reshape((PartE_train.size, 1))\n",
    "PartPt_train = PartPt_train.reshape((PartPt_train.size, 1))\n",
    "PartEta_train = PartEta_train.reshape((PartEta_train.size, 1))\n",
    "PartPhi_train = PartPhi_train.reshape((PartPhi_train.size, 1))\n",
    "PartIsPrimary_train = PartIsPrimary_train.reshape((PartIsPrimary_train.size, 1))\n",
    "PartPID_train = PartPID_train.reshape((PartPID_train.size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_test = ClusterN_test.reshape((ClusterN_test.size, 1))\n",
    "#Cluster_test = Cluster_test.reshape((ClusterE_test.size, maxClusN))\n",
    "ClusterType_test = ClusterType_test.reshape((ClusterType_test.size, 1))\n",
    "ClusterE_test = ClusterE_test.reshape((ClusterE_test.size, 1))\n",
    "ClusterPt_test = ClusterPt_test.reshape((ClusterPt_test.size, 1))\n",
    "#ClusterModuleNumber_test = ClusterModuleNumber_test.reshape((ClusterModuleNumber_test.size, maxClusN))\n",
    "#ClusterRow_test = ClusterRow_test.reshape((ClusterRow_test.size, maxClusN))\n",
    "#ClusterCol_test = ClusterCol_test.reshape((ClusterCol_test.size, maxClusN))\n",
    "ClusterM02_test = ClusterM02_test.reshape((ClusterM02_test.size, 1))\n",
    "ClusterM20_test = ClusterM20_test.reshape((ClusterM20_test.size, 1))\n",
    "ClusterDistFromVert_test = ClusterDistFromVert_test.reshape((ClusterDistFromVert_test.size, 1))\n",
    "PartE_test = PartE_test.reshape((PartE_test.size, 1))\n",
    "PartPt_test = PartPt_test.reshape((PartPt_test.size, 1))\n",
    "PartEta_test = PartEta_test.reshape((PartEta_test.size, 1))\n",
    "PartPhi_test = PartPhi_test.reshape((PartPhi_test.size, 1))\n",
    "PartIsPrimary_test = PartIsPrimary_test.reshape((PartIsPrimary_test.size, 1))\n",
    "PartPID_test = PartPID_test.reshape((PartPID_test.size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load it to pytorch `tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_train = torch.as_tensor(ClusterN_train, dtype=torch.uint8)\n",
    "Cluster_train = torch.as_tensor(Cluster_train, dtype=torch.float32)\n",
    "ClusterTiming_train = torch.as_tensor(ClusterTiming_train, dtype=torch.float32)\n",
    "ClusterType_train = torch.as_tensor(ClusterType_train, dtype=torch.uint8)\n",
    "ClusterE_train = torch.as_tensor(ClusterE_train, dtype=torch.float32)\n",
    "ClusterPt_train = torch.as_tensor(ClusterPt_train, dtype=torch.float32)\n",
    "ClusterModuleNumber_train = torch.as_tensor(ClusterModuleNumber_train, dtype=torch.uint8)\n",
    "ClusterRow_train = torch.as_tensor(ClusterRow_train, dtype=torch.uint8)\n",
    "ClusterCol_train = torch.as_tensor(ClusterCol_train, dtype=torch.uint8)\n",
    "ClusterM02_train = torch.as_tensor(ClusterM02_train, dtype=torch.float32)\n",
    "ClusterM20_train = torch.as_tensor(ClusterM20_train, dtype=torch.float32)\n",
    "ClusterDistFromVert_train = torch.as_tensor(ClusterDistFromVert_train, dtype=torch.float32)\n",
    "PartE_train = torch.as_tensor(PartE_train, dtype=torch.float32)\n",
    "PartPt_train = torch.as_tensor(PartPt_train, dtype=torch.float32)\n",
    "PartEta_train = torch.as_tensor(PartEta_train, dtype=torch.float32)\n",
    "PartPhi_train = torch.as_tensor(PartPhi_train, dtype=torch.float32)\n",
    "PartIsPrimary_train = torch.as_tensor(PartIsPrimary_train, dtype=torch.bool)\n",
    "PartPID_train = torch.as_tensor(PartPID_train, dtype=torch.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_test = torch.as_tensor(ClusterN_test, dtype=torch.uint8)\n",
    "Cluster_test = torch.as_tensor(Cluster_test, dtype=torch.float32)\n",
    "ClusterTiming_test = torch.as_tensor(ClusterTiming_test, dtype=torch.float32)\n",
    "ClusterType_test = torch.as_tensor(ClusterType_test, dtype=torch.uint8)\n",
    "ClusterE_test = torch.as_tensor(ClusterE_test, dtype=torch.float32)\n",
    "ClusterPt_test = torch.as_tensor(ClusterPt_test, dtype=torch.float32)\n",
    "ClusterModuleNumber_test = torch.as_tensor(ClusterModuleNumber_test, dtype=torch.uint8)\n",
    "ClusterRow_test = torch.as_tensor(ClusterRow_test, dtype=torch.uint8)\n",
    "ClusterCol_test = torch.as_tensor(ClusterCol_test, dtype=torch.uint8)\n",
    "ClusterM02_test = torch.as_tensor(ClusterM02_test, dtype=torch.float32)\n",
    "ClusterM20_test = torch.as_tensor(ClusterM20_test, dtype=torch.float32)\n",
    "ClusterDistFromVert_test = torch.as_tensor(ClusterDistFromVert_test, dtype=torch.float32)\n",
    "PartE_test = torch.as_tensor(PartE_test, dtype=torch.float32)\n",
    "PartPt_test = torch.as_tensor(PartPt_test, dtype=torch.float32)\n",
    "PartEta_test = torch.as_tensor(PartEta_test, dtype=torch.float32)\n",
    "PartPhi_test = torch.as_tensor(PartPhi_test, dtype=torch.float32)\n",
    "PartIsPrimary_test = torch.as_tensor(PartIsPrimary_test, dtype=torch.bool)\n",
    "PartPID_test = torch.as_tensor(PartPID_test, dtype=torch.short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for DataSet and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train():\n",
    "    dataset_train = utils.TensorDataset( ClusterN_train, Cluster_train, ClusterTiming_train, ClusterType_train\n",
    "                                    , ClusterE_train, ClusterPt_train, ClusterModuleNumber_train\n",
    "                                    , ClusterRow_train, ClusterCol_train, ClusterM02_train, ClusterM20_train\n",
    "                                    , ClusterDistFromVert_train, PartE_train, PartPt_train\n",
    "                                    , PartEta_train, PartPhi_train, PartIsPrimary_train, PartPID_train )\n",
    "    \n",
    "    return dataset_train\n",
    "\n",
    "def load_data_test():\n",
    "    dataset_test = utils.TensorDataset( ClusterN_test, Cluster_test, ClusterTiming_test, ClusterType_test\n",
    "                                   , ClusterE_test, ClusterPt_test, ClusterModuleNumber_test\n",
    "                                   , ClusterRow_test, ClusterCol_test, ClusterM02_test, ClusterM20_test\n",
    "                                   , ClusterDistFromVert_test, PartE_test, PartPt_test\n",
    "                                    , PartEta_test, PartPhi_test, PartIsPrimary_test, PartPID_test )\n",
    "    \n",
    "    return dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(train_ds, val_ds, bs):\n",
    "    return (\n",
    "        utils.DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=cpus_per_trial),\n",
    "        utils.DataLoader(val_ds, batch_size=bs * 2, shuffle=True, num_workers=cpus_per_trial),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1610.04490\n",
    "INSTANCE_NOISE = False\n",
    "\n",
    "def add_instance_noise(data, std=0.01):\n",
    "    return data + torch.distributions.Normal(0, std).sample(data.shape).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, l1=100, l2=50, l3=25):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10, kernel_size=3, padding=2)\n",
    "        self.conv2 = nn.Conv2d(10,10, kernel_size=3,  padding=2)\n",
    "        self.conv3 = nn.Conv2d(10,10, kernel_size=3, padding=0)\n",
    "        self.conv4 = nn.Conv2d(10,5, kernel_size=1, padding=0)\n",
    "        self.conv5 = nn.Conv2d(5,3, kernel_size=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_nn = nn.Sequential(\n",
    "            nn.Linear(4005, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3,3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, cluster, clusNumXYEPt):\n",
    "        cluster = F.relu(self.conv1(cluster))\n",
    "        cluster = F.relu(self.conv2(cluster))\n",
    "        cluster = F.relu(self.conv3(cluster))\n",
    "        cluster = F.relu(self.conv3(cluster))\n",
    "        x = self.flatten(cluster)\n",
    "        x = torch.cat([x, clusNumXYEPt], dim=1)\n",
    "        logits = self.dense_nn(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for cluster reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_cluster(ncell, modnum, row, col, data, arrsize=20):\n",
    "    if not torch.all( modnum[0] == modnum[:ncell]):\n",
    "        ModNumDif = modnum - torch.min(modnum[:ncell])\n",
    "        mask = torch.where(ModNumDif == 1)\n",
    "        col[mask] += 48\n",
    "        mask = torch.where(ModNumDif == 2)\n",
    "        row[mask] += 24\n",
    "        mask = torch.where(ModNumDif == 3)\n",
    "        row[mask] += 24\n",
    "        col[mask] += 48\n",
    "\n",
    "    arr = torch.zeros((arrsize,arrsize), dtype=torch.float32)\n",
    "  \n",
    "    col_min = torch.min(col[:ncell])\n",
    "    row_min = torch.min(row[:ncell])\n",
    "    width = torch.max(col[:ncell]) - col_min\n",
    "    height = torch.max(row[:ncell]) - row_min\n",
    "    offset_h = ((arrsize-height)/2).int()\n",
    "    offset_w = ((arrsize-width)/2).int()\n",
    "    \n",
    "    for i in range(ncell):\n",
    "        arr[ row[i] - row_min + offset_h, col[i] - col_min + offset_w ] = data[i]\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement train and validation loop\n",
    "[0: 'ClusterN', 1:'Cluster', 2:'ClusterTiming', 3:'ClusterType', 4:'ClusterE', 5:'ClusterPt', 6:'ClusterModuleNumber', 7:'ClusterRow', 8:'ClusterCol', 9:'ClusterM02', 10:'ClusterM20', 11:'ClusterDistFromVert', 12:'PartE', 13:'PartPt', 14:'PartEta', 15:'PartPhi', 16:'PartIsPrimary', 17:'PartPID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "    size = len(dataloader.dataset)\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        ClN, Cl, ClE, ClPt, ClModNum, ClRow, ClCol, ClM02, ClM20, ClDist, PartPID = data[0], data[1],\\\n",
    "        data[4], data[5], data[6], \\\n",
    "        data[7], data[8], data[9], data[10], \\\n",
    "        data[11], data[17]\n",
    "        \n",
    "        # reconstruct clusters from data\n",
    "        clusters_e = []\n",
    "\n",
    "        for i in range(ClN.shape[0]):\n",
    "            cluster_e = reconstruct_cluster(ClN[i], ClModNum[i], ClRow[i], ClCol[i], Cl[i])\n",
    "            clusters_e.append(cluster_e)\n",
    "        \n",
    "        clusters_e = torch.stack(clusters_e)\n",
    "        clusters_e = clusters_e.view(-1, 1, 20,20)\n",
    "        if INSTANCE_NOISE:\n",
    "            clusters_e = add_instance_noise(clusters_e)\n",
    "        clusters_e.to(device)\n",
    "        \n",
    "        ClusterProperties = torch.cat([ClE, ClPt, ClM02, ClM20, ClDist], dim=1)\n",
    "        ClusterProperties.to(device)\n",
    "               \n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction and loss\n",
    "        pred = model(clusters_e, ClusterProperties)\n",
    "        loss = loss_fn(pred, PartPID[:,0].long())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        \n",
    "        if batch % 10 == 9:\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, batch + 1,\n",
    "                                            running_loss / epoch_steps))\n",
    "            running_loss = 0.0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            ClN, Cl, ClE, ClPt, ClModNum, ClRow, ClCol, ClM02, ClM20, ClDist, PartPID = data[0], data[1],\\\n",
    "            data[4], data[5], data[6], \\\n",
    "            data[7], data[8], data[9], data[10], \\\n",
    "            data[11], data[17]\n",
    "        \n",
    "            # reconstruct clusters from data\n",
    "            clusters_e = []\n",
    "\n",
    "            for i in range(ClN.shape[0]):\n",
    "                cluster_e = reconstruct_cluster(ClN[i], ClModNum[i], ClRow[i], ClCol[i], Cl[i])\n",
    "                clusters_e.append(cluster_e)\n",
    "        \n",
    "            clusters_e = torch.stack(clusters_e)\n",
    "            clusters_e = clusters_e.view(-1, 1, 20,20)\n",
    "            if INSTANCE_NOISE:\n",
    "                clusters_e = add_instance_noise(clusters_e)\n",
    "            clusters_e.to(device)\n",
    "        \n",
    "            ClusterProperties = torch.cat([ClE, ClPt, ClM02, ClM20, ClDist], dim=1)\n",
    "            ClusterProperties.to(device)\n",
    "            \n",
    "            pred = model(clusters_e, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == PartPID[:,0]).type(torch.float).sum().item()\n",
    "\n",
    "            loss = loss_fn(pred, PartPID[:,0].long())#.item()\n",
    "            val_loss += loss.cpu().numpy()\n",
    "            val_steps += 1\n",
    "    \n",
    "    with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "        \n",
    "    tune.report(loss=(val_loss / val_steps), accuracy= correct / size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, checkpoint_dir=None):\n",
    "    \n",
    "    # load model\n",
    "    model = CNN(config[\"l1\"],config[\"l2\"],config[\"l3\"])\n",
    "    \n",
    "    # check for avlaible resource and initialize device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    # send model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # initialise loss function and opptimizer\n",
    "    loss_fn = F.cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    \n",
    "    # check whether checkpoint is available\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "    # load dataset\n",
    "    dataset_train = load_data_train()\n",
    "    \n",
    "    # split trainset in train and validation subsets\n",
    "    test_abs = int(len(dataset_train) * 0.8)\n",
    "    subset_train, subset_val = utils.random_split(\n",
    "        dataset_train, [test_abs, len(dataset_train) - test_abs])\n",
    "    \n",
    "    # get dataloaders \n",
    "    dataloader_train, dataloader_val = get_dataloader(subset_train, subset_val, int(config[\"batch_size\"]))\n",
    "                                                      \n",
    "    for epoch in range(100):\n",
    "        train_loop(epoch, dataloader_train, model, loss_fn, optimizer, device=device)\n",
    "        val_loop(epoch, dataloader_train, model, loss_fn, optimizer, device=device)                                              \n",
    "    \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement method for accuracy testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, device=\"cpu\"):\n",
    "    \n",
    "    dataset_test = load_data_test()\n",
    "    \n",
    "    dataloader_test = utils.DataLoader(\n",
    "        dataset_test, batch_size=4, shuffle=False, num_workers=2)\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(dataloader_test.dataset)\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader_test:\n",
    "            ClN, Cl, ClE, ClPt, ClModNum, ClRow, ClCol, ClM02, ClM20, ClDist, PartPID = data[0], data[1],\\\n",
    "            data[4], data[5], data[6], \\\n",
    "            data[7], data[8], data[9], data[10], \\\n",
    "            data[11], data[17]\n",
    "        \n",
    "            # reconstruct clusters from data\n",
    "            clusters_e = []\n",
    "\n",
    "            for i in range(ClN.shape[0]):\n",
    "                cluster_e = reconstruct_cluster(ClN[i], ClModNum[i], ClRow[i], ClCol[i], Cl[i])\n",
    "                clusters_e.append(cluster_e)\n",
    "        \n",
    "            clusters_e = torch.stack(clusters_e)\n",
    "            clusters_e = clusters_e.view(-1, 1, 20,20)\n",
    "            if INSTANCE_NOISE:\n",
    "                clusters_e = add_instance_noise(clusters_e)\n",
    "            clusters_e.to(device)\n",
    "        \n",
    "            ClusterProperties = torch.cat([ClE, ClPt, ClM02, ClM20, ClDist], dim=1)\n",
    "            ClusterProperties.to(device)\n",
    "            \n",
    "            pred = model(clusters_e, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == PartPID[:,0]).type(torch.float).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all Ray Tune functionality and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
    "    \n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"l3\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16, 32, 64])\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"l1\", \"l2\", \"l3\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    \n",
    "    #Get Current date and time\n",
    "    timestr = time.strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "    name = \"PBT-\" + timestr\n",
    "    \n",
    "    result = tune.run(\n",
    "        partial(train_model),\n",
    "        name = name,\n",
    "        resources_per_trial={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        local_dir = \"./Ray_Results\",\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "    \n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    best_trained_model = CNN(best_trial.config[\"l1\"], best_trial.config[\"l2\"], best_trial.config[\"l3\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "    \n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 14:29:16,792\tWARNING experiment.py:295 -- No name detected on trainable. Using DEFAULT.\n",
      "2021-09-15 14:29:16,793\tINFO registry.py:66 -- Detected unknown callable for trainable. Converting to class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 5.5/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/12 CPUs, 0/0 GPUs, 0.0/6.37 GiB heap, 0.0/3.19 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/DEFAULT_2021-09-15_14-29-16\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+---------------------+----------+-------+------+------+------+-------------+--------------+\n",
      "| Trial name          | status   | loc   |   l1 |   l2 |   l3 |          lr |   batch_size |\n",
      "|---------------------+----------+-------+------+------+------+-------------+--------------|\n",
      "| DEFAULT_8afa2_00000 | RUNNING  |       |   64 |   16 |    8 | 0.0662326   |           16 |\n",
      "| DEFAULT_8afa2_00001 | PENDING  |       |  128 |  128 |   64 | 0.00227478  |            4 |\n",
      "| DEFAULT_8afa2_00002 | PENDING  |       |  512 |   16 |  512 | 0.0001217   |           32 |\n",
      "| DEFAULT_8afa2_00003 | PENDING  |       |    4 |  512 |    8 | 0.0738792   |            8 |\n",
      "| DEFAULT_8afa2_00004 | PENDING  |       |   32 |  256 |  512 | 0.0423823   |           64 |\n",
      "| DEFAULT_8afa2_00005 | PENDING  |       |   16 |   32 |  128 | 0.000219416 |            4 |\n",
      "| DEFAULT_8afa2_00006 | PENDING  |       |  256 |  256 |  256 | 0.0398676   |           32 |\n",
      "| DEFAULT_8afa2_00007 | PENDING  |       |   64 |    8 |  128 | 0.0171225   |           16 |\n",
      "| DEFAULT_8afa2_00008 | PENDING  |       |   64 |    8 |   32 | 0.000220987 |            8 |\n",
      "| DEFAULT_8afa2_00009 | PENDING  |       |    4 |  512 |   64 | 0.000429127 |            4 |\n",
      "+---------------------+----------+-------+------+------+------+-------------+--------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=203854)\u001b[0m [1,    10] loss: 1.143\n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [1,    10] loss: 1.546\n",
      "\u001b[2m\u001b[36m(pid=203854)\u001b[0m [1,    20] loss: 0.484\n",
      "Result for DEFAULT_8afa2_00000:\n",
      "  accuracy: 0.19047619047619047\n",
      "  date: 2021-09-15_14-29-18\n",
      "  done: false\n",
      "  experiment_id: 900f12241f334ec48c3911e0d2c3b343\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.098612093925476\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 203850\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.7672183513641357\n",
      "  time_this_iter_s: 0.7672183513641357\n",
      "  time_total_s: 0.7672183513641357\n",
      "  timestamp: 1631708958\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=203854)\u001b[0m [1,    30] loss: 0.312\n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [1,    10] loss: 1.334\n",
      "\u001b[2m\u001b[36m(pid=205217)\u001b[0m [1,    10] loss: 1.055\n",
      "\u001b[2m\u001b[36m(pid=205216)\u001b[0m [1,    10] loss: 1.113\n",
      "\u001b[2m\u001b[36m(pid=205210)\u001b[0m [1,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [1,    10] loss: 0.942\n",
      "\u001b[2m\u001b[36m(pid=205217)\u001b[0m [1,    20] loss: 0.453\n",
      "Result for DEFAULT_8afa2_00004:\n",
      "  accuracy: 0.2108843537414966\n",
      "  date: 2021-09-15_14-29-19\n",
      "  done: true\n",
      "  experiment_id: 275a039ac013407ca04ce0cdbda8c8e2\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.1079031626383464\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205208\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.6859235763549805\n",
      "  time_this_iter_s: 0.6859235763549805\n",
      "  time_total_s: 0.6859235763549805\n",
      "  timestamp: 1631708959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00004\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [2,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=205210)\u001b[0m [1,    20] loss: 0.549\n",
      "Result for DEFAULT_8afa2_00007:\n",
      "  accuracy: 0.7278911564625851\n",
      "  date: 2021-09-15_14-29-19\n",
      "  done: false\n",
      "  experiment_id: d80abaf1cb75431e80bb6883623c7d28\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.7362619698047638\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205213\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.8868317604064941\n",
      "  time_this_iter_s: 0.8868317604064941\n",
      "  time_total_s: 0.8868317604064941\n",
      "  timestamp: 1631708959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00007\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=205217)\u001b[0m [1,    30] loss: 0.325\n",
      "\u001b[2m\u001b[36m(pid=205210)\u001b[0m [1,    30] loss: 0.366\n",
      "Result for DEFAULT_8afa2_00006:\n",
      "  accuracy: 0.21768707482993196\n",
      "  date: 2021-09-15_14-29-19\n",
      "  done: true\n",
      "  experiment_id: fc8c1b1852d944159a165ee0f6e78a89\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.09861216545105\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205212\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.8965137004852295\n",
      "  time_this_iter_s: 0.8965137004852295\n",
      "  time_total_s: 0.8965137004852295\n",
      "  timestamp: 1631708959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00006\n",
      "  \n",
      "Result for DEFAULT_8afa2_00002:\n",
      "  accuracy: 0.7006802721088435\n",
      "  date: 2021-09-15_14-29-19\n",
      "  done: false\n",
      "  experiment_id: bc60f548b89045f08d5253b8e4d21817\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8191492080688476\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205204\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.0795955657958984\n",
      "  time_this_iter_s: 1.0795955657958984\n",
      "  time_total_s: 1.0795955657958984\n",
      "  timestamp: 1631708959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00002\n",
      "  \n",
      "Result for DEFAULT_8afa2_00003:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-15_14-29-19\n",
      "  done: false\n",
      "  experiment_id: fc8a2a4ea8854c668b43101b6a6529a6\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.826191644919546\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205206\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.1179258823394775\n",
      "  time_this_iter_s: 1.1179258823394775\n",
      "  time_total_s: 1.1179258823394775\n",
      "  timestamp: 1631708959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00003\n",
      "  \n",
      "Result for DEFAULT_8afa2_00008:\n",
      "  accuracy: 0.22448979591836735\n",
      "  date: 2021-09-15_14-29-19\n",
      "  done: true\n",
      "  experiment_id: bd98079544044b2fbd9a0911bbd55430\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205216\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.1579325199127197\n",
      "  time_this_iter_s: 1.1579325199127197\n",
      "  time_total_s: 1.1579325199127197\n",
      "  timestamp: 1631708959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00008\n",
      "  \n",
      "Result for DEFAULT_8afa2_00001:\n",
      "  accuracy: 0.6802721088435374\n",
      "  date: 2021-09-15_14-29-19\n",
      "  done: false\n",
      "  experiment_id: ee7058d6bad34e6284102dd3b3eed1b7\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8713830991371257\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 203854\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.889538288116455\n",
      "  time_this_iter_s: 1.889538288116455\n",
      "  time_total_s: 1.889538288116455\n",
      "  timestamp: 1631708959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00001\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [2,    10] loss: 0.835\n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [2,    10] loss: 0.899\n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [3,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=203854)\u001b[0m [2,    10] loss: 0.821\n",
      "Result for DEFAULT_8afa2_00009:\n",
      "  accuracy: 0.6598639455782312\n",
      "  date: 2021-09-15_14-29-20\n",
      "  done: false\n",
      "  experiment_id: 7010cc839ec44711ac13601d91f822d4\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8964105135685688\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205217\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.61541748046875\n",
      "  time_this_iter_s: 1.61541748046875\n",
      "  time_total_s: 1.61541748046875\n",
      "  timestamp: 1631708960\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00009\n",
      "  \n",
      "Result for DEFAULT_8afa2_00005:\n",
      "  accuracy: 0.21768707482993196\n",
      "  date: 2021-09-15_14-29-20\n",
      "  done: true\n",
      "  experiment_id: eb8a0d4f5ce8421c8085c668c846794d\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205210\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.652113914489746\n",
      "  time_this_iter_s: 1.652113914489746\n",
      "  time_total_s: 1.652113914489746\n",
      "  timestamp: 1631708960\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 8afa2_00005\n",
      "  \n",
      "Result for DEFAULT_8afa2_00002:\n",
      "  accuracy: 0.7006802721088435\n",
      "  date: 2021-09-15_14-29-20\n",
      "  done: true\n",
      "  experiment_id: bc60f548b89045f08d5253b8e4d21817\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.7939288020133972\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205204\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.8919143676757812\n",
      "  time_this_iter_s: 0.8123188018798828\n",
      "  time_total_s: 1.8919143676757812\n",
      "  timestamp: 1631708960\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 8afa2_00002\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=203854)\u001b[0m [2,    20] loss: 0.453\n",
      "\u001b[2m\u001b[36m(pid=205217)\u001b[0m [2,    10] loss: 0.878\n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [3,    10] loss: 0.758\n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [4,    10] loss: 1.099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=203854)\u001b[0m [2,    30] loss: 0.300\n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [3,    10] loss: 0.904\n",
      "\u001b[2m\u001b[36m(pid=205217)\u001b[0m [2,    20] loss: 0.390\n",
      "\u001b[2m\u001b[36m(pid=205217)\u001b[0m [2,    30] loss: 0.331\n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [5,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [4,    10] loss: 0.807\n",
      "Result for DEFAULT_8afa2_00001:\n",
      "  accuracy: 0.6802721088435374\n",
      "  date: 2021-09-15_14-29-21\n",
      "  done: true\n",
      "  experiment_id: ee7058d6bad34e6284102dd3b3eed1b7\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.8466102754747545\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 203854\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 3.3572261333465576\n",
      "  time_this_iter_s: 1.4676878452301025\n",
      "  time_total_s: 3.3572261333465576\n",
      "  timestamp: 1631708961\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 8afa2_00001\n",
      "  \n",
      "Result for DEFAULT_8afa2_00009:\n",
      "  accuracy: 0.6598639455782312\n",
      "  date: 2021-09-15_14-29-21\n",
      "  done: true\n",
      "  experiment_id: 7010cc839ec44711ac13601d91f822d4\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.8765688602988785\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205217\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.6794052124023438\n",
      "  time_this_iter_s: 1.0639877319335938\n",
      "  time_total_s: 2.6794052124023438\n",
      "  timestamp: 1631708961\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 8afa2_00009\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [4,    10] loss: 0.820\n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [6,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [5,    10] loss: 0.758\n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [5,    10] loss: 0.759\n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [7,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [6,    10] loss: 0.831\n",
      "== Status ==\n",
      "Memory usage on this node: 5.9/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: None | Iter 4.000: -0.7917663925572446 | Iter 2.000: -0.8202695387440759 | Iter 1.000: -0.9975113037470225\n",
      "Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/6.37 GiB heap, 0.0/3.19 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/DEFAULT_2021-09-15_14-29-16\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+---------------------+------------+--------------------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "| Trial name          | status     | loc                |   l1 |   l2 |   l3 |          lr |   batch_size |     loss |   accuracy |   training_iteration |\n",
      "|---------------------+------------+--------------------+------+------+------+-------------+--------------+----------+------------+----------------------|\n",
      "| DEFAULT_8afa2_00000 | RUNNING    | 10.67.95.37:203850 |   64 |   16 |    8 | 0.0662326   |           16 | 1.09861  |   0.190476 |                    7 |\n",
      "| DEFAULT_8afa2_00003 | RUNNING    | 10.67.95.37:205206 |    4 |  512 |    8 | 0.0738792   |            8 | 0.791766 |   0.707483 |                    4 |\n",
      "| DEFAULT_8afa2_00007 | RUNNING    | 10.67.95.37:205213 |   64 |    8 |  128 | 0.0171225   |           16 | 0.825474 |   0.727891 |                    5 |\n",
      "| DEFAULT_8afa2_00001 | TERMINATED |                    |  128 |  128 |   64 | 0.00227478  |            4 | 0.84661  |   0.680272 |                    2 |\n",
      "| DEFAULT_8afa2_00002 | TERMINATED |                    |  512 |   16 |  512 | 0.0001217   |           32 | 0.793929 |   0.70068  |                    2 |\n",
      "| DEFAULT_8afa2_00004 | TERMINATED |                    |   32 |  256 |  512 | 0.0423823   |           64 | 1.1079   |   0.210884 |                    1 |\n",
      "| DEFAULT_8afa2_00005 | TERMINATED |                    |   16 |   32 |  128 | 0.000219416 |            4 | 1.09861  |   0.217687 |                    1 |\n",
      "| DEFAULT_8afa2_00006 | TERMINATED |                    |  256 |  256 |  256 | 0.0398676   |           32 | 1.09861  |   0.217687 |                    1 |\n",
      "| DEFAULT_8afa2_00008 | TERMINATED |                    |   64 |    8 |   32 | 0.000220987 |            8 | 1.09861  |   0.22449  |                    1 |\n",
      "| DEFAULT_8afa2_00009 | TERMINATED |                    |    4 |  512 |   64 | 0.000429127 |            4 | 0.876569 |   0.659864 |                    2 |\n",
      "+---------------------+------------+--------------------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [8,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [6,    10] loss: 0.881\n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [7,    10] loss: 0.831\n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [9,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [8,    10] loss: 0.787\n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [7,    10] loss: 0.843\n",
      "\u001b[2m\u001b[36m(pid=203850)\u001b[0m [10,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [9,    10] loss: 0.834\n",
      "Result for DEFAULT_8afa2_00000:\n",
      "  accuracy: 0.19047619047619047\n",
      "  date: 2021-09-15_14-29-23\n",
      "  done: true\n",
      "  experiment_id: 900f12241f334ec48c3911e0d2c3b343\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 1.098612093925476\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 203850\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 5.66039776802063\n",
      "  time_this_iter_s: 0.4753258228302002\n",
      "  time_total_s: 5.66039776802063\n",
      "  timestamp: 1631708963\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8afa2_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [8,    10] loss: 0.860\n",
      "\u001b[2m\u001b[36m(pid=205213)\u001b[0m [10,    10] loss: 0.783\n",
      "Result for DEFAULT_8afa2_00007:\n",
      "  accuracy: 0.7278911564625851\n",
      "  date: 2021-09-15_14-29-24\n",
      "  done: true\n",
      "  experiment_id: d80abaf1cb75431e80bb6883623c7d28\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.7413163214921952\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205213\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 5.552034616470337\n",
      "  time_this_iter_s: 0.45941948890686035\n",
      "  time_total_s: 5.552034616470337\n",
      "  timestamp: 1631708964\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8afa2_00007\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [9,    10] loss: 0.860\n",
      "\u001b[2m\u001b[36m(pid=205206)\u001b[0m [10,    10] loss: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 14:29:25,315\tINFO tune.py:561 -- Total run time: 8.52 seconds (8.37 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DEFAULT_8afa2_00003:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-15_14-29-25\n",
      "  done: true\n",
      "  experiment_id: fc8a2a4ea8854c668b43101b6a6529a6\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.7926494369381353\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 205206\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 6.602008581161499\n",
      "  time_this_iter_s: 0.5115988254547119\n",
      "  time_total_s: 6.602008581161499\n",
      "  timestamp: 1631708965\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 8afa2_00003\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 5.5/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 8.000: -0.8274913853720615 | Iter 4.000: -0.7917663925572446 | Iter 2.000: -0.8202695387440759 | Iter 1.000: -0.9975113037470225\n",
      "Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/6.37 GiB heap, 0.0/3.19 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/DEFAULT_2021-09-15_14-29-16\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "| Trial name          | status     | loc   |   l1 |   l2 |   l3 |          lr |   batch_size |     loss |   accuracy |   training_iteration |\n",
      "|---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------|\n",
      "| DEFAULT_8afa2_00000 | TERMINATED |       |   64 |   16 |    8 | 0.0662326   |           16 | 1.09861  |   0.190476 |                   10 |\n",
      "| DEFAULT_8afa2_00001 | TERMINATED |       |  128 |  128 |   64 | 0.00227478  |            4 | 0.84661  |   0.680272 |                    2 |\n",
      "| DEFAULT_8afa2_00002 | TERMINATED |       |  512 |   16 |  512 | 0.0001217   |           32 | 0.793929 |   0.70068  |                    2 |\n",
      "| DEFAULT_8afa2_00003 | TERMINATED |       |    4 |  512 |    8 | 0.0738792   |            8 | 0.792649 |   0.707483 |                   10 |\n",
      "| DEFAULT_8afa2_00004 | TERMINATED |       |   32 |  256 |  512 | 0.0423823   |           64 | 1.1079   |   0.210884 |                    1 |\n",
      "| DEFAULT_8afa2_00005 | TERMINATED |       |   16 |   32 |  128 | 0.000219416 |            4 | 1.09861  |   0.217687 |                    1 |\n",
      "| DEFAULT_8afa2_00006 | TERMINATED |       |  256 |  256 |  256 | 0.0398676   |           32 | 1.09861  |   0.217687 |                    1 |\n",
      "| DEFAULT_8afa2_00007 | TERMINATED |       |   64 |    8 |  128 | 0.0171225   |           16 | 0.741316 |   0.727891 |                   10 |\n",
      "| DEFAULT_8afa2_00008 | TERMINATED |       |   64 |    8 |   32 | 0.000220987 |            8 | 1.09861  |   0.22449  |                    1 |\n",
      "| DEFAULT_8afa2_00009 | TERMINATED |       |    4 |  512 |   64 | 0.000429127 |            4 | 0.876569 |   0.659864 |                    2 |\n",
      "+---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "\n",
      "\n",
      "Best trial config: {'l1': 64, 'l2': 8, 'l3': 128, 'lr': 0.017122493231773332, 'batch_size': 16}\n",
      "Best trial final validation loss: 0.7413163214921952\n",
      "Best trial final validation accuracy: 0.7278911564625851\n",
      "Best trial test set accuracy: 0.7021276595744681\n"
     ]
    }
   ],
   "source": [
    "main(num_samples=10, max_num_epochs=10, gpus_per_trial=gpus_per_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn-env]",
   "language": "python",
   "name": "conda-env-cnn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
