{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from functools import partial as func_partial\n",
    "from functools import reduce as func_reduce\n",
    "from operator import mul as op_mul\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from os import cpu_count, path\n",
    "from time import strftime\n",
    "\n",
    "#This class contains DatasetClass and several helper functions\n",
    "import ClassModule as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU's: 12\n"
     ]
    }
   ],
   "source": [
    "# Show number of avlaible CPU threads\n",
    "# With mulithreading this number is twice the number of physical cores\n",
    "cpu_av = cpu_count()\n",
    "print(\"Number of available CPU's: {}\".format(cpu_av))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number CPUS that should be used per trial and dataloader\n",
    "# If set to 1 number of cucurrent training networking is equal to this number\n",
    "# In case of training with GPU this will be limited to number of models training simultaneously on GPU\n",
    "# So number of CPU threads for each trial can be increased \n",
    "cpus_per_trial = 1\n",
    "gpus_per_trial = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(train_ds, val_ds, bs):\n",
    "    dl_train = utils.DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=cpus_per_trial-1)\n",
    "    dl_val = utils.DataLoader(val_ds, batch_size=bs * 2, shuffle=True, num_workers=cpus_per_trial-1)\n",
    "    return  dl_train, dl_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1610.04490\n",
    "INSTANCE_NOISE = False\n",
    "\n",
    "def add_instance_noise(data, std=0.01):\n",
    "    return data + torch.distributions.Normal(0, std).sample(data.shape).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, l1=100, l2=50, l3=25, input_dim=(2,20,20), num_in_features=5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.feature_ext = nn.Sequential(\n",
    "            nn.Conv2d(2,10, kernel_size=3, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,10, kernel_size=3,  padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,10, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,5, kernel_size=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Gives the number of features after the conv layer\n",
    "        num_features_after_conv = func_reduce(op_mul, list(self.feature_ext(torch.rand(1, *input_dim)).shape))\n",
    "        \n",
    "        self.dense_nn = nn.Sequential(\n",
    "            nn.Linear(num_features_after_conv + num_in_features, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3,3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, cluster, clusNumXYEPt):\n",
    "        cluster = self.feature_ext(cluster)\n",
    "        x = self.flatten(cluster)\n",
    "        x = torch.cat([x, clusNumXYEPt], dim=1)\n",
    "        logits = self.dense_nn(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement train and validation loop\n",
    "Data[0] contains an image of of the cell energies and timings. <br>\n",
    "Data[1] contains all features in a dict. Their shapes have to be changed from [batch_size] to [batch_size,1] for input into linear layers, implemented via function here <br>\n",
    "Data[2] contains all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "\n",
    "    for batch, Data in enumerate(dataloader):\n",
    "        Clusters = Data[0].to(device)\n",
    "        Features = cm.unsqueeze_features(Data[1])\n",
    "        Labels = Data[2]\n",
    "        \n",
    "        ClusterProperties = torch.cat([Features[\"ClusterE\"], Features[\"ClusterPt\"], Features[\"ClusterM02\"]\n",
    "                                      , Features[\"ClusterM20\"], Features[\"ClusterDist\"]], dim=1)\n",
    "        ClusterProperties.to(device)\n",
    "        \n",
    "        if INSTANCE_NOISE:\n",
    "            Clusters = add_instance_noise(Clusters)\n",
    "        \n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction and loss\n",
    "        pred = model(Clusters, ClusterProperties)\n",
    "        loss = loss_fn(pred, Labels[\"PartPID\"].long())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        \n",
    "        if batch % 2000 == 1999:\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, batch + 1,\n",
    "                                            running_loss / epoch_steps))\n",
    "            running_loss = 0.0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, Data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            Clusters = Data[0].to(device)\n",
    "            Features = cm.unsqueeze_features(Data[1])\n",
    "            Labels = Data[2]\n",
    "            ClusterProperties = torch.cat([Features[\"ClusterE\"], Features[\"ClusterPt\"], Features[\"ClusterM02\"]\n",
    "                                      , Features[\"ClusterM20\"], Features[\"ClusterDist\"]], dim=1)           \n",
    "            ClusterProperties.to(device)\n",
    "            \n",
    "            pred = model(Clusters, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == Labels[\"PartPID\"]).type(torch.float).sum().item()\n",
    "\n",
    "            loss = loss_fn(pred, Labels[\"PartPID\"].long())#.item()\n",
    "            val_loss += loss.cpu().numpy()\n",
    "            val_steps += 1\n",
    "    \n",
    "    with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "        _path = path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), _path)\n",
    "        \n",
    "    tune.report(loss=(val_loss / val_steps), accuracy= correct / size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement method for accuracy testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, device=\"cpu\"):\n",
    "    \n",
    "    dataset_test = cm.load_data_test()\n",
    "    \n",
    "    dataloader_test = utils.DataLoader(\n",
    "        dataset_test, batch_size=4, shuffle=False, num_workers=2)\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(dataloader_test.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, Data in enumerate(dataloader_test):\n",
    "            Clusters = Data[0].to(device)\n",
    "            Features = cm.unsqueeze_features(Data[1])\n",
    "            Labels = Data[2]\n",
    "            ClusterProperties = torch.cat([Features[\"ClusterE\"], Features[\"ClusterPt\"], Features[\"ClusterM02\"]\n",
    "                                      , Features[\"ClusterM20\"], Features[\"ClusterDist\"]], dim=1)            \n",
    "            ClusterProperties.to(device)\n",
    "            \n",
    "            \n",
    "            pred = model(Clusters, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == Labels[\"PartPID\"]).type(torch.float).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, checkpoint_dir=None):\n",
    "    \n",
    "    # load model\n",
    "    model = CNN(config[\"l1\"],config[\"l2\"],config[\"l3\"])\n",
    "    \n",
    "    # check for avlaible resource and initialize device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    # send model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # initialise loss function and opptimizer\n",
    "    loss_fn = F.cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    \n",
    "    # check whether checkpoint is available\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "    # load dataset\n",
    "    dataset_train = cm.load_data_train()\n",
    "    \n",
    "    # split trainset in train and validation subsets\n",
    "    test_abs = int(len(dataset_train) * 0.8)\n",
    "    subset_train, subset_val = utils.random_split(\n",
    "        dataset_train, [test_abs, len(dataset_train) - test_abs])\n",
    "\n",
    "    # get dataloaders \n",
    "    dataloader_train, dataloader_val = get_dataloader(subset_train, subset_val, int(config[\"batch_size\"]))\n",
    "                                                      \n",
    "    for epoch in range(100):\n",
    "        train_loop(epoch, dataloader_train, model, loss_fn, optimizer, device=device)\n",
    "        val_loop(epoch, dataloader_train, model, loss_fn, optimizer, device=device)                                              \n",
    "    \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all Ray Tune functionality and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
    "    \n",
    "    # Setup hyperparameter-space to search\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"l3\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16, 32, 64])\n",
    "    }\n",
    "\n",
    "    # Init the scheduler\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    # Init the Reporter\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"l1\", \"l2\", \"l3\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    #Get Current date and time\n",
    "    timestr = strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "    name = \"ASHA-\" + timestr\n",
    "    \n",
    "    # Init the run method\n",
    "    result = tune.run(\n",
    "        func_partial(train_model),\n",
    "        name = name,\n",
    "        resources_per_trial={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        local_dir = \"./Ray_Results\",\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "    \n",
    "    # Find best trial and use it on the testset\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    best_trained_model = CNN(best_trial.config[\"l1\"], best_trial.config[\"l2\"], best_trial.config[\"l3\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "    \n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhonerma/anaconda3/envs/cnn-env/lib/python3.9/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-09-22 11:05:06,245\tWARNING experiment.py:295 -- No name detected on trainable. Using DEFAULT.\n",
      "2021-09-22 11:05:06,246\tINFO registry.py:66 -- Detected unknown callable for trainable. Converting to class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 5.6/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/12 CPUs, 0/0 GPUs, 0.0/6.43 GiB heap, 0.0/3.22 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_22-11:05:05\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+---------------------+----------+-------+------+------+------+-------------+--------------+\n",
      "| Trial name          | status   | loc   |   l1 |   l2 |   l3 |          lr |   batch_size |\n",
      "|---------------------+----------+-------+------+------+------+-------------+--------------|\n",
      "| DEFAULT_2df8e_00000 | RUNNING  |       |  128 |   16 |   16 | 0.000121351 |            4 |\n",
      "| DEFAULT_2df8e_00001 | PENDING  |       |  512 |    8 |    4 | 0.000557998 |           64 |\n",
      "| DEFAULT_2df8e_00002 | PENDING  |       |  512 |   64 |   64 | 0.0334072   |           64 |\n",
      "| DEFAULT_2df8e_00003 | PENDING  |       |    4 |    8 |   32 | 0.0059895   |            8 |\n",
      "| DEFAULT_2df8e_00004 | PENDING  |       |  256 |    4 |  256 | 0.00777479  |           64 |\n",
      "| DEFAULT_2df8e_00005 | PENDING  |       |    8 |  256 |   64 | 0.00362587  |            4 |\n",
      "| DEFAULT_2df8e_00006 | PENDING  |       |   64 |   16 |   16 | 0.000190474 |            8 |\n",
      "| DEFAULT_2df8e_00007 | PENDING  |       |    4 |   64 |    4 | 0.0160797   |            2 |\n",
      "| DEFAULT_2df8e_00008 | PENDING  |       |    8 |   16 |   32 | 0.0760695   |            8 |\n",
      "| DEFAULT_2df8e_00009 | PENDING  |       |  256 |  512 |   16 | 0.0190998   |            2 |\n",
      "+---------------------+----------+-------+------+------+------+-------------+--------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [1,    10] loss: 0.999\n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [1,    20] loss: 0.440\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [1,    10] loss: 1.067\n",
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [1,    10] loss: 0.915\n",
      "\u001b[2m\u001b[36m(pid=19042)\u001b[0m [1,    10] loss: 1.141\n",
      "\u001b[2m\u001b[36m(pid=19052)\u001b[0m [1,    10] loss: 1.175\n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [1,    30] loss: 0.276\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [1,    20] loss: 0.495\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [1,    30] loss: 0.295\n",
      "\u001b[2m\u001b[36m(pid=19052)\u001b[0m [1,    20] loss: 0.585\n",
      "\u001b[2m\u001b[36m(pid=19044)\u001b[0m [1,    10] loss: 1.202\n",
      "Result for DEFAULT_2df8e_00004:\n",
      "  accuracy: 0.6938775510204082\n",
      "  date: 2021-09-22_11-05-07\n",
      "  done: false\n",
      "  experiment_id: 301a18c9ebe740ccb4d9a7dec893660b\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.895086387793223\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19041\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.36458373069763184\n",
      "  time_this_iter_s: 0.36458373069763184\n",
      "  time_total_s: 0.36458373069763184\n",
      "  timestamp: 1632301507\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00004\n",
      "  \n",
      "Result for DEFAULT_2df8e_00001:\n",
      "  accuracy: 0.7142857142857143\n",
      "  date: 2021-09-22_11-05-07\n",
      "  done: true\n",
      "  experiment_id: 24b388d8be764b39b42481a589707a64\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.9707675774892172\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19048\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.4344174861907959\n",
      "  time_this_iter_s: 0.4344174861907959\n",
      "  time_total_s: 0.4344174861907959\n",
      "  timestamp: 1632301507\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00001\n",
      "  \n",
      "Result for DEFAULT_2df8e_00003:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-22_11-05-07\n",
      "  done: false\n",
      "  experiment_id: a37d822791884833896183d8010a7d89\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.779373763423217\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19046\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.39531564712524414\n",
      "  time_this_iter_s: 0.39531564712524414\n",
      "  time_total_s: 0.39531564712524414\n",
      "  timestamp: 1632301507\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00003\n",
      "  \n",
      "Result for DEFAULT_2df8e_00002:\n",
      "  accuracy: 0.23129251700680273\n",
      "  date: 2021-09-22_11-05-07\n",
      "  done: true\n",
      "  experiment_id: 47cefed79b0348e88fae2a87d799fddd\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986120700836182\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19051\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.4299309253692627\n",
      "  time_this_iter_s: 0.4299309253692627\n",
      "  time_total_s: 0.4299309253692627\n",
      "  timestamp: 1632301507\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00002\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [1,    40] loss: 0.232\n",
      "\u001b[2m\u001b[36m(pid=19052)\u001b[0m [1,    30] loss: 0.373\n",
      "\u001b[2m\u001b[36m(pid=19050)\u001b[0m [1,    10] loss: 322.206\n",
      "Result for DEFAULT_2df8e_00005:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-22_11-05-07\n",
      "  done: false\n",
      "  experiment_id: be0cf6da905648e5a9aa5209ef260056\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8004729933029896\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19047\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.49846553802490234\n",
      "  time_this_iter_s: 0.49846553802490234\n",
      "  time_total_s: 0.49846553802490234\n",
      "  timestamp: 1632301507\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00005\n",
      "  \n",
      "Result for DEFAULT_2df8e_00008:\n",
      "  accuracy: 0.2108843537414966\n",
      "  date: 2021-09-22_11-05-07\n",
      "  done: true\n",
      "  experiment_id: 43612af7a4874ec6b678fcdcafc115de\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19044\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.36346936225891113\n",
      "  time_this_iter_s: 0.36346936225891113\n",
      "  time_total_s: 0.36346936225891113\n",
      "  timestamp: 1632301507\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00008\n",
      "  \n",
      "Result for DEFAULT_2df8e_00006:\n",
      "  accuracy: 0.08843537414965986\n",
      "  date: 2021-09-22_11-05-07\n",
      "  done: true\n",
      "  experiment_id: bb29d77cc7594cc4b9d8190c9e7380e9\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0854350767637555\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19042\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.47139644622802734\n",
      "  time_this_iter_s: 0.47139644622802734\n",
      "  time_total_s: 0.47139644622802734\n",
      "  timestamp: 1632301507\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00006\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [1,    50] loss: 0.160\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [1,    60] loss: 0.136\n",
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [2,    10] loss: 0.746\n",
      "Result for DEFAULT_2df8e_00000:\n",
      "  accuracy: 0.25170068027210885\n",
      "  date: 2021-09-22_11-05-08\n",
      "  done: true\n",
      "  experiment_id: 38a17f420a6d4e59ab247c78f764392b\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19052\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.6540827751159668\n",
      "  time_this_iter_s: 0.6540827751159668\n",
      "  time_total_s: 0.6540827751159668\n",
      "  timestamp: 1632301508\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [2,    10] loss: 0.878\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [1,    70] loss: 0.115\n",
      "\u001b[2m\u001b[36m(pid=19050)\u001b[0m [1,    20] loss: 0.549\n",
      "Result for DEFAULT_2df8e_00004:\n",
      "  accuracy: 0.6938775510204082\n",
      "  date: 2021-09-22_11-05-08\n",
      "  done: true\n",
      "  experiment_id: 301a18c9ebe740ccb4d9a7dec893660b\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.8512945771217346\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19041\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.6476261615753174\n",
      "  time_this_iter_s: 0.28304243087768555\n",
      "  time_total_s: 0.6476261615753174\n",
      "  timestamp: 1632301508\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 2df8e_00004\n",
      "  \n",
      "Result for DEFAULT_2df8e_00007:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-22_11-05-08\n",
      "  done: false\n",
      "  experiment_id: 6213cbb1b82b461b98eaa5e36bce0533\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8346403015626443\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19049\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.6196880340576172\n",
      "  time_this_iter_s: 0.6196880340576172\n",
      "  time_total_s: 0.6196880340576172\n",
      "  timestamp: 1632301508\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00007\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [2,    20] loss: 0.353\n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [2,    30] loss: 0.305\n",
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [3,    10] loss: 0.873\n",
      "\u001b[2m\u001b[36m(pid=19050)\u001b[0m [1,    30] loss: 0.366\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [2,    10] loss: 1.057\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [2,    20] loss: 0.341\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [2,    30] loss: 0.261\n",
      "\u001b[2m\u001b[36m(pid=19050)\u001b[0m [1,    40] loss: 0.275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [3,    10] loss: 0.695\n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [3,    20] loss: 0.495\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [2,    40] loss: 0.193\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [2,    50] loss: 0.153\n",
      "\u001b[2m\u001b[36m(pid=19050)\u001b[0m [1,    50] loss: 0.220\n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [3,    30] loss: 0.299\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [2,    60] loss: 0.127\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [2,    70] loss: 0.137\n",
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [4,    10] loss: 0.883\n",
      "\u001b[2m\u001b[36m(pid=19050)\u001b[0m [1,    60] loss: 0.183\n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [4,    10] loss: 0.814\n",
      "\u001b[2m\u001b[36m(pid=19050)\u001b[0m [1,    70] loss: 0.157\n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [4,    20] loss: 0.357\n",
      "\u001b[2m\u001b[36m(pid=19047)\u001b[0m [4,    30] loss: 0.288\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [3,    10] loss: 1.026\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [3,    20] loss: 0.345\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [3,    30] loss: 0.342\n",
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [5,    10] loss: 0.755\n",
      "Result for DEFAULT_2df8e_00009:\n",
      "  accuracy: 0.19727891156462585\n",
      "  date: 2021-09-22_11-05-08\n",
      "  done: true\n",
      "  experiment_id: 4ebe78960e474fae960e9e51bc552faf\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19050\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.2221853733062744\n",
      "  time_this_iter_s: 1.2221853733062744\n",
      "  time_total_s: 1.2221853733062744\n",
      "  timestamp: 1632301508\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 2df8e_00009\n",
      "  \n",
      "Result for DEFAULT_2df8e_00005:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-22_11-05-08\n",
      "  done: true\n",
      "  experiment_id: be0cf6da905648e5a9aa5209ef260056\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 4\n",
      "  loss: 0.829091885605374\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19047\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.390075445175171\n",
      "  time_this_iter_s: 0.25377845764160156\n",
      "  time_total_s: 1.390075445175171\n",
      "  timestamp: 1632301508\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 2df8e_00005\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [3,    40] loss: 0.138\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [3,    50] loss: 0.150\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [3,    60] loss: 0.100\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [3,    70] loss: 0.138\n",
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [6,    10] loss: 0.833\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [4,    10] loss: 0.820\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [4,    20] loss: 0.372\n",
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [7,    10] loss: 0.778\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [4,    30] loss: 0.273\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [4,    40] loss: 0.205\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [4,    50] loss: 0.149\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [4,    60] loss: 0.150\n",
      "\u001b[2m\u001b[36m(pid=19049)\u001b[0m [4,    70] loss: 0.117\n",
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [8,    10] loss: 0.767\n",
      "Result for DEFAULT_2df8e_00007:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-22_11-05-09\n",
      "  done: true\n",
      "  experiment_id: 6213cbb1b82b461b98eaa5e36bce0533\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 4\n",
      "  loss: 0.80441795047876\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19049\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.7684986591339111\n",
      "  time_this_iter_s: 0.36440110206604004\n",
      "  time_total_s: 1.7684986591339111\n",
      "  timestamp: 1632301509\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 2df8e_00007\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [9,    10] loss: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-22 11:05:09,891\tINFO tune.py:561 -- Total run time: 3.65 seconds (3.46 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19046)\u001b[0m [10,    10] loss: 0.669\n",
      "Result for DEFAULT_2df8e_00003:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-22_11-05-09\n",
      "  done: true\n",
      "  experiment_id: a37d822791884833896183d8010a7d89\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.7946597416149942\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 19046\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.227844715118408\n",
      "  time_this_iter_s: 0.18567919731140137\n",
      "  time_total_s: 2.227844715118408\n",
      "  timestamp: 1632301509\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 2df8e_00003\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 5.2/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 8.000: -0.7539932241565303 | Iter 4.000: -0.80441795047876 | Iter 2.000: -0.8110025585503191 | Iter 1.000: -1.0281013271264863\n",
      "Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/6.43 GiB heap, 0.0/3.22 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_22-11:05:05\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "| Trial name          | status     | loc   |   l1 |   l2 |   l3 |          lr |   batch_size |     loss |   accuracy |   training_iteration |\n",
      "|---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------|\n",
      "| DEFAULT_2df8e_00000 | TERMINATED |       |  128 |   16 |   16 | 0.000121351 |            4 | 1.09861  |  0.251701  |                    1 |\n",
      "| DEFAULT_2df8e_00001 | TERMINATED |       |  512 |    8 |    4 | 0.000557998 |           64 | 0.970768 |  0.714286  |                    1 |\n",
      "| DEFAULT_2df8e_00002 | TERMINATED |       |  512 |   64 |   64 | 0.0334072   |           64 | 1.09861  |  0.231293  |                    1 |\n",
      "| DEFAULT_2df8e_00003 | TERMINATED |       |    4 |    8 |   32 | 0.0059895   |            8 | 0.79466  |  0.707483  |                   10 |\n",
      "| DEFAULT_2df8e_00004 | TERMINATED |       |  256 |    4 |  256 | 0.00777479  |           64 | 0.851295 |  0.693878  |                    2 |\n",
      "| DEFAULT_2df8e_00005 | TERMINATED |       |    8 |  256 |   64 | 0.00362587  |            4 | 0.829092 |  0.707483  |                    4 |\n",
      "| DEFAULT_2df8e_00006 | TERMINATED |       |   64 |   16 |   16 | 0.000190474 |            8 | 1.08544  |  0.0884354 |                    1 |\n",
      "| DEFAULT_2df8e_00007 | TERMINATED |       |    4 |   64 |    4 | 0.0160797   |            2 | 0.804418 |  0.707483  |                    4 |\n",
      "| DEFAULT_2df8e_00008 | TERMINATED |       |    8 |   16 |   32 | 0.0760695   |            8 | 1.09861  |  0.210884  |                    1 |\n",
      "| DEFAULT_2df8e_00009 | TERMINATED |       |  256 |  512 |   16 | 0.0190998   |            2 | 1.09861  |  0.197279  |                    1 |\n",
      "+---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "\n",
      "\n",
      "Best trial config: {'l1': 4, 'l2': 8, 'l3': 32, 'lr': 0.005989504640133979, 'batch_size': 8}\n",
      "Best trial final validation loss: 0.7946597416149942\n",
      "Best trial final validation accuracy: 0.7074829931972789\n",
      "Best trial test set accuracy: 0.7021276595744681\n"
     ]
    }
   ],
   "source": [
    "main(num_samples=10, max_num_epochs=10, gpus_per_trial=gpus_per_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn-env]",
   "language": "python",
   "name": "conda-env-cnn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
