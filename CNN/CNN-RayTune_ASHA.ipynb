{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from functools import partial as func_partial\n",
    "from functools import reduce as func_reduce\n",
    "from operator import mul as op_mul\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from os import cpu_count, path\n",
    "from time import strftime\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "#This class contains DatasetClass and several helper functions\n",
    "import ClassModule as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU's: 6\n"
     ]
    }
   ],
   "source": [
    "# Show number of avlaible CPU threads\n",
    "# With mulithreading this number is twice the number of physical cores\n",
    "cpu_av = cpu_count()\n",
    "print(\"Number of available CPU's: {}\".format(cpu_av))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number CPUS that should be used per trial and dataloader\n",
    "# If set to 1 number of cucurrent training networking is equal to this number\n",
    "# In case of training with GPU this will be limited to number of models training simultaneously on GPU\n",
    "# So number of CPU threads for each trial can be increased \n",
    "cpus_per_trial = 2\n",
    "gpus_per_trial = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(train_ds, val_ds, bs):\n",
    "    \n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    #with FileLock(path.expanduser(\"/media/DATA/ML-Notebooks/CNN/Data/.data.lock\")):\n",
    "    dl_train = utils.DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=cpus_per_trial-1)\n",
    "    dl_val = utils.DataLoader(val_ds, batch_size=bs * 2, shuffle=True, num_workers=cpus_per_trial-1)\n",
    "    \n",
    "    return  dl_train, dl_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1610.04490\n",
    "INSTANCE_NOISE = True\n",
    "\n",
    "def add_instance_noise(data, device, std=0.01):\n",
    "    return data + torch.distributions.Normal(0, std).sample(data.shape).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, l1=100, l2=50, l3=25, input_dim=(2,20,20), num_in_features=5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.feature_ext = nn.Sequential(\n",
    "            nn.Conv2d(2,10, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,10, kernel_size=5, padding=0),\n",
    "            nn.ReLU(),  \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(10,10, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,6, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Gives the number of features after the conv layer\n",
    "        num_features_after_conv = func_reduce(op_mul, list(self.feature_ext(torch.rand(1, *input_dim)).shape))\n",
    "        \n",
    "        self.dense_nn = nn.Sequential(\n",
    "            nn.Linear(num_features_after_conv + num_in_features, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3,3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, cluster, clusNumXYEPt):\n",
    "        cluster = self.feature_ext(cluster)\n",
    "        x = self.flatten(cluster)\n",
    "        x = torch.cat([x, clusNumXYEPt], dim=1)\n",
    "        logits = self.dense_nn(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement train and validation loop\n",
    "Data[0] contains an image of of the cell energies and timings. <br>\n",
    "Data[1] contains all features in a dict. Their shapes have to be changed from [batch_size] to [batch_size,1] for input into linear layers, implemented via function here <br>\n",
    "Data[2] contains all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "\n",
    "    for batch, Data in enumerate(dataloader):\n",
    "        Clusters = Data[0].to(device)\n",
    "        Features = cm.unsqueeze_features(Data[1])\n",
    "        Labels = Data[2]\n",
    "        \n",
    "        ClusterProperties = torch.cat([Features[\"ClusterE\"], Features[\"ClusterPt\"], Features[\"ClusterM02\"]\n",
    "                                      , Features[\"ClusterM20\"], Features[\"ClusterDist\"]], dim=1).to(device)\n",
    "        #Labels = torch.cat([Labels[\"PartPID\"], dim=1]).to(device)\n",
    "        Label = Labels[\"PartPID\"].to(device)\n",
    "        \n",
    "        if INSTANCE_NOISE:\n",
    "            Clusters = add_instance_noise(Clusters, device)\n",
    "        \n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction and loss\n",
    "        pred = model(Clusters, ClusterProperties)\n",
    "        loss = loss_fn(pred, Label.long())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        \n",
    "        if batch % 100000 == 99999:\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, batch + 1,\n",
    "                                            running_loss / epoch_steps))\n",
    "            running_loss = 0.0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, Data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            Clusters = Data[0].to(device)\n",
    "            Features = cm.unsqueeze_features(Data[1])\n",
    "            Labels = Data[2]\n",
    "            ClusterProperties = torch.cat([Features[\"ClusterE\"], Features[\"ClusterPt\"], Features[\"ClusterM02\"]\n",
    "                                      , Features[\"ClusterM20\"], Features[\"ClusterDist\"]], dim=1).to(device)           \n",
    "            #Labels = torch.cat([Labels[\"PartPID\"], dim=1]).to(device)\n",
    "            Label = Labels[\"PartPID\"].to(device)\n",
    "            \n",
    "            pred = model(Clusters, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == Label).type(torch.float).sum().item()\n",
    "\n",
    "            loss = loss_fn(pred, Label.long())#.item()\n",
    "            val_loss += loss.cpu().numpy()\n",
    "            val_steps += 1\n",
    "    \n",
    "    with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "        _path = path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), _path)\n",
    "        \n",
    "    tune.report(loss=(val_loss / val_steps), accuracy= correct / size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement method for accuracy testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, device=\"cpu\"):\n",
    "    \n",
    "    dataset_test = cm.load_data_test('/media/DATA/ML-Notebooks/CNN/Data/data_test.npz')\n",
    "    \n",
    "    dataloader_test = utils.DataLoader(\n",
    "        dataset_test, batch_size=4, shuffle=False, num_workers=cpu_av-1)\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(dataloader_test.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, Data in enumerate(dataloader_test):\n",
    "            Clusters = Data[0].to(device)\n",
    "            Features = cm.unsqueeze_features(Data[1])\n",
    "            Labels = Data[2]\n",
    "            ClusterProperties = torch.cat([Features[\"ClusterE\"], Features[\"ClusterPt\"], Features[\"ClusterM02\"]\n",
    "                                      , Features[\"ClusterM20\"], Features[\"ClusterDist\"]], dim=1).to(device)            \n",
    "            #Labels = torch.cat([Labels[\"PartPID\"], dim=1]).to(device)\n",
    "            Label = Labels[\"PartPID\"].to(device)\n",
    "            \n",
    "            \n",
    "            pred = model(Clusters, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == Label).type(torch.float).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, dataloader_train=None, dataloader_test=None, checkpoint_dir=None):\n",
    "    \n",
    "    # load model\n",
    "    model = CNN(config[\"l1\"],config[\"l2\"],config[\"l3\"])\n",
    "    \n",
    "    # check for avlaible resource and initialize device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    # send model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # initialise loss function and opptimizer\n",
    "    loss_fn = F.cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=config[\"lr\"], weight_decay=config[\"wd\"])\n",
    "    \n",
    "    # check whether checkpoint is available\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "    # load dataset\n",
    "    dataset_train = cm.load_data_train('/media/DATA/ML-Notebooks/CNN/Data/data_train.npz')\n",
    "    \n",
    "    # split trainset in train and validation subsets\n",
    "    test_abs = int(len(dataset_train) * 0.8)\n",
    "    subset_train, subset_val = utils.random_split(\n",
    "        dataset_train, [test_abs, len(dataset_train) - test_abs])\n",
    "\n",
    "    # get dataloaders \n",
    "    dataloader_train, dataloader_val = get_dataloader(subset_train, subset_val, int(config[\"batch_size\"]))\n",
    "                                                      \n",
    "    for epoch in range(100):\n",
    "        train_loop(epoch, dataloader_train, model, loss_fn, optimizer, device=device)\n",
    "        val_loop(epoch, dataloader_val, model, loss_fn, optimizer, device=device)                                              \n",
    "    \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all Ray Tune functionality and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
    "    \n",
    "    # Setup hyperparameter-space to search\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 8)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 8)),\n",
    "        \"l3\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 8)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"wd\": tune.loguniform(1e-5, 1e-3),\n",
    "        \"batch_size\": tune.choice([16, 32, 64, 128, 256])\n",
    "    }\n",
    "\n",
    "    # Init the scheduler\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    # Init the Reporter\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"l1\", \"l2\", \"l3\", \"lr\",\"wd\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    #Get Current date and time\n",
    "    timestr = strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "    name = \"ASHA-\" + timestr\n",
    "    \n",
    "    #Load the dataset for use in shared memory\n",
    "    #data = cm.load_data_train('/media/DATA/ML-Notebooks/CNN/Data/data_train.npz')\n",
    "    #test_abs = int(len(data) * 0.8)\n",
    "    #subset_train, subset_val = utils.random_split(\n",
    "    #    data, [test_abs, len(data) - test_abs])\n",
    "\n",
    "    # get dataloaders \n",
    "    #dataloader_train, dataloader_val = get_dataloader(subset_train, subset_val, 64)\n",
    "    \n",
    "    # Init the run method\n",
    "    result = tune.run(\n",
    "        func_partial(train_model),\n",
    "        #tune.with_parameters(train_model, dataset_train=data),\n",
    "        #tune.with_parameters(train_model, dataloader_train=dataloader_train, dataloader_val=dataloader_val),\n",
    "        name = name,\n",
    "        resources_per_trial={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        local_dir = \"./Ray_Results\",\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "    \n",
    "    # Find best trial and use it on the testset\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    best_trained_model = CNN(best_trial.config[\"l1\"], best_trial.config[\"l2\"], best_trial.config[\"l3\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "    \n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j_hone04/anaconda3/envs/cnn-env/lib/python3.9/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-09-30 16:54:49,602\tWARNING experiment.py:295 -- No name detected on trainable. Using DEFAULT.\n",
      "2021-09-30 16:54:49,604\tINFO registry.py:66 -- Detected unknown callable for trainable. Converting to class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 22.1/31.2 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 2.0/6 CPUs, 0.3/1 GPUs, 0.0/6.12 GiB heap, 0.0/3.06 GiB objects (0.0/1.0 accelerator_type:GTX)\n",
      "Result logdir: /media/DATA/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_30-16:54:48\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+---------------------+----------+-------+------+------+------+-------------+-------------+--------------+\n",
      "| Trial name          | status   | loc   |   l1 |   l2 |   l3 |          lr |          wd |   batch_size |\n",
      "|---------------------+----------+-------+------+------+------+-------------+-------------+--------------|\n",
      "| DEFAULT_5c576_00000 | RUNNING  |       |    4 |   64 |   32 | 0.000264447 | 0.000178398 |           64 |\n",
      "| DEFAULT_5c576_00001 | PENDING  |       |  128 |    4 |  128 | 0.000280086 | 3.79861e-05 |           64 |\n",
      "| DEFAULT_5c576_00002 | PENDING  |       |   32 |   16 |   64 | 0.0152388   | 3.27647e-05 |           64 |\n",
      "| DEFAULT_5c576_00003 | PENDING  |       |   64 |   64 |   64 | 0.000137449 | 2.64349e-05 |           16 |\n",
      "| DEFAULT_5c576_00004 | PENDING  |       |   16 |   64 |   64 | 0.000350373 | 6.84146e-05 |          256 |\n",
      "| DEFAULT_5c576_00005 | PENDING  |       |   32 |   32 |    8 | 0.00216059  | 2.60605e-05 |           16 |\n",
      "| DEFAULT_5c576_00006 | PENDING  |       |  128 |   32 |   32 | 0.00150873  | 0.000115945 |           16 |\n",
      "| DEFAULT_5c576_00007 | PENDING  |       |   64 |   16 |  128 | 0.00414472  | 0.000917673 |          256 |\n",
      "| DEFAULT_5c576_00008 | PENDING  |       |   16 |   32 |   32 | 0.0123694   | 0.000178944 |          128 |\n",
      "| DEFAULT_5c576_00009 | PENDING  |       |   64 |   16 |    4 | 0.023704    | 0.000978759 |          256 |\n",
      "+---------------------+----------+-------+------+------+------+-------------+-------------+--------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 13.9/31.2 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 6.0/6 CPUs, 0.8999999999999999/1 GPUs, 0.0/6.12 GiB heap, 0.0/3.06 GiB objects (0.0/1.0 accelerator_type:GTX)\n",
      "Result logdir: /media/DATA/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_30-16:54:48\n",
      "Number of trials: 10/10 (7 PENDING, 3 RUNNING)\n",
      "+---------------------+----------+-------+------+------+------+-------------+-------------+--------------+\n",
      "| Trial name          | status   | loc   |   l1 |   l2 |   l3 |          lr |          wd |   batch_size |\n",
      "|---------------------+----------+-------+------+------+------+-------------+-------------+--------------|\n",
      "| DEFAULT_5c576_00000 | RUNNING  |       |    4 |   64 |   32 | 0.000264447 | 0.000178398 |           64 |\n",
      "| DEFAULT_5c576_00001 | RUNNING  |       |  128 |    4 |  128 | 0.000280086 | 3.79861e-05 |           64 |\n",
      "| DEFAULT_5c576_00002 | RUNNING  |       |   32 |   16 |   64 | 0.0152388   | 3.27647e-05 |           64 |\n",
      "| DEFAULT_5c576_00003 | PENDING  |       |   64 |   64 |   64 | 0.000137449 | 2.64349e-05 |           16 |\n",
      "| DEFAULT_5c576_00004 | PENDING  |       |   16 |   64 |   64 | 0.000350373 | 6.84146e-05 |          256 |\n",
      "| DEFAULT_5c576_00005 | PENDING  |       |   32 |   32 |    8 | 0.00216059  | 2.60605e-05 |           16 |\n",
      "| DEFAULT_5c576_00006 | PENDING  |       |  128 |   32 |   32 | 0.00150873  | 0.000115945 |           16 |\n",
      "| DEFAULT_5c576_00007 | PENDING  |       |   64 |   16 |  128 | 0.00414472  | 0.000917673 |          256 |\n",
      "| DEFAULT_5c576_00008 | PENDING  |       |   16 |   32 |   32 | 0.0123694   | 0.000178944 |          128 |\n",
      "| DEFAULT_5c576_00009 | PENDING  |       |   64 |   16 |    4 | 0.023704    | 0.000978759 |          256 |\n",
      "+---------------------+----------+-------+------+------+------+-------------+-------------+--------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(num_samples=10, max_num_epochs=10, gpus_per_trial=gpus_per_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn-env]",
   "language": "python",
   "name": "conda-env-cnn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
