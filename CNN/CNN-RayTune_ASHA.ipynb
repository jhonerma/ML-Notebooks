{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from functools import partial as func_partial\n",
    "from functools import reduce as func_reduce\n",
    "from operator import mul as op_mul\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from os import cpu_count, path\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU's: 12\n"
     ]
    }
   ],
   "source": [
    "# Show number of avlaible CPU threads\n",
    "# With mulithreading this number is twice the number of physical cores\n",
    "cpu_av = cpu_count()\n",
    "print(\"Number of available CPU's: {}\".format(cpu_av))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number CPUS that should be used per trial and dataloader\n",
    "# If set to 1 number of cucurrent training networking is equal to this number\n",
    "# In case of training with GPU this will be limited to number of models training simultaneously on GPU\n",
    "# So number of CPU threads for each trial can be increased \n",
    "cpus_per_trial = 1\n",
    "gpus_per_trial = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs reworking opening and closing the file too many times with multithreading\n",
    "# which leads to crashing\n",
    "# if clusters were saved as seperate images it would make sense to load them\n",
    "class ClusterDataset_Partial(utils.Dataset):\n",
    "    \"\"\"Cluster dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, npz_file, arrsize=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            npz_file (string): Path to the npz file.\n",
    "        \"\"\"\n",
    "        self.data = np.load(npz_file, allow_pickle=True)\n",
    "        self.arrsize = arrsize\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"Size\"]\n",
    "    \n",
    "    def __ReconstructCluster(self, ncell, modnum, row, col, cdata):\n",
    "        _row = row.copy()\n",
    "        _col = col.copy()       \n",
    "        if not np.all( modnum[0] == modnum[:ncell]):\n",
    "            ModNumDif = modnum - np.min(modnum[:ncell])\n",
    "            mask = np.where(ModNumDif == 1)\n",
    "            _col[mask] += 48\n",
    "            mask = np.where(ModNumDif == 2)\n",
    "            _row[mask] += 24\n",
    "            mask = np.where(ModNumDif == 3)\n",
    "            _row[mask] += 24\n",
    "            _col[mask] += 48\n",
    "        \n",
    "        arr = np.zeros(( self.arrsize, self.arrsize ), dtype=np.float32)\n",
    "  \n",
    "        col_min = np.min(_col[:ncell])\n",
    "        row_min = np.min(_row[:ncell])\n",
    "        width = np.max(_col[:ncell]) - col_min\n",
    "        height = np.max(_row[:ncell]) - row_min\n",
    "        offset_h = int((self.arrsize-height)/2)\n",
    "        offset_w = int((self.arrsize-width)/2)\n",
    "        \n",
    "        for i in range(ncell):\n",
    "            arr[ _row[i] - row_min + offset_h, _col[i] - col_min + offset_w ] = cdata[i]\n",
    "\n",
    "        return arr\n",
    "    \n",
    "    def __GetClusters(self, ncell, modnum, row, col, energy, timing):\n",
    "        \n",
    "        cluster_e = self.__ReconstructCluster(ncell, modnum, row, col, energy)\n",
    "        cluster_t = self.__ReconstructCluster(ncell, modnum, row, col, timing)\n",
    "\n",
    "        return np.stack([cluster_e, cluster_t], axis=1)\n",
    "    \n",
    "    def __ChangePID(self, PID):\n",
    "        if (PID != 111) & (PID != 221):\n",
    "            PID = np.int16(0)\n",
    "        if PID == 111:\n",
    "            PID = np.int16(1)\n",
    "        if PID == 221:\n",
    "            PID = np.int16(2)\n",
    "        return PID\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        ClusterN = self.data['ClusterN'][idx]\n",
    "        Cluster = self.data['Cluster'][idx]\n",
    "        ClusterTiming = self.data['ClusterTiming'][idx]\n",
    "        ClusterType = self.data['ClusterType'][idx]\n",
    "        ClusterE = self.data['ClusterE'][idx]\n",
    "        ClusterPt = self.data['ClusterPt'][idx]\n",
    "        ClusterModuleNumber = self.data['ClusterModuleNumber'][idx]\n",
    "        ClusterCol = self.data['ClusterCol'][idx]\n",
    "        ClusterRow = self.data['ClusterRow'][idx]\n",
    "        ClusterM02 = self.data['ClusterM02'][idx]\n",
    "        ClusterM20 = self.data['ClusterM20'][idx]\n",
    "        ClusterDistFromVert = self.data['ClusterDistFromVert'][idx]\n",
    "        PartE = self.data['PartE'][idx]\n",
    "        PartPt = self.data['PartPt'][idx]\n",
    "        PartEta = self.data['PartEta'][idx]\n",
    "        PartPhi = self.data['PartPhi'][idx]\n",
    "        PartIsPrimary = self.data['PartIsPrimary'][idx]\n",
    "        PartPID = self.data['PartPID'][idx]\n",
    "       \n",
    "        PartPID = self.__ChangePID(PartPID)\n",
    "        \n",
    "        img = self.__GetClusters(ClusterN, ClusterModuleNumber, ClusterRow, ClusterCol, Cluster, ClusterTiming)\n",
    "        img = torch.from_numpy(img)\n",
    "        \n",
    "        features = { \"ClusterType\" : ClusterType, \"ClusterE\" : ClusterE, \"ClusterPt\" : ClusterPt\n",
    "                    , \"ClusterM02\" : ClusterM02, \"ClusterM20\" : ClusterM20 , \"ClusterDistFromVert\" : ClusterDistFromVert}\n",
    "        labels = { \"PartE\" : PartE, \"PartPt\" : PartPt, \"PartEta\" : PartEta, \"PartPhi\" : PartPhi\n",
    "                  , \"PartIsPrimary\" : PartIsPrimary, \"PartPID\" : PartPID }\n",
    "        \n",
    "        return img, features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterDataset_Full(utils.Dataset):\n",
    "    \"\"\"Cluster dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, npz_file, arrsize=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            npz_file (string): Path to the npz file.\n",
    "        \"\"\"\n",
    "        self.data = np.load(npz_file, allow_pickle=True)\n",
    "        self.arrsize = arrsize\n",
    "        self.ClusterN = self.data['ClusterN']\n",
    "        self.Cluster = self.data['Cluster']\n",
    "        self.ClusterTiming = self.data['ClusterTiming']\n",
    "        self.ClusterType = self.data['ClusterType']\n",
    "        self.ClusterE = self.data['ClusterE']\n",
    "        self.ClusterPt = self.data['ClusterPt']\n",
    "        self.ClusterModuleNumber = self.data['ClusterModuleNumber']\n",
    "        self.ClusterCol = self.data['ClusterCol']\n",
    "        self.ClusterRow = self.data['ClusterRow']\n",
    "        self.ClusterM02 = self.data['ClusterM02']\n",
    "        self.ClusterM20 = self.data['ClusterM20']\n",
    "        self.ClusterDistFromVert = self.data['ClusterDistFromVert']\n",
    "        self.PartE = self.data['PartE']\n",
    "        self.PartPt = self.data['PartPt']\n",
    "        self.PartEta = self.data['PartEta']\n",
    "        self.PartPhi = self.data['PartPhi']\n",
    "        self.PartIsPrimary = self.data['PartIsPrimary']\n",
    "        self.PartPID = self.data['PartPID']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"Size\"]\n",
    "    \n",
    "    def __ReconstructCluster(self, ncell, modnum, row, col, cdata):\n",
    "        _row = row.copy()\n",
    "        _col = col.copy()       \n",
    "        if not np.all( modnum[0] == modnum[:ncell]):\n",
    "            ModNumDif = modnum - np.min(modnum[:ncell])\n",
    "            mask = np.where(ModNumDif == 1)\n",
    "            _col[mask] += 48\n",
    "            mask = np.where(ModNumDif == 2)\n",
    "            _row[mask] += 24\n",
    "            mask = np.where(ModNumDif == 3)\n",
    "            _row[mask] += 24\n",
    "            _col[mask] += 48\n",
    "        \n",
    "        arr = np.zeros(( self.arrsize, self.arrsize ), dtype=np.float32)\n",
    "  \n",
    "        col_min = np.min(_col[:ncell])\n",
    "        row_min = np.min(_row[:ncell])\n",
    "        width = np.max(_col[:ncell]) - col_min\n",
    "        height = np.max(_row[:ncell]) - row_min\n",
    "        offset_h = int((self.arrsize-height)/2)\n",
    "        offset_w = int((self.arrsize-width)/2)\n",
    "        \n",
    "        for i in range(ncell):\n",
    "            arr[ _row[i] - row_min + offset_h, _col[i] - col_min + offset_w ] = cdata[i]\n",
    "        return arr\n",
    "    \n",
    "    def __GetClusters(self, ncell, modnum, row, col, energy, timing):       \n",
    "        cluster_e = self.__ReconstructCluster(ncell, modnum, row, col, energy)\n",
    "        cluster_t = self.__ReconstructCluster(ncell, modnum, row, col, timing)\n",
    "        return np.stack([cluster_e, cluster_t], axis=0)\n",
    "    \n",
    "    def __ChangePID(self, PID):\n",
    "        if (PID != 111) & (PID != 221):\n",
    "            PID = np.int16(0)\n",
    "        if PID == 111:\n",
    "            PID = np.int16(1)\n",
    "        if PID == 221:\n",
    "            PID = np.int16(2)\n",
    "        return PID\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        ClusterN = self.ClusterN[idx]\n",
    "        Cluster = self.Cluster[idx]\n",
    "        ClusterTiming = self.ClusterTiming[idx]\n",
    "        ClusterType = self.ClusterType[idx]\n",
    "        ClusterE = self.ClusterE[idx]\n",
    "        ClusterPt = self.ClusterPt[idx]\n",
    "        ClusterModuleNumber = self.ClusterModuleNumber[idx]\n",
    "        ClusterCol = self.ClusterCol[idx]\n",
    "        ClusterRow = self.ClusterRow[idx]\n",
    "        ClusterM02 = self.ClusterM02[idx]\n",
    "        ClusterM20 = self.ClusterM20[idx]\n",
    "        ClusterDistFromVert = self.ClusterDistFromVert[idx]\n",
    "        PartE = self.PartE[idx]\n",
    "        PartPt = self.PartPt[idx]\n",
    "        PartEta = self.PartEta[idx]\n",
    "        PartPhi = self.PartPhi[idx]\n",
    "        PartIsPrimary = self.PartIsPrimary[idx]\n",
    "        PartPID = self.PartPID[idx]\n",
    "        \n",
    "        PartPID = self.__ChangePID(PartPID)\n",
    "        \n",
    "        img = self.__GetClusters(ClusterN, ClusterModuleNumber, ClusterRow, ClusterCol, Cluster, ClusterTiming)\n",
    "        #img = torch.from_numpy(img)\n",
    "        \n",
    "        features = { \"ClusterType\" : ClusterType, \"ClusterE\" : ClusterE, \"ClusterPt\" : ClusterPt\n",
    "                    , \"ClusterM02\" : ClusterM02, \"ClusterM20\" : ClusterM20 , \"ClusterDist\" : ClusterDistFromVert}\n",
    "        labels = { \"PartE\" : PartE, \"PartPt\" : PartPt, \"PartEta\" : PartEta, \"PartPhi\" : PartPhi\n",
    "                  , \"PartIsPrimary\" : PartIsPrimary, \"PartPID\" : PartPID }\n",
    "        \n",
    "        return (img, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train(path='/home/jhonerma/ML-Notebooks/CNN/Data/data_train.npz'):\n",
    "    ds_train = ClusterDataset_Full(path)\n",
    "    return ds_train\n",
    "\n",
    "def load_data_test(path='/home/jhonerma/ML-Notebooks/CNN/Data/data_test.npz'):\n",
    "    ds_test = ClusterDataset_Full(path)\n",
    "    return ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(train_ds, val_ds, bs):\n",
    "    dl_train = utils.DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=cpus_per_trial-1)\n",
    "    dl_val = utils.DataLoader(val_ds, batch_size=bs * 2, shuffle=True, num_workers=cpus_per_trial-1)\n",
    "    return  dl_train, dl_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsqueeze_features(features):\n",
    "    for key in features.keys():\n",
    "        features[key] = features[key].view(-1,1)        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1610.04490\n",
    "INSTANCE_NOISE = False\n",
    "\n",
    "def add_instance_noise(data, std=0.01):\n",
    "    return data + torch.distributions.Normal(0, std).sample(data.shape) #.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, l1=100, l2=50, l3=25, input_dim=(2,20,20), num_in_features=5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.feature_ext = nn.Sequential(\n",
    "            nn.Conv2d(2,10, kernel_size=3, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,10, kernel_size=3,  padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,10, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,5, kernel_size=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Gives the number of features after the conv layer\n",
    "        num_features_after_conv = func_reduce(op_mul, list(self.feature_ext(torch.rand(1, *input_dim)).shape))\n",
    "        \n",
    "        self.dense_nn = nn.Sequential(\n",
    "            nn.Linear(num_features_after_conv + num_in_features, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3,3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, cluster, clusNumXYEPt):\n",
    "        cluster = self.feature_ext(cluster)\n",
    "        x = self.flatten(cluster)\n",
    "        x = torch.cat([x, clusNumXYEPt], dim=1)\n",
    "        logits = self.dense_nn(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement train and validation loop\n",
    "Data[0] contains an image of of the cell energies and timings. <br>\n",
    "Data[1] contains all features in a dict. Their shapes have to be changed from [batch_size] to [batch_size,1] for input into linear layers, implemented via function here <br>\n",
    "Data[2] contains all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "\n",
    "    for batch, Data in enumerate(dataloader):\n",
    "        Clusters = Data[0].to(device)\n",
    "        Features = unsqueeze_features(Data[1])\n",
    "        Labels = Data[2]\n",
    "        \n",
    "        ClusterProperties = torch.cat([Features[\"ClusterE\"], Features[\"ClusterPt\"], Features[\"ClusterM02\"]\n",
    "                                      , Features[\"ClusterM20\"], Features[\"ClusterDist\"]], dim=1)\n",
    "        ClusterProperties.to(device)\n",
    "               \n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction and loss\n",
    "        pred = model(Clusters, ClusterProperties)\n",
    "        loss = loss_fn(pred, Labels[\"PartPID\"].long())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        \n",
    "        if batch % 10 == 9:\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, batch + 1,\n",
    "                                            running_loss / epoch_steps))\n",
    "            running_loss = 0.0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, Data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            Clusters = Data[0].to(device)\n",
    "            Features = unsqueeze_features(Data[1])\n",
    "            Labels = Data[2]\n",
    "            ClusterProperties = torch.cat([Features[\"ClusterE\"], Features[\"ClusterPt\"], Features[\"ClusterM02\"]\n",
    "                                      , Features[\"ClusterM20\"], Features[\"ClusterDist\"]], dim=1)           \n",
    "            ClusterProperties.to(device)\n",
    "            \n",
    "            pred = model(Clusters, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == Labels[\"PartPID\"]).type(torch.float).sum().item()\n",
    "\n",
    "            loss = loss_fn(pred, Labels[\"PartPID\"].long())#.item()\n",
    "            val_loss += loss.cpu().numpy()\n",
    "            val_steps += 1\n",
    "    \n",
    "    with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "        _path = path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), _path)\n",
    "        \n",
    "    tune.report(loss=(val_loss / val_steps), accuracy= correct / size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement method for accuracy testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, device=\"cpu\"):\n",
    "    \n",
    "    dataset_test = load_data_test()\n",
    "    \n",
    "    dataloader_test = utils.DataLoader(\n",
    "        dataset_test, batch_size=4, shuffle=False, num_workers=2)\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(dataloader_test.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, Data in enumerate(dataloader_test):\n",
    "            Clusters = Data[0].to(device)\n",
    "            Features = unsqueeze_features(Data[1])\n",
    "            Labels = Data[2]\n",
    "            ClusterProperties = torch.cat([Features[\"ClusterE\"], Features[\"ClusterPt\"], Features[\"ClusterM02\"]\n",
    "                                      , Features[\"ClusterM20\"], Features[\"ClusterDist\"]], dim=1)            \n",
    "            ClusterProperties.to(device)\n",
    "            \n",
    "            \n",
    "            pred = model(Clusters, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == Labels[\"PartPID\"]).type(torch.float).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, checkpoint_dir=None):\n",
    "    \n",
    "    # load model\n",
    "    model = CNN(config[\"l1\"],config[\"l2\"],config[\"l3\"])\n",
    "    \n",
    "    # check for avlaible resource and initialize device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    # send model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # initialise loss function and opptimizer\n",
    "    loss_fn = F.cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    \n",
    "    # check whether checkpoint is available\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "    # load dataset\n",
    "    dataset_train = load_data_train()\n",
    "    \n",
    "    # split trainset in train and validation subsets\n",
    "    test_abs = int(len(dataset_train) * 0.8)\n",
    "    subset_train, subset_val = utils.random_split(\n",
    "        dataset_train, [test_abs, len(dataset_train) - test_abs])\n",
    "\n",
    "    # get dataloaders \n",
    "    dataloader_train, dataloader_val = get_dataloader(subset_train, subset_val, int(config[\"batch_size\"]))\n",
    "                                                      \n",
    "    for epoch in range(100):\n",
    "        train_loop(epoch, dataloader_train, model, loss_fn, optimizer, device=device)\n",
    "        val_loop(epoch, dataloader_train, model, loss_fn, optimizer, device=device)                                              \n",
    "    \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all Ray Tune functionality and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
    "    \n",
    "    # Setup hyperparameter-space to search\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"l3\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16, 32, 64])\n",
    "    }\n",
    "\n",
    "    # Init the scheduler\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    # Init the Reporter\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"l1\", \"l2\", \"l3\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    #Get Current date and time\n",
    "    timestr = strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "    name = \"ASHA-\" + timestr\n",
    "    \n",
    "    # Init the run method\n",
    "    result = tune.run(\n",
    "        func_partial(train_model),\n",
    "        name = name,\n",
    "        resources_per_trial={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        local_dir = \"./Ray_Results\",\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "    \n",
    "    # Find best trial and use it on the testset\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    best_trained_model = CNN(best_trial.config[\"l1\"], best_trial.config[\"l2\"], best_trial.config[\"l3\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "    \n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhonerma/anaconda3/envs/cnn-env/lib/python3.9/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-09-17 16:16:59,700\tWARNING experiment.py:295 -- No name detected on trainable. Using DEFAULT.\n",
      "2021-09-17 16:16:59,701\tINFO registry.py:66 -- Detected unknown callable for trainable. Converting to class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 5.2/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/12 CPUs, 0/0 GPUs, 0.0/6.63 GiB heap, 0.0/3.32 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_17-16:16:58\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+---------------------+----------+-------+------+------+------+-------------+--------------+\n",
      "| Trial name          | status   | loc   |   l1 |   l2 |   l3 |          lr |   batch_size |\n",
      "|---------------------+----------+-------+------+------+------+-------------+--------------|\n",
      "| DEFAULT_ec003_00000 | RUNNING  |       |  128 |   32 |  128 | 0.0301595   |           32 |\n",
      "| DEFAULT_ec003_00001 | PENDING  |       |  256 |   32 |  512 | 0.000866715 |            4 |\n",
      "| DEFAULT_ec003_00002 | PENDING  |       |   32 |   64 |    4 | 0.000342879 |           64 |\n",
      "| DEFAULT_ec003_00003 | PENDING  |       |   64 |  256 |   16 | 0.00259224  |            2 |\n",
      "| DEFAULT_ec003_00004 | PENDING  |       |   64 |   16 |  256 | 0.0497963   |           16 |\n",
      "| DEFAULT_ec003_00005 | PENDING  |       |  128 |   16 |   32 | 0.00717963  |           64 |\n",
      "| DEFAULT_ec003_00006 | PENDING  |       |   16 |   64 |  256 | 0.000238847 |           32 |\n",
      "| DEFAULT_ec003_00007 | PENDING  |       |  256 |    4 |  256 | 0.000727573 |            4 |\n",
      "| DEFAULT_ec003_00008 | PENDING  |       |   32 |  128 |  512 | 0.000385783 |            8 |\n",
      "| DEFAULT_ec003_00009 | PENDING  |       |   16 |   32 |  128 | 0.000319124 |           64 |\n",
      "+---------------------+----------+-------+------+------+------+-------------+--------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [1,    10] loss: 1.120\n",
      "Result for DEFAULT_ec003_00002:\n",
      "  accuracy: 0.09523809523809523\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: false\n",
      "  experiment_id: d2fe9ea29a2c41a290c658dd6506620f\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.1016191244125366\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39623\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.25428080558776855\n",
      "  time_this_iter_s: 0.25428080558776855\n",
      "  time_total_s: 0.25428080558776855\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00002\n",
      "  \n",
      "Result for DEFAULT_ec003_00005:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: false\n",
      "  experiment_id: f5a212e84cb949e199636acb711059eb\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.7555074691772461\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39615\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.2882683277130127\n",
      "  time_this_iter_s: 0.2882683277130127\n",
      "  time_total_s: 0.2882683277130127\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00005\n",
      "  \n",
      "Result for DEFAULT_ec003_00000:\n",
      "  accuracy: 0.20408163265306123\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: true\n",
      "  experiment_id: bdddffafc6604c2a871f9625a25c95fa\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.09861216545105\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39618\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.3015615940093994\n",
      "  time_this_iter_s: 0.3015615940093994\n",
      "  time_total_s: 0.3015615940093994\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [1,    20] loss: 0.530\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [1,    10] loss: 1.001\n",
      "\u001b[2m\u001b[36m(pid=39622)\u001b[0m [1,    10] loss: 7.005\n",
      "Result for DEFAULT_ec003_00004:\n",
      "  accuracy: 0.6666666666666666\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: true\n",
      "  experiment_id: bbabd2e567d14ca6846fb0c0bdc1d8a0\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.9884374618530274\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39622\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.3635671138763428\n",
      "  time_this_iter_s: 0.3635671138763428\n",
      "  time_total_s: 0.3635671138763428\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00004\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [1,    30] loss: 0.322\n",
      "\u001b[2m\u001b[36m(pid=39621)\u001b[0m [1,    10] loss: 1.116\n",
      "Result for DEFAULT_ec003_00006:\n",
      "  accuracy: 0.2108843537414966\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: true\n",
      "  experiment_id: 94f4c748c64e4a7aaf9baa5781d17fad\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.1039288997650147\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39619\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.3098633289337158\n",
      "  time_this_iter_s: 0.3098633289337158\n",
      "  time_total_s: 0.3098633289337158\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00006\n",
      "  \n",
      "Result for DEFAULT_ec003_00009:\n",
      "  accuracy: 0.6870748299319728\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: false\n",
      "  experiment_id: 3c3b7d14d47543ee97e7e2646c4843ab\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0550395647684734\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39613\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.3095109462738037\n",
      "  time_this_iter_s: 0.3095109462738037\n",
      "  time_total_s: 0.3095109462738037\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00009\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [1,    40] loss: 0.268\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [1,    20] loss: 0.436\n",
      "\u001b[2m\u001b[36m(pid=39620)\u001b[0m [1,    10] loss: 1.099\n",
      "Result for DEFAULT_ec003_00008:\n",
      "  accuracy: 0.23129251700680273\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: false\n",
      "  experiment_id: c84ec071b91341d2b47ff126ae323bf5\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39621\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.4372556209564209\n",
      "  time_this_iter_s: 0.4372556209564209\n",
      "  time_total_s: 0.4372556209564209\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00008\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [1,    50] loss: 0.135\n",
      "Result for DEFAULT_ec003_00009:\n",
      "  accuracy: 0.6870748299319728\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: true\n",
      "  experiment_id: 3c3b7d14d47543ee97e7e2646c4843ab\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 2\n",
      "  loss: 1.0285710096359253\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39613\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.5703368186950684\n",
      "  time_this_iter_s: 0.26082587242126465\n",
      "  time_total_s: 0.5703368186950684\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: ec003_00009\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [1,    60] loss: 0.123\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [1,    30] loss: 0.269\n",
      "\u001b[2m\u001b[36m(pid=39621)\u001b[0m [2,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=39620)\u001b[0m [1,    20] loss: 0.549\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [1,    70] loss: 0.113\n",
      "Result for DEFAULT_ec003_00008:\n",
      "  accuracy: 0.23129251700680273\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: true\n",
      "  experiment_id: c84ec071b91341d2b47ff126ae323bf5\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 2\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39621\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.7978205680847168\n",
      "  time_this_iter_s: 0.3605649471282959\n",
      "  time_total_s: 0.7978205680847168\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: ec003_00008\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=39620)\u001b[0m [1,    30] loss: 0.366\n",
      "Result for DEFAULT_ec003_00001:\n",
      "  accuracy: 0.6802721088435374\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: false\n",
      "  experiment_id: dfb1eaae1e314076b7b470384c78c61e\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8591368303105638\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39624\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.966012716293335\n",
      "  time_this_iter_s: 0.966012716293335\n",
      "  time_total_s: 0.966012716293335\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00001\n",
      "  \n",
      "Result for DEFAULT_ec003_00003:\n",
      "  accuracy: 0.6938775510204082\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: false\n",
      "  experiment_id: df0e9f5791514efc85cb693a306a719c\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8304357125952437\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39614\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.9635355472564697\n",
      "  time_this_iter_s: 0.9635355472564697\n",
      "  time_total_s: 0.9635355472564697\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00003\n",
      "  \n",
      "Result for DEFAULT_ec003_00007:\n",
      "  accuracy: 0.23129251700680273\n",
      "  date: 2021-09-17_16-17-01\n",
      "  done: true\n",
      "  experiment_id: 146270bca4684c549ba439fa95a7fbeb\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39620\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.9554126262664795\n",
      "  time_this_iter_s: 0.9554126262664795\n",
      "  time_total_s: 0.9554126262664795\n",
      "  timestamp: 1631888221\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: ec003_00007\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [2,    10] loss: 0.928\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [2,    20] loss: 0.365\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [2,    10] loss: 0.926\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [2,    30] loss: 0.300\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [2,    40] loss: 0.267\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [2,    50] loss: 0.159\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [2,    60] loss: 0.125\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [2,    20] loss: 0.445\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [2,    70] loss: 0.128\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [2,    30] loss: 0.178\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [3,    10] loss: 1.120\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [3,    20] loss: 0.450\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [3,    10] loss: 0.918\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [3,    30] loss: 0.252\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [3,    40] loss: 0.211\n",
      "Result for DEFAULT_ec003_00002:\n",
      "  accuracy: 0.7006802721088435\n",
      "  date: 2021-09-17_16-17-02\n",
      "  done: true\n",
      "  experiment_id: d2fe9ea29a2c41a290c658dd6506620f\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.9903576970100403\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39623\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.941173791885376\n",
      "  time_this_iter_s: 0.1569833755493164\n",
      "  time_total_s: 1.941173791885376\n",
      "  timestamp: 1631888222\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: ec003_00002\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [3,    50] loss: 0.198\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [3,    20] loss: 0.471\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [3,    60] loss: 0.128\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [3,    70] loss: 0.092\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [3,    30] loss: 0.285\n",
      "Result for DEFAULT_ec003_00005:\n",
      "  accuracy: 0.7074829931972789\n",
      "  date: 2021-09-17_16-17-03\n",
      "  done: true\n",
      "  experiment_id: f5a212e84cb949e199636acb711059eb\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.8893228570620219\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39615\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.1525096893310547\n",
      "  time_this_iter_s: 0.174912691116333\n",
      "  time_total_s: 2.1525096893310547\n",
      "  timestamp: 1631888223\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: ec003_00005\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [4,    10] loss: 1.022\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [4,    20] loss: 0.353\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [4,    30] loss: 0.308\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [4,    10] loss: 0.826\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [4,    40] loss: 0.207\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [4,    50] loss: 0.202\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [4,    20] loss: 0.395\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [4,    60] loss: 0.134\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [4,    70] loss: 0.096\n",
      "\u001b[2m\u001b[36m(pid=39624)\u001b[0m [4,    30] loss: 0.302\n",
      "Result for DEFAULT_ec003_00001:\n",
      "  accuracy: 0.6802721088435374\n",
      "  date: 2021-09-17_16-17-03\n",
      "  done: true\n",
      "  experiment_id: dfb1eaae1e314076b7b470384c78c61e\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 4\n",
      "  loss: 0.8649105784055349\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39624\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.746537446975708\n",
      "  time_this_iter_s: 0.5310611724853516\n",
      "  time_total_s: 2.746537446975708\n",
      "  timestamp: 1631888223\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: ec003_00001\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [5,    10] loss: 0.853\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [5,    20] loss: 0.452\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [5,    30] loss: 0.224\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [5,    40] loss: 0.183\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [5,    50] loss: 0.191\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [5,    60] loss: 0.139\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [5,    70] loss: 0.156\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [6,    10] loss: 0.848\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [6,    20] loss: 0.516\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [6,    30] loss: 0.274\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [6,    40] loss: 0.184\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [6,    50] loss: 0.194\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [6,    60] loss: 0.081\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [6,    70] loss: 0.130\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [7,    10] loss: 0.883\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [7,    20] loss: 0.396\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [7,    30] loss: 0.279\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [7,    40] loss: 0.244\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [7,    50] loss: 0.111\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [7,    60] loss: 0.154\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [7,    70] loss: 0.149\n",
      "== Status ==\n",
      "Memory usage on this node: 5.0/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -0.8919612864653269 | Iter 4.000: -0.8614063615331778 | Iter 2.000: -0.9566315009787276 | Iter 1.000: -1.0986122369766236\n",
      "Resources requested: 1.0/12 CPUs, 0/0 GPUs, 0.0/6.63 GiB heap, 0.0/3.32 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_17-16:16:58\n",
      "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
      "+---------------------+------------+-------------------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "| Trial name          | status     | loc               |   l1 |   l2 |   l3 |          lr |   batch_size |     loss |   accuracy |   training_iteration |\n",
      "|---------------------+------------+-------------------+------+------+------+-------------+--------------+----------+------------+----------------------|\n",
      "| DEFAULT_ec003_00003 | RUNNING    | 10.67.95.37:39614 |   64 |  256 |   16 | 0.00259224  |            2 | 0.837755 |   0.693878 |                    7 |\n",
      "| DEFAULT_ec003_00000 | TERMINATED |                   |  128 |   32 |  128 | 0.0301595   |           32 | 1.09861  |   0.204082 |                    1 |\n",
      "| DEFAULT_ec003_00001 | TERMINATED |                   |  256 |   32 |  512 | 0.000866715 |            4 | 0.864911 |   0.680272 |                    4 |\n",
      "| DEFAULT_ec003_00002 | TERMINATED |                   |   32 |   64 |    4 | 0.000342879 |           64 | 0.990358 |   0.70068  |                   10 |\n",
      "| DEFAULT_ec003_00004 | TERMINATED |                   |   64 |   16 |  256 | 0.0497963   |           16 | 1.98844  |   0.666667 |                    1 |\n",
      "| DEFAULT_ec003_00005 | TERMINATED |                   |  128 |   16 |   32 | 0.00717963  |           64 | 0.889323 |   0.707483 |                   10 |\n",
      "| DEFAULT_ec003_00006 | TERMINATED |                   |   16 |   64 |  256 | 0.000238847 |           32 | 1.10393  |   0.210884 |                    1 |\n",
      "| DEFAULT_ec003_00007 | TERMINATED |                   |  256 |    4 |  256 | 0.000727573 |            4 | 1.09861  |   0.231293 |                    1 |\n",
      "| DEFAULT_ec003_00008 | TERMINATED |                   |   32 |  128 |  512 | 0.000385783 |            8 | 1.09861  |   0.231293 |                    2 |\n",
      "| DEFAULT_ec003_00009 | TERMINATED |                   |   16 |   32 |  128 | 0.000319124 |           64 | 1.02857  |   0.687075 |                    2 |\n",
      "+---------------------+------------+-------------------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [8,    10] loss: 0.909\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [8,    20] loss: 0.441\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [8,    30] loss: 0.317\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [8,    40] loss: 0.213\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [8,    50] loss: 0.179\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [8,    60] loss: 0.125\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [8,    70] loss: 0.081\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [9,    10] loss: 0.846\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [9,    20] loss: 0.262\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [9,    30] loss: 0.372\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [9,    40] loss: 0.203\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [9,    50] loss: 0.181\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [9,    60] loss: 0.170\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [9,    70] loss: 0.128\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [10,    10] loss: 0.592\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [10,    20] loss: 0.582\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [10,    30] loss: 0.231\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [10,    40] loss: 0.228\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [10,    50] loss: 0.204\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [10,    60] loss: 0.142\n",
      "\u001b[2m\u001b[36m(pid=39614)\u001b[0m [10,    70] loss: 0.120\n",
      "Result for DEFAULT_ec003_00003:\n",
      "  accuracy: 0.6938775510204082\n",
      "  date: 2021-09-17_16-17-06\n",
      "  done: true\n",
      "  experiment_id: df0e9f5791514efc85cb693a306a719c\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.8191089835521337\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 39614\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 5.5948662757873535\n",
      "  time_this_iter_s: 0.4720344543457031\n",
      "  time_total_s: 5.5948662757873535\n",
      "  timestamp: 1631888226\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: ec003_00003\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 4.8/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 8.000: -0.8536082715601534 | Iter 4.000: -0.8614063615331778 | Iter 2.000: -0.9566315009787276 | Iter 1.000: -1.0986122369766236\n",
      "Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/6.63 GiB heap, 0.0/3.32 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_17-16:16:58\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "| Trial name          | status     | loc   |   l1 |   l2 |   l3 |          lr |   batch_size |     loss |   accuracy |   training_iteration |\n",
      "|---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------|\n",
      "| DEFAULT_ec003_00000 | TERMINATED |       |  128 |   32 |  128 | 0.0301595   |           32 | 1.09861  |   0.204082 |                    1 |\n",
      "| DEFAULT_ec003_00001 | TERMINATED |       |  256 |   32 |  512 | 0.000866715 |            4 | 0.864911 |   0.680272 |                    4 |\n",
      "| DEFAULT_ec003_00002 | TERMINATED |       |   32 |   64 |    4 | 0.000342879 |           64 | 0.990358 |   0.70068  |                   10 |\n",
      "| DEFAULT_ec003_00003 | TERMINATED |       |   64 |  256 |   16 | 0.00259224  |            2 | 0.819109 |   0.693878 |                   10 |\n",
      "| DEFAULT_ec003_00004 | TERMINATED |       |   64 |   16 |  256 | 0.0497963   |           16 | 1.98844  |   0.666667 |                    1 |\n",
      "| DEFAULT_ec003_00005 | TERMINATED |       |  128 |   16 |   32 | 0.00717963  |           64 | 0.889323 |   0.707483 |                   10 |\n",
      "| DEFAULT_ec003_00006 | TERMINATED |       |   16 |   64 |  256 | 0.000238847 |           32 | 1.10393  |   0.210884 |                    1 |\n",
      "| DEFAULT_ec003_00007 | TERMINATED |       |  256 |    4 |  256 | 0.000727573 |            4 | 1.09861  |   0.231293 |                    1 |\n",
      "| DEFAULT_ec003_00008 | TERMINATED |       |   32 |  128 |  512 | 0.000385783 |            8 | 1.09861  |   0.231293 |                    2 |\n",
      "| DEFAULT_ec003_00009 | TERMINATED |       |   16 |   32 |  128 | 0.000319124 |           64 | 1.02857  |   0.687075 |                    2 |\n",
      "+---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 16:17:06,658\tINFO tune.py:561 -- Total run time: 6.96 seconds (6.75 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'l1': 64, 'l2': 256, 'l3': 16, 'lr': 0.0025922365967975597, 'batch_size': 2}\n",
      "Best trial final validation loss: 0.8191089835521337\n",
      "Best trial final validation accuracy: 0.6938775510204082\n",
      "Best trial test set accuracy: 0.7021276595744681\n"
     ]
    }
   ],
   "source": [
    "main(num_samples=10, max_num_epochs=10, gpus_per_trial=gpus_per_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn-env]",
   "language": "python",
   "name": "conda-env-cnn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
