{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU's: 12\n"
     ]
    }
   ],
   "source": [
    "# Show number of avlaible CPU threads\n",
    "# With mulithreading this number is twice the number of physical cores\n",
    "cpu_av = os.cpu_count()\n",
    "print(\"Number of available CPU's: {}\".format(cpu_av))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number CPUS that should be used per trial and dataloader\n",
    "# If set to 1 number of cucurrent training networking is equal to this number\n",
    "# In case of training with GPU this will be limited to number of models training simultaneously on GPU\n",
    "# So number of CPU threads for each trial can be increased \n",
    "cpus_per_trial = 1\n",
    "gpus_per_trial = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to data\n",
    "train_path = 'Data/data_train.npz'\n",
    "test_path = 'Data/data_test.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(train_path, allow_pickle=True)\n",
    "test_data = np.load(test_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_train = train_data['ClusterN']\n",
    "Cluster_train = train_data['Cluster']\n",
    "ClusterTiming_train = train_data['ClusterTiming']\n",
    "ClusterType_train = train_data['ClusterType']\n",
    "ClusterE_train = train_data['ClusterE']\n",
    "ClusterPt_train = train_data['ClusterPt']\n",
    "ClusterModuleNumber_train = train_data['ClusterModuleNumber']\n",
    "ClusterCol_train = train_data['ClusterCol']\n",
    "ClusterRow_train = train_data['ClusterRow']\n",
    "ClusterM02_train = train_data['ClusterM02']\n",
    "ClusterM20_train = train_data['ClusterM20']\n",
    "ClusterDistFromVert_train = train_data['ClusterDistFromVert']\n",
    "PartE_train = train_data['PartE']\n",
    "PartPt_train = train_data['PartPt']\n",
    "PartEta_train = train_data['PartEta']\n",
    "PartPhi_train = train_data['PartPhi']\n",
    "PartIsPrimary_train = train_data['PartIsPrimary']\n",
    "PartPID_train = train_data['PartPID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_test = test_data['ClusterN']\n",
    "Cluster_test = test_data['Cluster']\n",
    "ClusterTiming_test = test_data['ClusterTiming']\n",
    "ClusterType_test = test_data['ClusterType']\n",
    "ClusterE_test = test_data['ClusterE']\n",
    "ClusterPt_test = test_data['ClusterPt']\n",
    "ClusterModuleNumber_test = test_data['ClusterModuleNumber']\n",
    "ClusterCol_test = test_data['ClusterCol']\n",
    "ClusterRow_test = test_data['ClusterRow']\n",
    "ClusterM02_test = test_data['ClusterM02']\n",
    "ClusterM20_test = test_data['ClusterM20']\n",
    "ClusterDistFromVert_test = test_data['ClusterDistFromVert']\n",
    "PartE_test = test_data['PartE']\n",
    "PartPt_test = test_data['PartPt']\n",
    "PartEta_test = test_data['PartEta']\n",
    "PartPhi_test = test_data['PartPhi']\n",
    "PartIsPrimary_test = test_data['PartIsPrimary']\n",
    "PartPID_test = test_data['PartPID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary change for PID into three categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_pid(arr):\n",
    "    arr[np.nonzero((arr != 111) & (arr != 221))] = 0\n",
    "    arr[arr == 111] = 1\n",
    "    arr[arr == 221] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_pid(PartPID_test)\n",
    "change_pid(PartPID_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the arrays into [size, 1] for usage with ptorch\n",
    "\n",
    "For linear layers input is expected as [batch_size, num_features] so no need to reshape the existing arrays like Cluster\n",
    "\n",
    "reconstrcuted clusters later will have to have dim [batch_size, channel, height, width] as input for conv2d-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxClusN_train = np.max(ClusterN_train)\n",
    "maxClusN_test = np.max(ClusterN_test)\n",
    "maxClusN = np.max([maxClusN_test, maxClusN_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_train = ClusterN_train.reshape((ClusterN_train.size, 1))\n",
    "#Cluster_train = Cluster_train.reshape((ClusterE_train.size, maxClusN))\n",
    "ClusterType_train = ClusterType_train.reshape((ClusterType_train.size, 1))\n",
    "ClusterE_train = ClusterE_train.reshape((ClusterE_train.size, 1))\n",
    "ClusterPt_train = ClusterPt_train.reshape((ClusterPt_train.size, 1))\n",
    "#ClusterModuleNumber_train = ClusterModuleNumber_train.reshape((ClusterModuleNumber_train.size, maxClusN))\n",
    "#ClusterRow_train = ClusterRow_train.reshape((ClusterRow_train.size, maxClusN))\n",
    "#ClusterCol_train = ClusterCol_train.reshape((ClusterCol_train.size, maxClusN))\n",
    "ClusterM02_train = ClusterM02_train.reshape((ClusterM02_train.size, 1))\n",
    "ClusterM20_train = ClusterM20_train.reshape((ClusterM20_train.size, 1))\n",
    "ClusterDistFromVert_train = ClusterDistFromVert_train.reshape((ClusterDistFromVert_train.size, 1))\n",
    "PartE_train = PartE_train.reshape((PartE_train.size, 1))\n",
    "PartPt_train = PartPt_train.reshape((PartPt_train.size, 1))\n",
    "PartEta_train = PartEta_train.reshape((PartEta_train.size, 1))\n",
    "PartPhi_train = PartPhi_train.reshape((PartPhi_train.size, 1))\n",
    "PartIsPrimary_train = PartIsPrimary_train.reshape((PartIsPrimary_train.size, 1))\n",
    "PartPID_train = PartPID_train.reshape((PartPID_train.size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_test = ClusterN_test.reshape((ClusterN_test.size, 1))\n",
    "#Cluster_test = Cluster_test.reshape((ClusterE_test.size, maxClusN))\n",
    "ClusterType_test = ClusterType_test.reshape((ClusterType_test.size, 1))\n",
    "ClusterE_test = ClusterE_test.reshape((ClusterE_test.size, 1))\n",
    "ClusterPt_test = ClusterPt_test.reshape((ClusterPt_test.size, 1))\n",
    "#ClusterModuleNumber_test = ClusterModuleNumber_test.reshape((ClusterModuleNumber_test.size, maxClusN))\n",
    "#ClusterRow_test = ClusterRow_test.reshape((ClusterRow_test.size, maxClusN))\n",
    "#ClusterCol_test = ClusterCol_test.reshape((ClusterCol_test.size, maxClusN))\n",
    "ClusterM02_test = ClusterM02_test.reshape((ClusterM02_test.size, 1))\n",
    "ClusterM20_test = ClusterM20_test.reshape((ClusterM20_test.size, 1))\n",
    "ClusterDistFromVert_test = ClusterDistFromVert_test.reshape((ClusterDistFromVert_test.size, 1))\n",
    "PartE_test = PartE_test.reshape((PartE_test.size, 1))\n",
    "PartPt_test = PartPt_test.reshape((PartPt_test.size, 1))\n",
    "PartEta_test = PartEta_test.reshape((PartEta_test.size, 1))\n",
    "PartPhi_test = PartPhi_test.reshape((PartPhi_test.size, 1))\n",
    "PartIsPrimary_test = PartIsPrimary_test.reshape((PartIsPrimary_test.size, 1))\n",
    "PartPID_test = PartPID_test.reshape((PartPID_test.size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load it to pytorch `tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_train = torch.as_tensor(ClusterN_train, dtype=torch.uint8)\n",
    "Cluster_train = torch.as_tensor(Cluster_train, dtype=torch.float32)\n",
    "ClusterTiming_train = torch.as_tensor(ClusterTiming_train, dtype=torch.float32)\n",
    "ClusterType_train = torch.as_tensor(ClusterType_train, dtype=torch.uint8)\n",
    "ClusterE_train = torch.as_tensor(ClusterE_train, dtype=torch.float32)\n",
    "ClusterPt_train = torch.as_tensor(ClusterPt_train, dtype=torch.float32)\n",
    "ClusterModuleNumber_train = torch.as_tensor(ClusterModuleNumber_train, dtype=torch.uint8)\n",
    "ClusterRow_train = torch.as_tensor(ClusterRow_train, dtype=torch.uint8)\n",
    "ClusterCol_train = torch.as_tensor(ClusterCol_train, dtype=torch.uint8)\n",
    "ClusterM02_train = torch.as_tensor(ClusterM02_train, dtype=torch.float32)\n",
    "ClusterM20_train = torch.as_tensor(ClusterM20_train, dtype=torch.float32)\n",
    "ClusterDistFromVert_train = torch.as_tensor(ClusterDistFromVert_train, dtype=torch.float32)\n",
    "PartE_train = torch.as_tensor(PartE_train, dtype=torch.float32)\n",
    "PartPt_train = torch.as_tensor(PartPt_train, dtype=torch.float32)\n",
    "PartEta_train = torch.as_tensor(PartEta_train, dtype=torch.float32)\n",
    "PartPhi_train = torch.as_tensor(PartPhi_train, dtype=torch.float32)\n",
    "PartIsPrimary_train = torch.as_tensor(PartIsPrimary_train, dtype=torch.bool)\n",
    "PartPID_train = torch.as_tensor(PartPID_train, dtype=torch.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterN_test = torch.as_tensor(ClusterN_test, dtype=torch.uint8)\n",
    "Cluster_test = torch.as_tensor(Cluster_test, dtype=torch.float32)\n",
    "ClusterTiming_test = torch.as_tensor(ClusterTiming_test, dtype=torch.float32)\n",
    "ClusterType_test = torch.as_tensor(ClusterType_test, dtype=torch.uint8)\n",
    "ClusterE_test = torch.as_tensor(ClusterE_test, dtype=torch.float32)\n",
    "ClusterPt_test = torch.as_tensor(ClusterPt_test, dtype=torch.float32)\n",
    "ClusterModuleNumber_test = torch.as_tensor(ClusterModuleNumber_test, dtype=torch.uint8)\n",
    "ClusterRow_test = torch.as_tensor(ClusterRow_test, dtype=torch.uint8)\n",
    "ClusterCol_test = torch.as_tensor(ClusterCol_test, dtype=torch.uint8)\n",
    "ClusterM02_test = torch.as_tensor(ClusterM02_test, dtype=torch.float32)\n",
    "ClusterM20_test = torch.as_tensor(ClusterM20_test, dtype=torch.float32)\n",
    "ClusterDistFromVert_test = torch.as_tensor(ClusterDistFromVert_test, dtype=torch.float32)\n",
    "PartE_test = torch.as_tensor(PartE_test, dtype=torch.float32)\n",
    "PartPt_test = torch.as_tensor(PartPt_test, dtype=torch.float32)\n",
    "PartEta_test = torch.as_tensor(PartEta_test, dtype=torch.float32)\n",
    "PartPhi_test = torch.as_tensor(PartPhi_test, dtype=torch.float32)\n",
    "PartIsPrimary_test = torch.as_tensor(PartIsPrimary_test, dtype=torch.bool)\n",
    "PartPID_test = torch.as_tensor(PartPID_test, dtype=torch.short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for DataSet and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train():\n",
    "    dataset_train = utils.TensorDataset( ClusterN_train, Cluster_train, ClusterTiming_train, ClusterType_train\n",
    "                                    , ClusterE_train, ClusterPt_train, ClusterModuleNumber_train\n",
    "                                    , ClusterRow_train, ClusterCol_train, ClusterM02_train, ClusterM20_train\n",
    "                                    , ClusterDistFromVert_train, PartE_train, PartPt_train\n",
    "                                    , PartEta_train, PartPhi_train, PartIsPrimary_train, PartPID_train )\n",
    "    \n",
    "    return dataset_train\n",
    "\n",
    "def load_data_test():\n",
    "    dataset_test = utils.TensorDataset( ClusterN_test, Cluster_test, ClusterTiming_test, ClusterType_test\n",
    "                                   , ClusterE_test, ClusterPt_test, ClusterModuleNumber_test\n",
    "                                   , ClusterRow_test, ClusterCol_test, ClusterM02_test, ClusterM20_test\n",
    "                                   , ClusterDistFromVert_test, PartE_test, PartPt_test\n",
    "                                    , PartEta_test, PartPhi_test, PartIsPrimary_test, PartPID_test )\n",
    "    \n",
    "    return dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(train_ds, val_ds, bs):\n",
    "    return (\n",
    "        utils.DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=cpus_per_trial),\n",
    "        utils.DataLoader(val_ds, batch_size=bs * 2, shuffle=True, num_workers=cpus_per_trial),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1610.04490\n",
    "INSTANCE_NOISE = False\n",
    "\n",
    "def add_instance_noise(data, std=0.01):\n",
    "    return data + torch.distributions.Normal(0, std).sample(data.shape) #.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, l1=100, l2=50, l3=25):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10, kernel_size=3, padding=2)\n",
    "        self.conv2 = nn.Conv2d(10,10, kernel_size=3,  padding=2)\n",
    "        self.conv3 = nn.Conv2d(10,10, kernel_size=3, padding=0)\n",
    "        self.conv4 = nn.Conv2d(10,5, kernel_size=1, padding=0)\n",
    "        self.conv5 = nn.Conv2d(5,3, kernel_size=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_nn = nn.Sequential(\n",
    "            nn.Linear(4005, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3,3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, cluster, clusNumXYEPt):\n",
    "        cluster = F.relu(self.conv1(cluster))\n",
    "        cluster = F.relu(self.conv2(cluster))\n",
    "        cluster = F.relu(self.conv3(cluster))\n",
    "        cluster = F.relu(self.conv3(cluster))\n",
    "        x = self.flatten(cluster)\n",
    "        x = torch.cat([x, clusNumXYEPt], dim=1)\n",
    "        logits = self.dense_nn(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for cluster reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_cluster(ncell, modnum, row, col, data, arrsize=20):\n",
    "    if not torch.all( modnum[0] == modnum[:ncell]):\n",
    "        ModNumDif = modnum - torch.min(modnum[:ncell])\n",
    "        mask = torch.where(ModNumDif == 1)\n",
    "        col[mask] += 48\n",
    "        mask = torch.where(ModNumDif == 2)\n",
    "        row[mask] += 24\n",
    "        mask = torch.where(ModNumDif == 3)\n",
    "        row[mask] += 24\n",
    "        col[mask] += 48\n",
    "\n",
    "    arr = torch.zeros((arrsize,arrsize), dtype=torch.float32)\n",
    "  \n",
    "    col_min = torch.min(col[:ncell])\n",
    "    row_min = torch.min(row[:ncell])\n",
    "    width = torch.max(col[:ncell]) - col_min\n",
    "    height = torch.max(row[:ncell]) - row_min\n",
    "    offset_h = ((arrsize-height)/2).int()\n",
    "    offset_w = ((arrsize-width)/2).int()\n",
    "    \n",
    "    for i in range(ncell):\n",
    "        arr[ row[i] - row_min + offset_h, col[i] - col_min + offset_w ] = data[i]\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement train and validation loop\n",
    "[0: 'ClusterN', 1:'Cluster', 2:'ClusterTiming', 3:'ClusterType', 4:'ClusterE', 5:'ClusterPt', 6:'ClusterModuleNumber', 7:'ClusterRow', 8:'ClusterCol', 9:'ClusterM02', 10:'ClusterM20', 11:'ClusterDistFromVert', 12:'PartE', 13:'PartPt', 14:'PartEta', 15:'PartPhi', 16:'PartIsPrimary', 17:'PartPID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "    size = len(dataloader.dataset)\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        ClN, Cl, ClE, ClPt, ClModNum, ClRow, ClCol, ClM02, ClM20, ClDist, PartPID = data[0], data[1],\\\n",
    "        data[4], data[5], data[6], \\\n",
    "        data[7], data[8], data[9], data[10], \\\n",
    "        data[11], data[17]\n",
    "        \n",
    "        # reconstruct clusters from data\n",
    "        clusters_e = []\n",
    "\n",
    "        for i in range(ClN.shape[0]):\n",
    "            cluster_e = reconstruct_cluster(ClN[i], ClModNum[i], ClRow[i], ClCol[i], Cl[i])\n",
    "            clusters_e.append(cluster_e)\n",
    "        \n",
    "        clusters_e = torch.stack(clusters_e)\n",
    "        clusters_e = clusters_e.view(-1, 1, 20,20)\n",
    "        if INSTANCE_NOISE:\n",
    "            clusters_e = add_instance_noise(clusters_e)\n",
    "        clusters_e.to(device)\n",
    "        \n",
    "        ClusterProperties = torch.cat([ClE, ClPt, ClM02, ClM20, ClDist], dim=1)\n",
    "        ClusterProperties.to(device)\n",
    "               \n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction and loss\n",
    "        pred = model(clusters_e, ClusterProperties)\n",
    "        loss = loss_fn(pred, PartPID[:,0].long())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        \n",
    "        if batch % 10 == 9:\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, batch + 1,\n",
    "                                            running_loss / epoch_steps))\n",
    "            running_loss = 0.0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(epoch, dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            ClN, Cl, ClE, ClPt, ClModNum, ClRow, ClCol, ClM02, ClM20, ClDist, PartPID = data[0], data[1],\\\n",
    "            data[4], data[5], data[6], \\\n",
    "            data[7], data[8], data[9], data[10], \\\n",
    "            data[11], data[17]\n",
    "        \n",
    "            # reconstruct clusters from data\n",
    "            clusters_e = []\n",
    "\n",
    "            for i in range(ClN.shape[0]):\n",
    "                cluster_e = reconstruct_cluster(ClN[i], ClModNum[i], ClRow[i], ClCol[i], Cl[i])\n",
    "                clusters_e.append(cluster_e)\n",
    "        \n",
    "            clusters_e = torch.stack(clusters_e)\n",
    "            clusters_e = clusters_e.view(-1, 1, 20,20)\n",
    "            if INSTANCE_NOISE:\n",
    "                clusters_e = add_instance_noise(clusters_e)\n",
    "            clusters_e.to(device)\n",
    "        \n",
    "            ClusterProperties = torch.cat([ClE, ClPt, ClM02, ClM20, ClDist], dim=1)\n",
    "            ClusterProperties.to(device)\n",
    "            \n",
    "            pred = model(clusters_e, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == PartPID[:,0]).type(torch.float).sum().item()\n",
    "\n",
    "            loss = loss_fn(pred, PartPID[:,0].long())#.item()\n",
    "            val_loss += loss.cpu().numpy()\n",
    "            val_steps += 1\n",
    "    \n",
    "    with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "        \n",
    "    tune.report(loss=(val_loss / val_steps), accuracy= correct / size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, checkpoint_dir=None):\n",
    "    \n",
    "    # load model\n",
    "    model = CNN(config[\"l1\"],config[\"l2\"],config[\"l3\"])\n",
    "    \n",
    "    # check for avlaible resource and initialize device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    # send model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # initialise loss function and opptimizer\n",
    "    loss_fn = F.cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    \n",
    "    # check whether checkpoint is available\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "    # load dataset\n",
    "    dataset_train = load_data_train()\n",
    "    \n",
    "    # split trainset in train and validation subsets\n",
    "    test_abs = int(len(dataset_train) * 0.8)\n",
    "    subset_train, subset_val = utils.random_split(\n",
    "        dataset_train, [test_abs, len(dataset_train) - test_abs])\n",
    "    \n",
    "    # get dataloaders \n",
    "    dataloader_train, dataloader_val = get_dataloader(subset_train, subset_val, int(config[\"batch_size\"]))\n",
    "                                                      \n",
    "    for epoch in range(100):\n",
    "        train_loop(epoch, dataloader_train, model, loss_fn, optimizer, device=device)\n",
    "        val_loop(epoch, dataloader_train, model, loss_fn, optimizer, device=device)                                              \n",
    "    \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement method for accuracy testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, device=\"cpu\"):\n",
    "    \n",
    "    dataset_test = load_data_test()\n",
    "    \n",
    "    dataloader_test = utils.DataLoader(\n",
    "        dataset_test, batch_size=4, shuffle=False, num_workers=2)\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(dataloader_test.dataset)\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader_test:\n",
    "            ClN, Cl, ClE, ClPt, ClModNum, ClRow, ClCol, ClM02, ClM20, ClDist, PartPID = data[0], data[1],\\\n",
    "            data[4], data[5], data[6], \\\n",
    "            data[7], data[8], data[9], data[10], \\\n",
    "            data[11], data[17]\n",
    "        \n",
    "            # reconstruct clusters from data\n",
    "            clusters_e = []\n",
    "\n",
    "            for i in range(ClN.shape[0]):\n",
    "                cluster_e = reconstruct_cluster(ClN[i], ClModNum[i], ClRow[i], ClCol[i], Cl[i])\n",
    "                clusters_e.append(cluster_e)\n",
    "        \n",
    "            clusters_e = torch.stack(clusters_e)\n",
    "            clusters_e = clusters_e.view(-1, 1, 20,20)\n",
    "            if INSTANCE_NOISE:\n",
    "                clusters_e = add_instance_noise(clusters_e)\n",
    "            clusters_e.to(device)\n",
    "        \n",
    "            ClusterProperties = torch.cat([ClE, ClPt, ClM02, ClM20, ClDist], dim=1)\n",
    "            ClusterProperties.to(device)\n",
    "            \n",
    "            pred = model(clusters_e, ClusterProperties)\n",
    "            correct += (pred.argmax(1) == PartPID[:,0]).type(torch.float).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all Ray Tune functionality and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
    "    \n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"l3\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 10)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16, 32, 64])\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"l1\", \"l2\", \"l3\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    #Get Current date and time\n",
    "    timestr = time.strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "    name = \"ASHA-\" + timestr\n",
    "    \n",
    "    result = tune.run(\n",
    "        partial(train_model),\n",
    "        name = name,\n",
    "        resources_per_trial={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        local_dir = \"./Ray_Results\",\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "    \n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    best_trained_model = CNN(best_trial.config[\"l1\"], best_trial.config[\"l2\"], best_trial.config[\"l3\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "    \n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 15:03:41,909\tERROR services.py:1272 -- Failed to start the dashboard: Failed to start the dashboard, return code 0. The last 10 lines of /tmp/ray/session_2021-09-15_15-03-40_938529_215227/logs/dashboard.log:\n",
      "  File \"/home/jhonerma/anaconda3/envs/cnn-env/lib/python3.9/site-packages/ray/new_dashboard/dashboard.py\", line 207, in <module>\n",
      "    dashboard = Dashboard(\n",
      "  File \"/home/jhonerma/anaconda3/envs/cnn-env/lib/python3.9/site-packages/ray/new_dashboard/dashboard.py\", line 98, in __init__\n",
      "    raise ex\n",
      "  File \"/home/jhonerma/anaconda3/envs/cnn-env/lib/python3.9/site-packages/ray/new_dashboard/dashboard.py\", line 89, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"/home/jhonerma/anaconda3/envs/cnn-env/lib/python3.9/site-packages/ray/new_dashboard/dashboard.py\", line 42, in setup_static_dir\n",
      "    raise FrontendNotFoundError(\n",
      "FrontendNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/new_dashboard/client && npm install && npm ci && npm run build): '/home/jhonerma/anaconda3/envs/cnn-env/lib/python3.9/site-packages/ray/new_dashboard/client/build'\n",
      "2021-09-15 15:03:42,846\tWARNING experiment.py:295 -- No name detected on trainable. Using DEFAULT.\n",
      "2021-09-15 15:03:42,847\tINFO registry.py:66 -- Detected unknown callable for trainable. Converting to class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 6.2/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/12 CPUs, 0/0 GPUs, 0.0/6.22 GiB heap, 0.0/3.11 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_15-15:03:40\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+---------------------+----------+-------+------+------+------+-------------+--------------+\n",
      "| Trial name          | status   | loc   |   l1 |   l2 |   l3 |          lr |   batch_size |\n",
      "|---------------------+----------+-------+------+------+------+-------------+--------------|\n",
      "| DEFAULT_5a715_00000 | RUNNING  |       |  128 |    8 |   16 | 0.0128276   |            2 |\n",
      "| DEFAULT_5a715_00001 | PENDING  |       |    4 |   16 |   16 | 0.00042926  |           16 |\n",
      "| DEFAULT_5a715_00002 | PENDING  |       |   16 |    4 |  128 | 0.0111267   |            8 |\n",
      "| DEFAULT_5a715_00003 | PENDING  |       |   32 |    4 |    8 | 0.0211389   |           16 |\n",
      "| DEFAULT_5a715_00004 | PENDING  |       |   64 |  512 |   16 | 0.000293835 |            2 |\n",
      "| DEFAULT_5a715_00005 | PENDING  |       |   16 |   16 |  256 | 0.000485688 |           32 |\n",
      "| DEFAULT_5a715_00006 | PENDING  |       |   16 |  128 |   16 | 0.0114991   |            2 |\n",
      "| DEFAULT_5a715_00007 | PENDING  |       |    4 |  512 |   16 | 0.024128    |           16 |\n",
      "| DEFAULT_5a715_00008 | PENDING  |       |  512 |  128 |  128 | 0.0013626   |           32 |\n",
      "| DEFAULT_5a715_00009 | PENDING  |       |  128 |   32 |   16 | 0.00308609  |           64 |\n",
      "+---------------------+----------+-------+------+------+------+-------------+--------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=215342)\u001b[0m [1,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=215348)\u001b[0m [1,    10] loss: 1.131\n",
      "\u001b[2m\u001b[36m(pid=215343)\u001b[0m [1,    10] loss: 0.839\n",
      "\u001b[2m\u001b[36m(pid=215349)\u001b[0m [1,    10] loss: 1.099\n",
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [1,    10] loss: 0.818\n",
      "\u001b[2m\u001b[36m(pid=215340)\u001b[0m [1,    10] loss: 1.193\n",
      "Result for DEFAULT_5a715_00005:\n",
      "  accuracy: 0.6870748299319728\n",
      "  date: 2021-09-15_15-03-44\n",
      "  done: false\n",
      "  experiment_id: 42c09e4d813a4dcdae5868b3b914323a\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0266180992126466\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215347\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.6699588298797607\n",
      "  time_this_iter_s: 0.6699588298797607\n",
      "  time_total_s: 0.6699588298797607\n",
      "  timestamp: 1631711024\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00005\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=215342)\u001b[0m [1,    20] loss: 0.549\n",
      "\u001b[2m\u001b[36m(pid=215343)\u001b[0m [1,    20] loss: 0.542\n",
      "\u001b[2m\u001b[36m(pid=215345)\u001b[0m [1,    10] loss: 0.950\n",
      "Result for DEFAULT_5a715_00009:\n",
      "  accuracy: 0.6870748299319728\n",
      "  date: 2021-09-15_15-03-44\n",
      "  done: false\n",
      "  experiment_id: 57679a700604483ab0682b871c40a9cb\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8591038386027018\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215341\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.6182675361633301\n",
      "  time_this_iter_s: 0.6182675361633301\n",
      "  time_total_s: 0.6182675361633301\n",
      "  timestamp: 1631711024\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00009\n",
      "  \n",
      "Result for DEFAULT_5a715_00001:\n",
      "  accuracy: 0.22448979591836735\n",
      "  date: 2021-09-15_15-03-44\n",
      "  done: true\n",
      "  experiment_id: 8ea4a67e89ee453787d7c5853ea989a8\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.1258550763130188\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215348\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.7880303859710693\n",
      "  time_this_iter_s: 0.7880303859710693\n",
      "  time_total_s: 0.7880303859710693\n",
      "  timestamp: 1631711024\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00001\n",
      "  \n",
      "Result for DEFAULT_5a715_00003:\n",
      "  accuracy: 0.6938775510204082\n",
      "  date: 2021-09-15_15-03-44\n",
      "  done: false\n",
      "  experiment_id: 89918d9c5dd24b84a7704575edc38633\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.8602171584963798\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215346\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.8118267059326172\n",
      "  time_this_iter_s: 0.8118267059326172\n",
      "  time_total_s: 0.8118267059326172\n",
      "  timestamp: 1631711024\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00003\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=215340)\u001b[0m [1,    20] loss: 0.550\n",
      "\u001b[2m\u001b[36m(pid=215342)\u001b[0m [1,    30] loss: 0.366\n",
      "\u001b[2m\u001b[36m(pid=215343)\u001b[0m [1,    30] loss: 0.373\n",
      "Result for DEFAULT_5a715_00007:\n",
      "  accuracy: 0.7006802721088435\n",
      "  date: 2021-09-15_15-03-44\n",
      "  done: false\n",
      "  experiment_id: 9ed8c9d2f58b4ca38d47edf955acb69e\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.836990675330162\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215345\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.7545685768127441\n",
      "  time_this_iter_s: 0.7545685768127441\n",
      "  time_total_s: 0.7545685768127441\n",
      "  timestamp: 1631711024\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00007\n",
      "  \n",
      "Result for DEFAULT_5a715_00002:\n",
      "  accuracy: 0.2108843537414966\n",
      "  date: 2021-09-15_15-03-45\n",
      "  done: true\n",
      "  experiment_id: c0144af2c0f443bf853fe1a18632651b\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215349\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.076061487197876\n",
      "  time_this_iter_s: 1.076061487197876\n",
      "  time_total_s: 1.076061487197876\n",
      "  timestamp: 1631711025\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00002\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=215343)\u001b[0m [1,    40] loss: 0.211\n",
      "\u001b[2m\u001b[36m(pid=215340)\u001b[0m [1,    30] loss: 0.366\n",
      "Result for DEFAULT_5a715_00008:\n",
      "  accuracy: 0.7142857142857143\n",
      "  date: 2021-09-15_15-03-45\n",
      "  done: false\n",
      "  experiment_id: 249b14461be9455180d328247076452a\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.7380674540996551\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215344\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.0546021461486816\n",
      "  time_this_iter_s: 1.0546021461486816\n",
      "  time_total_s: 1.0546021461486816\n",
      "  timestamp: 1631711025\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00008\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=215342)\u001b[0m [1,    40] loss: 0.275\n",
      "\u001b[2m\u001b[36m(pid=215343)\u001b[0m [1,    50] loss: 0.175\n",
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [2,    10] loss: 0.857\n",
      "Result for DEFAULT_5a715_00009:\n",
      "  accuracy: 0.6870748299319728\n",
      "  date: 2021-09-15_15-03-45\n",
      "  done: true\n",
      "  experiment_id: 57679a700604483ab0682b871c40a9cb\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.9804191788037618\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215341\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.1923089027404785\n",
      "  time_this_iter_s: 0.5740413665771484\n",
      "  time_total_s: 1.1923089027404785\n",
      "  timestamp: 1631711025\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 5a715_00009\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=215340)\u001b[0m [1,    40] loss: 0.275\n",
      "\u001b[2m\u001b[36m(pid=215345)\u001b[0m [2,    10] loss: 0.920\n",
      "\u001b[2m\u001b[36m(pid=215342)\u001b[0m [1,    50] loss: 0.220\n",
      "\u001b[2m\u001b[36m(pid=215343)\u001b[0m [1,    60] loss: 0.120\n",
      "\u001b[2m\u001b[36m(pid=215342)\u001b[0m [1,    60] loss: 0.183\n",
      "\u001b[2m\u001b[36m(pid=215343)\u001b[0m [1,    70] loss: 0.189\n",
      "\u001b[2m\u001b[36m(pid=215340)\u001b[0m [1,    50] loss: 0.220\n",
      "\u001b[2m\u001b[36m(pid=215342)\u001b[0m [1,    70] loss: 0.157\n",
      "\u001b[2m\u001b[36m(pid=215340)\u001b[0m [1,    60] loss: 0.183\n",
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [3,    10] loss: 0.781\n",
      "\u001b[2m\u001b[36m(pid=215345)\u001b[0m [3,    10] loss: 0.861\n",
      "\u001b[2m\u001b[36m(pid=215340)\u001b[0m [1,    70] loss: 0.157\n",
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [4,    10] loss: 0.795\n",
      "Result for DEFAULT_5a715_00006:\n",
      "  accuracy: 0.7346938775510204\n",
      "  date: 2021-09-15_15-03-46\n",
      "  done: true\n",
      "  experiment_id: 4dee1fe36da1453392cb38c9a3ff937f\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.888889230586387\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215343\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.3877780437469482\n",
      "  time_this_iter_s: 2.3877780437469482\n",
      "  time_total_s: 2.3877780437469482\n",
      "  timestamp: 1631711026\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00006\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=215345)\u001b[0m [4,    10] loss: 0.868\n",
      "Result for DEFAULT_5a715_00004:\n",
      "  accuracy: 0.22448979591836735\n",
      "  date: 2021-09-15_15-03-46\n",
      "  done: true\n",
      "  experiment_id: 6edab73e2506443e877df44ab0db5357\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215342\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.6832709312438965\n",
      "  time_this_iter_s: 2.6832709312438965\n",
      "  time_total_s: 2.6832709312438965\n",
      "  timestamp: 1631711026\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00004\n",
      "  \n",
      "Result for DEFAULT_5a715_00007:\n",
      "  accuracy: 0.7006802721088435\n",
      "  date: 2021-09-15_15-03-46\n",
      "  done: true\n",
      "  experiment_id: 9ed8c9d2f58b4ca38d47edf955acb69e\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 4\n",
      "  loss: 0.8385461449623108\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215345\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.51983904838562\n",
      "  time_this_iter_s: 0.5263621807098389\n",
      "  time_total_s: 2.51983904838562\n",
      "  timestamp: 1631711026\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 5a715_00007\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [5,    10] loss: 0.775\n",
      "Result for DEFAULT_5a715_00000:\n",
      "  accuracy: 0.21768707482993196\n",
      "  date: 2021-09-15_15-03-47\n",
      "  done: true\n",
      "  experiment_id: f06570d4f3fa425e84b4d0e4a570bb8f\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.0986123085021973\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215340\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.957535743713379\n",
      "  time_this_iter_s: 2.957535743713379\n",
      "  time_total_s: 2.957535743713379\n",
      "  timestamp: 1631711027\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5a715_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [6,    10] loss: 0.840\n",
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [7,    10] loss: 0.823\n",
      "== Status ==\n",
      "Memory usage on this node: 6.3/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -0.82437162399292 | Iter 4.000: -0.824166116118431 | Iter 2.000: -0.8172131359577179 | Iter 1.000: -0.9577536648995169\n",
      "Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/6.22 GiB heap, 0.0/3.11 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_15-15:03:40\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+---------------------+------------+--------------------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "| Trial name          | status     | loc                |   l1 |   l2 |   l3 |          lr |   batch_size |     loss |   accuracy |   training_iteration |\n",
      "|---------------------+------------+--------------------+------+------+------+-------------+--------------+----------+------------+----------------------|\n",
      "| DEFAULT_5a715_00003 | RUNNING    | 10.67.95.37:215346 |   32 |    4 |    8 | 0.0211389   |           16 | 0.823816 |   0.693878 |                    7 |\n",
      "| DEFAULT_5a715_00005 | RUNNING    | 10.67.95.37:215347 |   16 |   16 |  256 | 0.000485688 |           32 | 0.831867 |   0.687075 |                    9 |\n",
      "| DEFAULT_5a715_00008 | RUNNING    | 10.67.95.37:215344 |  512 |  128 |  128 | 0.0013626   |           32 | 0.811475 |   0.714286 |                    5 |\n",
      "| DEFAULT_5a715_00000 | TERMINATED |                    |  128 |    8 |   16 | 0.0128276   |            2 | 1.09861  |   0.217687 |                    1 |\n",
      "| DEFAULT_5a715_00001 | TERMINATED |                    |    4 |   16 |   16 | 0.00042926  |           16 | 1.12586  |   0.22449  |                    1 |\n",
      "| DEFAULT_5a715_00002 | TERMINATED |                    |   16 |    4 |  128 | 0.0111267   |            8 | 1.09861  |   0.210884 |                    1 |\n",
      "| DEFAULT_5a715_00004 | TERMINATED |                    |   64 |  512 |   16 | 0.000293835 |            2 | 1.09861  |   0.22449  |                    1 |\n",
      "| DEFAULT_5a715_00006 | TERMINATED |                    |   16 |  128 |   16 | 0.0114991   |            2 | 0.888889 |   0.734694 |                    1 |\n",
      "| DEFAULT_5a715_00007 | TERMINATED |                    |    4 |  512 |   16 | 0.024128    |           16 | 0.838546 |   0.70068  |                    4 |\n",
      "| DEFAULT_5a715_00009 | TERMINATED |                    |  128 |   32 |   16 | 0.00308609  |           64 | 0.980419 |   0.687075 |                    2 |\n",
      "+---------------------+------------+--------------------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [8,    10] loss: 0.800\n",
      "Result for DEFAULT_5a715_00005:\n",
      "  accuracy: 0.6870748299319728\n",
      "  date: 2021-09-15_15-03-48\n",
      "  done: true\n",
      "  experiment_id: 42c09e4d813a4dcdae5868b3b914323a\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.8153103828430176\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215347\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 4.455486297607422\n",
      "  time_this_iter_s: 0.34427738189697266\n",
      "  time_total_s: 4.455486297607422\n",
      "  timestamp: 1631711028\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 5a715_00005\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [9,    10] loss: 0.823\n",
      "\u001b[2m\u001b[36m(pid=215346)\u001b[0m [10,    10] loss: 0.867\n",
      "Result for DEFAULT_5a715_00003:\n",
      "  accuracy: 0.6938775510204082\n",
      "  date: 2021-09-15_15-03-49\n",
      "  done: true\n",
      "  experiment_id: 89918d9c5dd24b84a7704575edc38633\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.859547746181488\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215346\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 5.308492183685303\n",
      "  time_this_iter_s: 0.4189009666442871\n",
      "  time_total_s: 5.308492183685303\n",
      "  timestamp: 1631711029\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 5a715_00003\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 15:03:50,668\tINFO tune.py:561 -- Total run time: 7.82 seconds (7.64 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DEFAULT_5a715_00008:\n",
      "  accuracy: 0.7142857142857143\n",
      "  date: 2021-09-15_15-03-50\n",
      "  done: true\n",
      "  experiment_id: 249b14461be9455180d328247076452a\n",
      "  hostname: jhonerma-tuxedo\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.7618562817573548\n",
      "  node_ip: 10.67.95.37\n",
      "  pid: 215344\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 6.328228950500488\n",
      "  time_this_iter_s: 0.511955738067627\n",
      "  time_total_s: 6.328228950500488\n",
      "  timestamp: 1631711030\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 5a715_00008\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 5.7/15.1 GiB\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 8.000: -0.813855129480362 | Iter 4.000: -0.824166116118431 | Iter 2.000: -0.8172131359577179 | Iter 1.000: -0.9577536648995169\n",
      "Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/6.22 GiB heap, 0.0/3.11 GiB objects\n",
      "Result logdir: /home/jhonerma/ML-Notebooks/CNN/Ray_Results/ASHA-2021_09_15-15:03:40\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "| Trial name          | status     | loc   |   l1 |   l2 |   l3 |          lr |   batch_size |     loss |   accuracy |   training_iteration |\n",
      "|---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------|\n",
      "| DEFAULT_5a715_00000 | TERMINATED |       |  128 |    8 |   16 | 0.0128276   |            2 | 1.09861  |   0.217687 |                    1 |\n",
      "| DEFAULT_5a715_00001 | TERMINATED |       |    4 |   16 |   16 | 0.00042926  |           16 | 1.12586  |   0.22449  |                    1 |\n",
      "| DEFAULT_5a715_00002 | TERMINATED |       |   16 |    4 |  128 | 0.0111267   |            8 | 1.09861  |   0.210884 |                    1 |\n",
      "| DEFAULT_5a715_00003 | TERMINATED |       |   32 |    4 |    8 | 0.0211389   |           16 | 0.859548 |   0.693878 |                   10 |\n",
      "| DEFAULT_5a715_00004 | TERMINATED |       |   64 |  512 |   16 | 0.000293835 |            2 | 1.09861  |   0.22449  |                    1 |\n",
      "| DEFAULT_5a715_00005 | TERMINATED |       |   16 |   16 |  256 | 0.000485688 |           32 | 0.81531  |   0.687075 |                   10 |\n",
      "| DEFAULT_5a715_00006 | TERMINATED |       |   16 |  128 |   16 | 0.0114991   |            2 | 0.888889 |   0.734694 |                    1 |\n",
      "| DEFAULT_5a715_00007 | TERMINATED |       |    4 |  512 |   16 | 0.024128    |           16 | 0.838546 |   0.70068  |                    4 |\n",
      "| DEFAULT_5a715_00008 | TERMINATED |       |  512 |  128 |  128 | 0.0013626   |           32 | 0.761856 |   0.714286 |                   10 |\n",
      "| DEFAULT_5a715_00009 | TERMINATED |       |  128 |   32 |   16 | 0.00308609  |           64 | 0.980419 |   0.687075 |                    2 |\n",
      "+---------------------+------------+-------+------+------+------+-------------+--------------+----------+------------+----------------------+\n",
      "\n",
      "\n",
      "Best trial config: {'l1': 512, 'l2': 128, 'l3': 128, 'lr': 0.001362599523142901, 'batch_size': 32}\n",
      "Best trial final validation loss: 0.7618562817573548\n",
      "Best trial final validation accuracy: 0.7142857142857143\n",
      "Best trial test set accuracy: 0.7021276595744681\n"
     ]
    }
   ],
   "source": [
    "main(num_samples=10, max_num_epochs=10, gpus_per_trial=gpus_per_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn-env]",
   "language": "python",
   "name": "conda-env-cnn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
